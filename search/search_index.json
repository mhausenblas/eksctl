{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"eksctl - The official CLI for Amazon EKS \u00b6 sponsored by and built by on eksctl is a simple CLI tool for creating clusters on EKS - Amazon's new managed Kubernetes service for EC2. It is written in Go, uses CloudFormation, was created by Weaveworks and it welcomes contributions from the community. Create a basic cluster in minutes with just one command: 1 eksctl create cluster A cluster will be created with default parameters: exciting auto-generated name, e.g., fabulous-mushroom-1527688624 two m5.large worker nodes\u2014this instance type suits most common use-cases, and is good value for money use the official AWS EKS AMI us-west-2 region a dedicated VPC (check your quotas) using static AMI resolver Example output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 $ eksctl create cluster [\u2139] using region us - west - 2 [\u2139] setting availability zones to [ us - west - 2 a us - west - 2 c us - west - 2 b ] [\u2139] subnets for us - west - 2 a - public : 192 . 168 . 0 . 0 / 19 private : 192 . 168 . 96 . 0 / 19 [\u2139] subnets for us - west - 2 c - public : 192 . 168 . 32 . 0 / 19 private : 192 . 168 . 128 . 0 / 19 [\u2139] subnets for us - west - 2 b - public : 192 . 168 . 64 . 0 / 19 private : 192 . 168 . 160 . 0 / 19 [\u2139] nodegroup \" ng-98b3b83a \" will use \" ami-05ecac759c81e0b0c \" [ AmazonLinux2 / 1 . 11 ] [\u2139] creating EKS cluster \" floral-unicorn-1540567338 \" in \" us-west-2 \" region [\u2139] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup [\u2139] if you encounter any issues , check CloudFormation console or try ' eksctl utils describe-stacks --region=us-west-2 --cluster=floral-unicorn-1540567338 ' [\u2139] 2 sequential tasks : { create cluster control plane \" floral-unicorn-1540567338 \" , create nodegroup \" ng-98b3b83a \" } [\u2139] building cluster stack \" eksctl-floral-unicorn-1540567338-cluster \" [\u2139] deploying stack \" eksctl-floral-unicorn-1540567338-cluster \" [\u2139] building nodegroup stack \" eksctl-floral-unicorn-1540567338-nodegroup-ng-98b3b83a \" [\u2139] -- nodes - min = 2 was set automatically for nodegroup ng - 98 b3b83a [\u2139] -- nodes - max = 2 was set automatically for nodegroup ng - 98 b3b83a [\u2139] deploying stack \" eksctl-floral-unicorn-1540567338-nodegroup-ng-98b3b83a \" [\u2714] all EKS cluster resource for \" floral-unicorn-1540567338 \" had been created [\u2714] saved kubeconfig as \" ~/.kube/config \" [\u2139] adding role \" arn:aws:iam::376248598259:role/eksctl-ridiculous-sculpture-15547-NodeInstanceRole-1F3IHNVD03Z74 \" to auth ConfigMap [\u2139] nodegroup \" ng-98b3b83a \" has 1 node ( s ) [\u2139] node \" ip-192-168-64-220.us-west-2.compute.internal \" is not ready [\u2139] waiting for at least 2 node ( s ) to become ready in \" ng-98b3b83a \" [\u2139] nodegroup \" ng-98b3b83a \" has 2 node ( s ) [\u2139] node \" ip-192-168-64-220.us-west-2.compute.internal \" is ready [\u2139] node \" ip-192-168-8-135.us-west-2.compute.internal \" is ready [\u2139] kubectl command should work with \" ~/.kube/config \" , try ' kubectl get nodes ' [\u2714] EKS cluster \" floral-unicorn-1540567338 \" in \" us-west-2 \" region is ready Need help? Join Weave Community Slack . Customize your cluster by using a config file. Just run 1 eksctl create cluster - f cluster . yaml to apply a cluster.yaml file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : basic-cluster region : eu-north-1 nodeGroups : - name : ng-1 instanceType : m5.large desiredCapacity : 10 - name : ng-2 instanceType : m5.xlarge desiredCapacity : 2 Once you have created a cluster, you will find that cluster credentials were added in ~/.kube/config . If you have kubectl v1.10.x as well as aws-iam-authenticator commands in your PATH, you should be able to use kubectl . You will need to make sure to use the same AWS API credentials for this also. Check EKS docs for instructions. If you installed eksctl via Homebrew, you should have all of these dependencies installed already. To learn more about how to create clusters and other features continue reading the usage section . Credits Original Gophers drawn by Ashley McNamara , unique E, K, S, C, T & L Gopher identities had been produced with Gopherize.me .","title":"`eksctl` - The official CLI for Amazon EKS"},{"location":"#eksctl-the-official-cli-for-amazon-eks","text":"sponsored by and built by on eksctl is a simple CLI tool for creating clusters on EKS - Amazon's new managed Kubernetes service for EC2. It is written in Go, uses CloudFormation, was created by Weaveworks and it welcomes contributions from the community. Create a basic cluster in minutes with just one command: 1 eksctl create cluster A cluster will be created with default parameters: exciting auto-generated name, e.g., fabulous-mushroom-1527688624 two m5.large worker nodes\u2014this instance type suits most common use-cases, and is good value for money use the official AWS EKS AMI us-west-2 region a dedicated VPC (check your quotas) using static AMI resolver Example output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 $ eksctl create cluster [\u2139] using region us - west - 2 [\u2139] setting availability zones to [ us - west - 2 a us - west - 2 c us - west - 2 b ] [\u2139] subnets for us - west - 2 a - public : 192 . 168 . 0 . 0 / 19 private : 192 . 168 . 96 . 0 / 19 [\u2139] subnets for us - west - 2 c - public : 192 . 168 . 32 . 0 / 19 private : 192 . 168 . 128 . 0 / 19 [\u2139] subnets for us - west - 2 b - public : 192 . 168 . 64 . 0 / 19 private : 192 . 168 . 160 . 0 / 19 [\u2139] nodegroup \" ng-98b3b83a \" will use \" ami-05ecac759c81e0b0c \" [ AmazonLinux2 / 1 . 11 ] [\u2139] creating EKS cluster \" floral-unicorn-1540567338 \" in \" us-west-2 \" region [\u2139] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup [\u2139] if you encounter any issues , check CloudFormation console or try ' eksctl utils describe-stacks --region=us-west-2 --cluster=floral-unicorn-1540567338 ' [\u2139] 2 sequential tasks : { create cluster control plane \" floral-unicorn-1540567338 \" , create nodegroup \" ng-98b3b83a \" } [\u2139] building cluster stack \" eksctl-floral-unicorn-1540567338-cluster \" [\u2139] deploying stack \" eksctl-floral-unicorn-1540567338-cluster \" [\u2139] building nodegroup stack \" eksctl-floral-unicorn-1540567338-nodegroup-ng-98b3b83a \" [\u2139] -- nodes - min = 2 was set automatically for nodegroup ng - 98 b3b83a [\u2139] -- nodes - max = 2 was set automatically for nodegroup ng - 98 b3b83a [\u2139] deploying stack \" eksctl-floral-unicorn-1540567338-nodegroup-ng-98b3b83a \" [\u2714] all EKS cluster resource for \" floral-unicorn-1540567338 \" had been created [\u2714] saved kubeconfig as \" ~/.kube/config \" [\u2139] adding role \" arn:aws:iam::376248598259:role/eksctl-ridiculous-sculpture-15547-NodeInstanceRole-1F3IHNVD03Z74 \" to auth ConfigMap [\u2139] nodegroup \" ng-98b3b83a \" has 1 node ( s ) [\u2139] node \" ip-192-168-64-220.us-west-2.compute.internal \" is not ready [\u2139] waiting for at least 2 node ( s ) to become ready in \" ng-98b3b83a \" [\u2139] nodegroup \" ng-98b3b83a \" has 2 node ( s ) [\u2139] node \" ip-192-168-64-220.us-west-2.compute.internal \" is ready [\u2139] node \" ip-192-168-8-135.us-west-2.compute.internal \" is ready [\u2139] kubectl command should work with \" ~/.kube/config \" , try ' kubectl get nodes ' [\u2714] EKS cluster \" floral-unicorn-1540567338 \" in \" us-west-2 \" region is ready Need help? Join Weave Community Slack . Customize your cluster by using a config file. Just run 1 eksctl create cluster - f cluster . yaml to apply a cluster.yaml file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : basic-cluster region : eu-north-1 nodeGroups : - name : ng-1 instanceType : m5.large desiredCapacity : 10 - name : ng-2 instanceType : m5.xlarge desiredCapacity : 2 Once you have created a cluster, you will find that cluster credentials were added in ~/.kube/config . If you have kubectl v1.10.x as well as aws-iam-authenticator commands in your PATH, you should be able to use kubectl . You will need to make sure to use the same AWS API credentials for this also. Check EKS docs for instructions. If you installed eksctl via Homebrew, you should have all of these dependencies installed already. To learn more about how to create clusters and other features continue reading the usage section . Credits Original Gophers drawn by Ashley McNamara , unique E, K, S, C, T & L Gopher identities had been produced with Gopherize.me .","title":"eksctl - The official CLI for Amazon EKS"},{"location":"community/","text":"Get in touch \u00b6 Create an issue , or login to Weave Community Slack (#eksctl) ( signup ). Release Cadence \u00b6 Starting with 0.2.0 onwards, minor releases of eksctl should be expected every two weeks and patch releases will be made available as needed. One or more release candidate(s) (RC) builds will be made available prior to each minor release. RC builds are intended only for testing purposes. Project Roadmap Themes \u00b6 Cluster Quick Start with gitops \u00b6 It should be easy to create a cluster with various applications pre-installed, e.g. Weave Flux, Helm 2 (Tiller), ALB Ingress controller. It should also be easy to manage these applications in a declarative way using config files in a git repo (with gitops ). Declarative configuration management for clusters \u00b6 One should be able to make EKS cluster configuration through declarative config files ( eksctl apply ). Additionally, they should be able to manage a cluster via a git repo. Cluster addons \u00b6 Understanding how the add-ons spec by SIG Cluster Lifecycle will evolve and how we can implement management of some cluster applications via the addon spec.","title":"Community"},{"location":"community/#get-in-touch","text":"Create an issue , or login to Weave Community Slack (#eksctl) ( signup ).","title":"Get in touch"},{"location":"community/#release-cadence","text":"Starting with 0.2.0 onwards, minor releases of eksctl should be expected every two weeks and patch releases will be made available as needed. One or more release candidate(s) (RC) builds will be made available prior to each minor release. RC builds are intended only for testing purposes.","title":"Release Cadence"},{"location":"community/#project-roadmap-themes","text":"","title":"Project Roadmap Themes"},{"location":"community/#cluster-quick-start-with-gitops","text":"It should be easy to create a cluster with various applications pre-installed, e.g. Weave Flux, Helm 2 (Tiller), ALB Ingress controller. It should also be easy to manage these applications in a declarative way using config files in a git repo (with gitops ).","title":"Cluster Quick Start with gitops"},{"location":"community/#declarative-configuration-management-for-clusters","text":"One should be able to make EKS cluster configuration through declarative config files ( eksctl apply ). Additionally, they should be able to manage a cluster via a git repo.","title":"Declarative configuration management for clusters"},{"location":"community/#cluster-addons","text":"Understanding how the add-ons spec by SIG Cluster Lifecycle will evolve and how we can implement management of some cluster applications via the addon spec.","title":"Cluster addons"},{"location":"gitops-quickstart/","text":"Setup your cluster with gitops \u00b6 Welcome to eksctl gitops Quick Starts. In this guide we will show you how to launch fully-configured Kubernetes clusters that are ready to run production workloads in minutes: easy for you to get started running Kubernetes on EKS and to launch standard clusters in your organisation. At the end of this, you will have a Kubernetes cluster including control plane, worker nodes, and all of the software needed for code deployment, monitoring, and logging. Quick Start to gitops \u00b6 gitops is a way to do Kubernetes application delivery. It works by using Git as a single source of truth for Kubernetes resources and everything else. With Git at the center of your delivery pipelines, you and your team can make pull requests to accelerate and simplify application deployments and operations tasks to Kubernetes. Using gitops Quick Starts will get you set up in next to no time. You will benefit from a setup that is based on the experience of companies who run workloads at scale. Prerequisites \u00b6 To use EKS, you need to have your AWS account set up. Next, you will have to have the following tools installed: AWS CLI : at least 1.16.156 - older versions will require AWS IAM Authenticator to be installed too a specific version of kubectl which works with EKS Getting ready for gitops \u00b6 The main point of gitops is to keep everything (config, alerts, dashboards, apps, literally everything) in Git and use it as a single source of truth. To keep your cluster configuration in Git, please go ahead and create an empty repository. On GitHub, for example, follow these steps . Standing up your cluster \u00b6 First we follow the usual steps to stand up a cluster on EKS. Please review the list of flags to see if you need to tweak it for your purposes. In essence it is going to be a variation of: 1 eksctl create cluster Once it is finished, you should be able to check the cluster contents and see some system workloads: 1 2 3 4 $ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-15-6.eu-central-1.compute.internal Ready <none> 39s v1.13.8-eks-cd3eb0 ip-192-168-64-189.eu-central-1.compute.internal Ready <none> 38s v1.13.8-eks-cd3eb0 1 2 3 4 5 6 7 8 $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-l8mk7 1 /1 Running 0 45s kube-system aws-node-s2p2c 1 /1 Running 0 45s kube-system coredns-7d7755744b-f88w7 1 /1 Running 0 45s kube-system coredns-7d7755744b-qgc6r 1 /1 Running 0 45s kube-system kube-proxy-kg57w 1 /1 Running 0 45s kube-system kube-proxy-qzcmk 1 /1 Running 0 45s Enabling a gitops operator \u00b6 The following command will set up your cluster with: a gitops operator, Flux Helm and add their manifests to Git, so you can configure them through pull requests. The most important ingredient using eksctl enable repo is your config repository (which will include your workload manifests, etc). You can start with an empty repository and push that to Git, or use the one you intend to deploy to the cluster. Warning This is an experimental feature. To enable it, set the environment variable EKSCTL_EXPERIMENTAL=true . Experimental features are not stable and their command name and flags may change. Run this command from any directory in your file system. eksctl will clone your repository in a temporary directory that will be removed later. 1 2 3 4 5 6 EKSCTL_EXPERIMENTAL = true \\ eksctl enable repo \\ --git-url git@github.com:example/my-eks-config \\ --git-email your@email.com \\ --cluster your-cluster-name \\ --region your-cluster-region Let us go through the specified arguments one by one: --git-url : this points to a Git URL where the configuration for your cluster will be stored. This will contain config for the workloads and infrastructure later on. --git-email : the email used to commit changes to your config repository. --cluster : the name of your cluster. Use eksctl get cluster to see all clusters in your default region. --region : the region of your cluster. There are more arguments and options, please refer to the gitops reference of eksctl which details all the flags and resulting directory structure. The command will take a while to run and it's a good idea to scan the output. You will note a similar bit of information in the log like this one: 1 2 3 4 [ \u2139 ] Flux will only operate properly once it has write - access to the Git repository ... [ \u2139 ] please configure git @github . com : YOURUSER / eks - quickstart - app - dev . git so that the following Flux SSH public key has write access to it ssh - rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC8msUDG9tEEWHKKJw1o8BpwfMkCvCepeUSMa9iTVK6Bmxeu2pA / ivBS8Qgx / Lg8Jnu4Gk2RbXYMt3KL3 / lcIezLwqipGmVvLgBLvUccbBpeUpWt + SlW2LMwcMOnhF3n86VOYjaRPggoPtWfLhFIfnkvKOFLHPRYS3nqyYspFeCGUmOzQim + JAWokf4oGOOX4SNzRKjusboh93oy8fvWk8SrtSwLBWXOKu + kKXC0ecZJK7G0jW91qb40QvB + VeSAbfk8LJZcXGWWvWa3W0 / woKzGNWBPZz + pGuflUjVwQG5GoOq5VVWu71gmXoXBS3bUNqlu6nDobd2LlqiXNViaszX Copy the lines starting with ssh-rsa and give it read/write access to your repository. For example, in GitHub, by adding it as a deploy key. There you can easily do this in the Settings > Deploy keys > Add deploy key . Just make sure you check Allow write access as well. The next time Flux syncs from Git, it will start updating the cluster and actively deploying. If you run git pull next, you will see that Flux has committed them to your config repository already. In our case we are going to see these new arrivals in the cluster: flux , the Flux Helm Operator , and Tiller, running: 1 2 3 4 5 6 7 8 9 10 $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE flux flux-56b5664cdd-nfzx2 1 /1 Running 0 11m flux flux-helm-operator-6bc7c85bb5-l2nzn 1 /1 Running 0 11m flux memcached-958f745c-dqllc 1 /1 Running 0 11m flux tiller-deploy-7ccc4b4d45-w2mrt 1 /1 Running 0 11m kube-system aws-node-l49ct 1 /1 Running 0 14m kube-system coredns-7d7755744b-4jkp6 1 /1 Running 0 21m kube-system coredns-7d7755744b-ls5d9 1 /1 Running 0 21m kube-system kube-proxy-wllff 1 /1 Running 0 14m All of the cluster configuration can be easily edited in Git now. Welcome to a fully gitopsed world! Enabling a Quick Start profile \u00b6 The following command will set up your cluster with the app-dev profile, the first gitops Quick Start. All of the config files you need for a production-ready cluster will be in the git repo you have provided and those components will be deployed to your cluster. When you make changes in the configuration they will be reflected on your cluster. Warning This is an experimental feature. To enable it, set the environment variable EKSCTL_EXPERIMENTAL=true . Experimental features are not stable and their command name and flags may change. Run this command from any directory in your file system. eksctl will clone your repository in a temporary directory that will be removed later. 1 2 3 4 5 6 7 EKSCTL_EXPERIMENTAL = true eksctl \\ enable profile \\ --git-url git@github.com:example/my-eks-config \\ --git-email your@email.com \\ --cluster your-cluster-name \\ --region your-cluster-region \\ app - dev Let us go through the specified arguments one by one: --git-url : this points to a Git URL where the configuration for your cluster will be stored. This will contain config for the workloads and infrastructure later on. --git-email : the email used to commit changes to your config repository. --cluster : the name of your cluster. Use eksctl get cluster to see all clusters in your default region. --region : the region of your cluster. positional argument: this is the name of one of the profiles we put together, so you can easily pick and choose and will not have to start from scratch every time. We use app-dev here. There are more arguments and options, please refer to the gitops reference of eksctl which details all the flags and resulting directory structure. This will load gitops Quick Start manifests into your repo. It will use templating to add your cluster name and region to the configuration so that cluster components that need those values can work (e.g. alb-ingress ). In our case we are going to see these new arrivals in the cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE amazon-cloudwatch cloudwatch-agent-qtdmc 1 /1 Running 0 4m28s amazon-cloudwatch fluentd-cloudwatch-4rwwr 1 /1 Running 0 4m28s demo podinfo-75b8547f78-56dll 1 /1 Running 0 103s flux flux-56b5664cdd-nfzx2 1 /1 Running 0 11m flux flux-helm-operator-6bc7c85bb5-l2nzn 1 /1 Running 0 11m flux memcached-958f745c-dqllc 1 /1 Running 0 11m flux tiller-deploy-7ccc4b4d45-w2mrt 1 /1 Running 0 11m kube-system alb-ingress-controller-6b64bcbbd8-6l7kf 1 /1 Running 0 4m28s kube-system aws-node-l49ct 1 /1 Running 0 14m kube-system cluster-autoscaler-5b8c96cd98-26z5f 1 /1 Running 0 4m28s kube-system coredns-7d7755744b-4jkp6 1 /1 Running 0 21m kube-system coredns-7d7755744b-ls5d9 1 /1 Running 0 21m kube-system kube-proxy-wllff 1 /1 Running 0 14m kubernetes-dashboard dashboard-metrics-scraper-f7b5dbf7d-rm5z7 1 /1 Running 0 4m28s kubernetes-dashboard kubernetes-dashboard-7447f48f55-94rhg 1 /1 Running 0 4m28s monitoring alertmanager-prometheus-operator-alertmanager-0 2 /2 Running 0 78s monitoring metrics-server-7dfc675884-q9qps 1 /1 Running 0 4m24s monitoring prometheus-operator-grafana-9bb769cf-pjk4r 2 /2 Running 0 89s monitoring prometheus-operator-kube-state-metrics-79f476bff6-r9m2s 1 /1 Running 0 89s monitoring prometheus-operator-operator-58fcb66576-6dwpg 1 /1 Running 0 89s monitoring prometheus-operator-prometheus-node-exporter-tllwl 1 /1 Running 0 89s monitoring prometheus-prometheus-operator-prometheus-0 3 /3 Running 1 72s Your gitops cluster \u00b6 Welcome to your fully gitopsed cluster. By choosing the app-dev Quick Start profile, you will now also have the following components running in your cluster: ALB ingress controller -- to easily expose services to the World. Cluster autoscaler -- to automatically add/remove nodes to/from your cluster based on its usage. Prometheus (its Alertmanager , its operator , its node-exporter , kube-state-metrics , and metrics-server ) -- for powerful metrics & alerts. Grafana -- for a rich way to visualize metrics via dashboards you can create, explore, and share. Kubernetes dashboard -- Kubernetes' standard dashboard. Fluentd & Amazon's CloudWatch agent -- for cluster & containers' log collection, aggregation & analytics in CloudWatch . podinfo -- a toy demo application. It's easy to confirm if all of this is up and running for you. Let's check podinfo and see if it's up. 1 2 3 $ kubectl get service --namespace demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE podinfo ClusterIP 10 .100.255.220 <none> 9898 /TCP 2m Now let's port-forward the service, so we can easily access it: 1 kubectl port - forward - n demo svc / podinfo 9898 : 9898 If you open localhost:9898 in your browser, you will see Congratulations to your gitopsed cluster on EKS! Advanced setups \u00b6 Handcrafting your configuration \u00b6 eksctl enable profile can largely be decomposed into eksctl generate profile git commit git push In this section we will use eksctl generate profile , so you can easily handcraft your workloads' configuration locally, before pushing these to manage them in Git. During the previous call ( eksctl enable repo ), we instructed Flux to watch a repository and deploy changes to the cluster. This repository is where the workloads are defined. Now we will add the config of the infrastructure tooling as well. eksctl has the ability to use a base config for for the infrastructure tooling, also known as a Quick Start profile. So if your organisation already has experience in setting up clusters and use the same defaults, it makes sense to use those as a profile. You could be entirely starting from scratch here too. What we will do in this part of the tutorial is using weaveworks/eks-quickstart-app-dev , which is the app-dev gitops Quick Start profile. To create your own profile check out the documentation . Now please run: 1 2 3 4 EKSCTL_EXPERIMENTAL = true eksctl generate profile \\ --cluster wonderful-wardrobe-1565767990 \\ --git-url git@github.com:weaveworks/eks-quickstart-app-dev.git \\ --profile-path ~/dev/flux-get-started/cluster-config Let's break this down here. eksctl generate profile at the very least wants: --cluster : the name of the cluster - check eksctl get cluster to see what the name of yours is --git-url : the Git URL of the Quick Start profile to deploy to the cluster --profile-path : a local path: this is an empty new directory (here cluster-config ) you create in your local checkout of the config repository, which we used in the previous command The Quick Start profile can be something you and your organisation tailored to your needs, but can be something like our app-dev Quick Start profile as well. It is meant to be a starting point for clusters you can iterate over. So after all this preface, what happens when we run the command? eksctl will check out the Quick Start profile (here we use app-dev profile ) in an empty sub-directory ( cluster-config ) of our local checkout of flux-get-started . All that is left now to get our cluster components managed by Flux is to commit them to our config repository: 1 2 3 4 cd ~/ dev / flux - get - started / cluster - config git add . git commit - m \"add cluster config\" git push Flux will pick this up in its next sync and make the changes to your cluster. In our case we are going to see these new arrivals in the cluster: 1 2 3 $ kubectl get pods --namespace kubernetes-dashboard kubernetes-dashboard dashboard-metrics-scraper-f7b5dbf7d-kwz7n 1 /1 Running 0 4m kubernetes-dashboard kubernetes-dashboard-7447f48f55-2pl66 1 /1 Running 0 4m All of the cluster configuration can be easily edited in Git now. Welcome to a fully gitopsed world! Conclusion \u00b6 We look forward to hearing your thoughts and feedback. Please get in touch and let us know how things worked out for you.","title":"GitOps Quickstart"},{"location":"gitops-quickstart/#setup-your-cluster-with-gitops","text":"Welcome to eksctl gitops Quick Starts. In this guide we will show you how to launch fully-configured Kubernetes clusters that are ready to run production workloads in minutes: easy for you to get started running Kubernetes on EKS and to launch standard clusters in your organisation. At the end of this, you will have a Kubernetes cluster including control plane, worker nodes, and all of the software needed for code deployment, monitoring, and logging.","title":"Setup your cluster with gitops"},{"location":"gitops-quickstart/#quick-start-to-gitops","text":"gitops is a way to do Kubernetes application delivery. It works by using Git as a single source of truth for Kubernetes resources and everything else. With Git at the center of your delivery pipelines, you and your team can make pull requests to accelerate and simplify application deployments and operations tasks to Kubernetes. Using gitops Quick Starts will get you set up in next to no time. You will benefit from a setup that is based on the experience of companies who run workloads at scale.","title":"Quick Start to gitops"},{"location":"gitops-quickstart/#prerequisites","text":"To use EKS, you need to have your AWS account set up. Next, you will have to have the following tools installed: AWS CLI : at least 1.16.156 - older versions will require AWS IAM Authenticator to be installed too a specific version of kubectl which works with EKS","title":"Prerequisites"},{"location":"gitops-quickstart/#getting-ready-for-gitops","text":"The main point of gitops is to keep everything (config, alerts, dashboards, apps, literally everything) in Git and use it as a single source of truth. To keep your cluster configuration in Git, please go ahead and create an empty repository. On GitHub, for example, follow these steps .","title":"Getting ready for gitops"},{"location":"gitops-quickstart/#standing-up-your-cluster","text":"First we follow the usual steps to stand up a cluster on EKS. Please review the list of flags to see if you need to tweak it for your purposes. In essence it is going to be a variation of: 1 eksctl create cluster Once it is finished, you should be able to check the cluster contents and see some system workloads: 1 2 3 4 $ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-15-6.eu-central-1.compute.internal Ready <none> 39s v1.13.8-eks-cd3eb0 ip-192-168-64-189.eu-central-1.compute.internal Ready <none> 38s v1.13.8-eks-cd3eb0 1 2 3 4 5 6 7 8 $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-l8mk7 1 /1 Running 0 45s kube-system aws-node-s2p2c 1 /1 Running 0 45s kube-system coredns-7d7755744b-f88w7 1 /1 Running 0 45s kube-system coredns-7d7755744b-qgc6r 1 /1 Running 0 45s kube-system kube-proxy-kg57w 1 /1 Running 0 45s kube-system kube-proxy-qzcmk 1 /1 Running 0 45s","title":"Standing up your cluster"},{"location":"gitops-quickstart/#enabling-a-gitops-operator","text":"The following command will set up your cluster with: a gitops operator, Flux Helm and add their manifests to Git, so you can configure them through pull requests. The most important ingredient using eksctl enable repo is your config repository (which will include your workload manifests, etc). You can start with an empty repository and push that to Git, or use the one you intend to deploy to the cluster. Warning This is an experimental feature. To enable it, set the environment variable EKSCTL_EXPERIMENTAL=true . Experimental features are not stable and their command name and flags may change. Run this command from any directory in your file system. eksctl will clone your repository in a temporary directory that will be removed later. 1 2 3 4 5 6 EKSCTL_EXPERIMENTAL = true \\ eksctl enable repo \\ --git-url git@github.com:example/my-eks-config \\ --git-email your@email.com \\ --cluster your-cluster-name \\ --region your-cluster-region Let us go through the specified arguments one by one: --git-url : this points to a Git URL where the configuration for your cluster will be stored. This will contain config for the workloads and infrastructure later on. --git-email : the email used to commit changes to your config repository. --cluster : the name of your cluster. Use eksctl get cluster to see all clusters in your default region. --region : the region of your cluster. There are more arguments and options, please refer to the gitops reference of eksctl which details all the flags and resulting directory structure. The command will take a while to run and it's a good idea to scan the output. You will note a similar bit of information in the log like this one: 1 2 3 4 [ \u2139 ] Flux will only operate properly once it has write - access to the Git repository ... [ \u2139 ] please configure git @github . com : YOURUSER / eks - quickstart - app - dev . git so that the following Flux SSH public key has write access to it ssh - rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC8msUDG9tEEWHKKJw1o8BpwfMkCvCepeUSMa9iTVK6Bmxeu2pA / ivBS8Qgx / Lg8Jnu4Gk2RbXYMt3KL3 / lcIezLwqipGmVvLgBLvUccbBpeUpWt + SlW2LMwcMOnhF3n86VOYjaRPggoPtWfLhFIfnkvKOFLHPRYS3nqyYspFeCGUmOzQim + JAWokf4oGOOX4SNzRKjusboh93oy8fvWk8SrtSwLBWXOKu + kKXC0ecZJK7G0jW91qb40QvB + VeSAbfk8LJZcXGWWvWa3W0 / woKzGNWBPZz + pGuflUjVwQG5GoOq5VVWu71gmXoXBS3bUNqlu6nDobd2LlqiXNViaszX Copy the lines starting with ssh-rsa and give it read/write access to your repository. For example, in GitHub, by adding it as a deploy key. There you can easily do this in the Settings > Deploy keys > Add deploy key . Just make sure you check Allow write access as well. The next time Flux syncs from Git, it will start updating the cluster and actively deploying. If you run git pull next, you will see that Flux has committed them to your config repository already. In our case we are going to see these new arrivals in the cluster: flux , the Flux Helm Operator , and Tiller, running: 1 2 3 4 5 6 7 8 9 10 $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE flux flux-56b5664cdd-nfzx2 1 /1 Running 0 11m flux flux-helm-operator-6bc7c85bb5-l2nzn 1 /1 Running 0 11m flux memcached-958f745c-dqllc 1 /1 Running 0 11m flux tiller-deploy-7ccc4b4d45-w2mrt 1 /1 Running 0 11m kube-system aws-node-l49ct 1 /1 Running 0 14m kube-system coredns-7d7755744b-4jkp6 1 /1 Running 0 21m kube-system coredns-7d7755744b-ls5d9 1 /1 Running 0 21m kube-system kube-proxy-wllff 1 /1 Running 0 14m All of the cluster configuration can be easily edited in Git now. Welcome to a fully gitopsed world!","title":"Enabling a gitops operator"},{"location":"gitops-quickstart/#enabling-a-quick-start-profile","text":"The following command will set up your cluster with the app-dev profile, the first gitops Quick Start. All of the config files you need for a production-ready cluster will be in the git repo you have provided and those components will be deployed to your cluster. When you make changes in the configuration they will be reflected on your cluster. Warning This is an experimental feature. To enable it, set the environment variable EKSCTL_EXPERIMENTAL=true . Experimental features are not stable and their command name and flags may change. Run this command from any directory in your file system. eksctl will clone your repository in a temporary directory that will be removed later. 1 2 3 4 5 6 7 EKSCTL_EXPERIMENTAL = true eksctl \\ enable profile \\ --git-url git@github.com:example/my-eks-config \\ --git-email your@email.com \\ --cluster your-cluster-name \\ --region your-cluster-region \\ app - dev Let us go through the specified arguments one by one: --git-url : this points to a Git URL where the configuration for your cluster will be stored. This will contain config for the workloads and infrastructure later on. --git-email : the email used to commit changes to your config repository. --cluster : the name of your cluster. Use eksctl get cluster to see all clusters in your default region. --region : the region of your cluster. positional argument: this is the name of one of the profiles we put together, so you can easily pick and choose and will not have to start from scratch every time. We use app-dev here. There are more arguments and options, please refer to the gitops reference of eksctl which details all the flags and resulting directory structure. This will load gitops Quick Start manifests into your repo. It will use templating to add your cluster name and region to the configuration so that cluster components that need those values can work (e.g. alb-ingress ). In our case we are going to see these new arrivals in the cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE amazon-cloudwatch cloudwatch-agent-qtdmc 1 /1 Running 0 4m28s amazon-cloudwatch fluentd-cloudwatch-4rwwr 1 /1 Running 0 4m28s demo podinfo-75b8547f78-56dll 1 /1 Running 0 103s flux flux-56b5664cdd-nfzx2 1 /1 Running 0 11m flux flux-helm-operator-6bc7c85bb5-l2nzn 1 /1 Running 0 11m flux memcached-958f745c-dqllc 1 /1 Running 0 11m flux tiller-deploy-7ccc4b4d45-w2mrt 1 /1 Running 0 11m kube-system alb-ingress-controller-6b64bcbbd8-6l7kf 1 /1 Running 0 4m28s kube-system aws-node-l49ct 1 /1 Running 0 14m kube-system cluster-autoscaler-5b8c96cd98-26z5f 1 /1 Running 0 4m28s kube-system coredns-7d7755744b-4jkp6 1 /1 Running 0 21m kube-system coredns-7d7755744b-ls5d9 1 /1 Running 0 21m kube-system kube-proxy-wllff 1 /1 Running 0 14m kubernetes-dashboard dashboard-metrics-scraper-f7b5dbf7d-rm5z7 1 /1 Running 0 4m28s kubernetes-dashboard kubernetes-dashboard-7447f48f55-94rhg 1 /1 Running 0 4m28s monitoring alertmanager-prometheus-operator-alertmanager-0 2 /2 Running 0 78s monitoring metrics-server-7dfc675884-q9qps 1 /1 Running 0 4m24s monitoring prometheus-operator-grafana-9bb769cf-pjk4r 2 /2 Running 0 89s monitoring prometheus-operator-kube-state-metrics-79f476bff6-r9m2s 1 /1 Running 0 89s monitoring prometheus-operator-operator-58fcb66576-6dwpg 1 /1 Running 0 89s monitoring prometheus-operator-prometheus-node-exporter-tllwl 1 /1 Running 0 89s monitoring prometheus-prometheus-operator-prometheus-0 3 /3 Running 1 72s","title":"Enabling a Quick Start profile"},{"location":"gitops-quickstart/#your-gitops-cluster","text":"Welcome to your fully gitopsed cluster. By choosing the app-dev Quick Start profile, you will now also have the following components running in your cluster: ALB ingress controller -- to easily expose services to the World. Cluster autoscaler -- to automatically add/remove nodes to/from your cluster based on its usage. Prometheus (its Alertmanager , its operator , its node-exporter , kube-state-metrics , and metrics-server ) -- for powerful metrics & alerts. Grafana -- for a rich way to visualize metrics via dashboards you can create, explore, and share. Kubernetes dashboard -- Kubernetes' standard dashboard. Fluentd & Amazon's CloudWatch agent -- for cluster & containers' log collection, aggregation & analytics in CloudWatch . podinfo -- a toy demo application. It's easy to confirm if all of this is up and running for you. Let's check podinfo and see if it's up. 1 2 3 $ kubectl get service --namespace demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE podinfo ClusterIP 10 .100.255.220 <none> 9898 /TCP 2m Now let's port-forward the service, so we can easily access it: 1 kubectl port - forward - n demo svc / podinfo 9898 : 9898 If you open localhost:9898 in your browser, you will see Congratulations to your gitopsed cluster on EKS!","title":"Your gitops cluster"},{"location":"gitops-quickstart/#advanced-setups","text":"","title":"Advanced setups"},{"location":"gitops-quickstart/#handcrafting-your-configuration","text":"eksctl enable profile can largely be decomposed into eksctl generate profile git commit git push In this section we will use eksctl generate profile , so you can easily handcraft your workloads' configuration locally, before pushing these to manage them in Git. During the previous call ( eksctl enable repo ), we instructed Flux to watch a repository and deploy changes to the cluster. This repository is where the workloads are defined. Now we will add the config of the infrastructure tooling as well. eksctl has the ability to use a base config for for the infrastructure tooling, also known as a Quick Start profile. So if your organisation already has experience in setting up clusters and use the same defaults, it makes sense to use those as a profile. You could be entirely starting from scratch here too. What we will do in this part of the tutorial is using weaveworks/eks-quickstart-app-dev , which is the app-dev gitops Quick Start profile. To create your own profile check out the documentation . Now please run: 1 2 3 4 EKSCTL_EXPERIMENTAL = true eksctl generate profile \\ --cluster wonderful-wardrobe-1565767990 \\ --git-url git@github.com:weaveworks/eks-quickstart-app-dev.git \\ --profile-path ~/dev/flux-get-started/cluster-config Let's break this down here. eksctl generate profile at the very least wants: --cluster : the name of the cluster - check eksctl get cluster to see what the name of yours is --git-url : the Git URL of the Quick Start profile to deploy to the cluster --profile-path : a local path: this is an empty new directory (here cluster-config ) you create in your local checkout of the config repository, which we used in the previous command The Quick Start profile can be something you and your organisation tailored to your needs, but can be something like our app-dev Quick Start profile as well. It is meant to be a starting point for clusters you can iterate over. So after all this preface, what happens when we run the command? eksctl will check out the Quick Start profile (here we use app-dev profile ) in an empty sub-directory ( cluster-config ) of our local checkout of flux-get-started . All that is left now to get our cluster components managed by Flux is to commit them to our config repository: 1 2 3 4 cd ~/ dev / flux - get - started / cluster - config git add . git commit - m \"add cluster config\" git push Flux will pick this up in its next sync and make the changes to your cluster. In our case we are going to see these new arrivals in the cluster: 1 2 3 $ kubectl get pods --namespace kubernetes-dashboard kubernetes-dashboard dashboard-metrics-scraper-f7b5dbf7d-kwz7n 1 /1 Running 0 4m kubernetes-dashboard kubernetes-dashboard-7447f48f55-2pl66 1 /1 Running 0 4m All of the cluster configuration can be easily edited in Git now. Welcome to a fully gitopsed world!","title":"Handcrafting your configuration"},{"location":"gitops-quickstart/#conclusion","text":"We look forward to hearing your thoughts and feedback. Please get in touch and let us know how things worked out for you.","title":"Conclusion"},{"location":"intro/","text":"Getting started \u00b6 Need help? Join Weave Community Slack . To list the details about a cluster or all of the clusters, use: 1 eksctl get cluster [ --name=<name>][--region=<region>] To create the same kind of basic cluster, but with a different name, run: 1 eksctl create cluster --name=cluster-1 --nodes=4 EKS supports versions 1.12 , 1.13 , 1.14 (default) and 1.15 . With eksctl you can deploy either version by passing --version . 1 eksctl create cluster --version=1.14 To write cluster credentials to a file other than default, run: 1 eksctl create cluster --name=cluster-2 --nodes=4 --kubeconfig=./kubeconfig.cluster-2.yaml To prevent storing cluster credentials locally, run: 1 eksctl create cluster --name=cluster-3 --nodes=4 --write-kubeconfig=false To let eksctl manage cluster credentials under ~/.kube/eksctl/clusters directory, run: 1 eksctl create cluster --name=cluster-3 --nodes=4 --auto-kubeconfig To obtain cluster credentials at any point in time, run: 1 eksctl utils write - kubeconfig --cluster=<name> [--kubeconfig=<path>][--set-kubeconfig-context=<bool>] To use a 3-5 node Auto Scaling Group, run: 1 eksctl create cluster --name=cluster-5 --nodes-min=3 --nodes-max=5 NOTE: You will still need to install and configure Auto Scaling. See the \"Enable Auto Scaling\" section below. Also note that depending on your workloads you might need to use a separate nodegroup for each AZ. See Zone-aware Auto Scaling below for more info. To use 30 c4.xlarge nodes and prevent updating current context in ~/.kube/config , run: 1 eksctl create cluster --name=cluster-6 --nodes=30 --node-type=c4.xlarge --set-kubeconfig-context=false In order to allow SSH access to nodes, eksctl imports ~/.ssh/id_rsa.pub by default, to use a different SSH public key, e.g. my_eks_node_id.pub , run: 1 eksctl create cluster --ssh-access --ssh-public-key=my_eks_node_id.pub To use a pre-existing EC2 key pair in us-east-1 region, you can specify key pair name (which must not resolve to a local file path), e.g. to use my_kubernetes_key run: 1 eksctl create cluster --ssh-access --ssh-public-key=my_kubernetes_key --region=us-east-1 To add custom tags for all resources, use --tags . NOTE: Until #25 is resolved, tags cannot be applied to EKS cluster itself, but most of other resources (e.g. EC2 nodes). 1 eksctl create cluster --tags environment=staging --region=us-east-1 To configure node root volume, use the --node-volume-size (and optionally --node-volume-type ), e.g.: 1 eksctl create cluster --node-volume-size=50 --node-volume-type=io1 NOTE: In us-east-1 you are likely to get UnsupportedAvailabilityZoneException . If you do, copy the suggested zones and pass --zones flag, e.g. eksctl create cluster --region=us-east-1 --zones=us-east-1a,us-east-1b,us-east-1d . This may occur in other regions, but less likely. You shouldn't need to use --zone flag otherwise. You can also create a cluster passing all configuration information in a file using --config-file : 1 eksctl create cluster --config-file=<path> To create a cluster using a configuration file and skip creating nodegroups until later: 1 eksctl create cluster --config-file=<path> --without-nodegroup To delete a cluster, run: 1 eksctl delete cluster --name=<name> [--region=<region>] NOTE: Cluster info will be cleaned up in kubernetes config file. Please run kubectl config get-contexts to select right context. Contributions \u00b6 Code contributions are very welcome. If you are interested in helping make eksctl great then see our contributing guide . Installation \u00b6 To download the latest release, run: 1 2 curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp sudo mv / tmp / eksctl / usr / local / bin Alternatively, macOS users can use Homebrew : 1 2 brew tap weaveworks / tap brew install weaveworks / tap / eksctl or MacPorts : 1 port install eksctl and Windows users can use chocolatey : 1 chocolatey install eksctl You will need to have AWS API credentials configured. What works for AWS CLI or any other tools (kops, Terraform etc), should be sufficient. You can use ~/.aws/credentials file or environment variables . For more information read AWS documentation . You will also need AWS IAM Authenticator for Kubernetes command (either aws-iam-authenticator or aws eks get-token (available in version 1.16.156 or greater of AWS CLI) in your PATH . Shell Completion \u00b6 To enable bash completion, run the following, or put it in ~/.bashrc or ~/.profile : 1 . < ( eksctl completion bash ) Or for zsh, run: 1 2 mkdir - p ~/ . zsh / completion / eksctl completion zsh > ~/ . zsh / completion / _eksctl and put the following in ~/.zshrc : 1 fpath = ( $ fpath ~/ . zsh / completion ) Note if you're not running a distribution like oh-my-zsh you may first have to enable autocompletion: 1 2 autoload - U compinit compinit To make the above persistent, run the first two lines, and put the above in ~/.zshrc . Feature list \u00b6 The features that are currently implemented are: Create, get, list and delete clusters Create, drain and delete nodegroups Scale a nodegroup Update a cluster Use custom AMIs Configure VPC Networking Configure accesss to API endpoints Support for GPU nodegroups Spot instances and mixed instances IAM Management and Add-on Policies List cluster Cloudformation stacks Install coredns Write kubeconfig file for a cluster","title":"Introduction"},{"location":"intro/#getting-started","text":"Need help? Join Weave Community Slack . To list the details about a cluster or all of the clusters, use: 1 eksctl get cluster [ --name=<name>][--region=<region>] To create the same kind of basic cluster, but with a different name, run: 1 eksctl create cluster --name=cluster-1 --nodes=4 EKS supports versions 1.12 , 1.13 , 1.14 (default) and 1.15 . With eksctl you can deploy either version by passing --version . 1 eksctl create cluster --version=1.14 To write cluster credentials to a file other than default, run: 1 eksctl create cluster --name=cluster-2 --nodes=4 --kubeconfig=./kubeconfig.cluster-2.yaml To prevent storing cluster credentials locally, run: 1 eksctl create cluster --name=cluster-3 --nodes=4 --write-kubeconfig=false To let eksctl manage cluster credentials under ~/.kube/eksctl/clusters directory, run: 1 eksctl create cluster --name=cluster-3 --nodes=4 --auto-kubeconfig To obtain cluster credentials at any point in time, run: 1 eksctl utils write - kubeconfig --cluster=<name> [--kubeconfig=<path>][--set-kubeconfig-context=<bool>] To use a 3-5 node Auto Scaling Group, run: 1 eksctl create cluster --name=cluster-5 --nodes-min=3 --nodes-max=5 NOTE: You will still need to install and configure Auto Scaling. See the \"Enable Auto Scaling\" section below. Also note that depending on your workloads you might need to use a separate nodegroup for each AZ. See Zone-aware Auto Scaling below for more info. To use 30 c4.xlarge nodes and prevent updating current context in ~/.kube/config , run: 1 eksctl create cluster --name=cluster-6 --nodes=30 --node-type=c4.xlarge --set-kubeconfig-context=false In order to allow SSH access to nodes, eksctl imports ~/.ssh/id_rsa.pub by default, to use a different SSH public key, e.g. my_eks_node_id.pub , run: 1 eksctl create cluster --ssh-access --ssh-public-key=my_eks_node_id.pub To use a pre-existing EC2 key pair in us-east-1 region, you can specify key pair name (which must not resolve to a local file path), e.g. to use my_kubernetes_key run: 1 eksctl create cluster --ssh-access --ssh-public-key=my_kubernetes_key --region=us-east-1 To add custom tags for all resources, use --tags . NOTE: Until #25 is resolved, tags cannot be applied to EKS cluster itself, but most of other resources (e.g. EC2 nodes). 1 eksctl create cluster --tags environment=staging --region=us-east-1 To configure node root volume, use the --node-volume-size (and optionally --node-volume-type ), e.g.: 1 eksctl create cluster --node-volume-size=50 --node-volume-type=io1 NOTE: In us-east-1 you are likely to get UnsupportedAvailabilityZoneException . If you do, copy the suggested zones and pass --zones flag, e.g. eksctl create cluster --region=us-east-1 --zones=us-east-1a,us-east-1b,us-east-1d . This may occur in other regions, but less likely. You shouldn't need to use --zone flag otherwise. You can also create a cluster passing all configuration information in a file using --config-file : 1 eksctl create cluster --config-file=<path> To create a cluster using a configuration file and skip creating nodegroups until later: 1 eksctl create cluster --config-file=<path> --without-nodegroup To delete a cluster, run: 1 eksctl delete cluster --name=<name> [--region=<region>] NOTE: Cluster info will be cleaned up in kubernetes config file. Please run kubectl config get-contexts to select right context.","title":"Getting started"},{"location":"intro/#contributions","text":"Code contributions are very welcome. If you are interested in helping make eksctl great then see our contributing guide .","title":"Contributions"},{"location":"intro/#installation","text":"To download the latest release, run: 1 2 curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp sudo mv / tmp / eksctl / usr / local / bin Alternatively, macOS users can use Homebrew : 1 2 brew tap weaveworks / tap brew install weaveworks / tap / eksctl or MacPorts : 1 port install eksctl and Windows users can use chocolatey : 1 chocolatey install eksctl You will need to have AWS API credentials configured. What works for AWS CLI or any other tools (kops, Terraform etc), should be sufficient. You can use ~/.aws/credentials file or environment variables . For more information read AWS documentation . You will also need AWS IAM Authenticator for Kubernetes command (either aws-iam-authenticator or aws eks get-token (available in version 1.16.156 or greater of AWS CLI) in your PATH .","title":"Installation"},{"location":"intro/#shell-completion","text":"To enable bash completion, run the following, or put it in ~/.bashrc or ~/.profile : 1 . < ( eksctl completion bash ) Or for zsh, run: 1 2 mkdir - p ~/ . zsh / completion / eksctl completion zsh > ~/ . zsh / completion / _eksctl and put the following in ~/.zshrc : 1 fpath = ( $ fpath ~/ . zsh / completion ) Note if you're not running a distribution like oh-my-zsh you may first have to enable autocompletion: 1 2 autoload - U compinit compinit To make the above persistent, run the first two lines, and put the above in ~/.zshrc .","title":"Shell Completion"},{"location":"intro/#feature-list","text":"The features that are currently implemented are: Create, get, list and delete clusters Create, drain and delete nodegroups Scale a nodegroup Update a cluster Use custom AMIs Configure VPC Networking Configure accesss to API endpoints Support for GPU nodegroups Spot instances and mixed instances IAM Management and Add-on Policies List cluster Cloudformation stacks Install coredns Write kubeconfig file for a cluster","title":"Feature list"},{"location":"examples/reusing-iam-and-vpc/","text":"Custom IAM and VPC config \u00b6 This example shows how to create a cluster reusing pre-existing IAM and VPC resources: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : my-test region : us-east-1 vpc : id : \"vpc-11111\" cidr : \"152.28.0.0/16\" subnets : private : us-east-1d : id : \"subnet-1111\" cidr : \"152.28.152.0/21\" us-east-1c : id : \"subnet-11112\" cidr : \"152.28.144.0/21\" us-east-1a : id : \"subnet-11113\" cidr : \"152.28.136.0/21\" iam : serviceRoleARN : \"arn:aws:iam::11111:role/eks-base-service-role\" nodeGroups : - name : ng-1 instanceType : m5.large desiredCapacity : 3 iam : instanceProfileARN : \"arn:aws:iam::11111:instance-profile/eks-nodes-base-role\" instanceRoleARN : \"arn:aws:iam::1111:role/eks-nodes-base-role\" privateNetworking : true securityGroups : withShared : true withLocal : true attachIDs : [ 'sg-11111' , 'sg-11112' ] ssh : publicKeyName : 'my-instance-key' tags : 'environment:basedomain' : 'example.org' managedNodeGroups : - name : managed-1 instanceType : m5.large minSize : 2 desiredCapacity : 3 maxSize : 4 availabilityZones : [ \"us-west-2a\" , \"us-west-2b\" ] volumeSize : 20 ssh : allow : false labels : { role : worker } tags : 'environment:basedomain' : 'example.org' iam : instanceRoleARN : \"arn:aws:iam::1111:role/eks-nodes-base-role\" withAddonPolicies : externalDNS : true certManager : true","title":"Custom IAM and VPC config"},{"location":"examples/reusing-iam-and-vpc/#custom-iam-and-vpc-config","text":"This example shows how to create a cluster reusing pre-existing IAM and VPC resources: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : my-test region : us-east-1 vpc : id : \"vpc-11111\" cidr : \"152.28.0.0/16\" subnets : private : us-east-1d : id : \"subnet-1111\" cidr : \"152.28.152.0/21\" us-east-1c : id : \"subnet-11112\" cidr : \"152.28.144.0/21\" us-east-1a : id : \"subnet-11113\" cidr : \"152.28.136.0/21\" iam : serviceRoleARN : \"arn:aws:iam::11111:role/eks-base-service-role\" nodeGroups : - name : ng-1 instanceType : m5.large desiredCapacity : 3 iam : instanceProfileARN : \"arn:aws:iam::11111:instance-profile/eks-nodes-base-role\" instanceRoleARN : \"arn:aws:iam::1111:role/eks-nodes-base-role\" privateNetworking : true securityGroups : withShared : true withLocal : true attachIDs : [ 'sg-11111' , 'sg-11112' ] ssh : publicKeyName : 'my-instance-key' tags : 'environment:basedomain' : 'example.org' managedNodeGroups : - name : managed-1 instanceType : m5.large minSize : 2 desiredCapacity : 3 maxSize : 4 availabilityZones : [ \"us-west-2a\" , \"us-west-2b\" ] volumeSize : 20 ssh : allow : false labels : { role : worker } tags : 'environment:basedomain' : 'example.org' iam : instanceRoleARN : \"arn:aws:iam::1111:role/eks-nodes-base-role\" withAddonPolicies : externalDNS : true certManager : true","title":"Custom IAM and VPC config"},{"location":"usage/autoscaling/","text":"Auto Scaling \u00b6 Enable Auto Scaling \u00b6 You can create a cluster (or nodegroup in an existing cluster) with IAM role that will allow use of cluster autoscaler : 1 eksctl create cluster --asg-access Once cluster is running, you will need to install cluster autoscaler itself. This flag also sets k8s.io/cluster-autoscaler/enabled and k8s.io/cluster-autoscaler/<clusterName> tags, so nodegroup discovery should work. Scaling up from 0 \u00b6 If you'd like to be able to scale your node group up from 0 and you have labels and/or taints defined on your nodegroups you'll need corresponding tags on your ASGs. You can do this with the tags key on your node group definitions. For example, given a node group with the following labels and taints: 1 2 3 4 5 6 7 nodeGroups : - name : ng1-public ... labels : my-cool-label : pizza taints : feaster : \"true:NoSchedule\" You would need to add the following ASG tags: 1 2 3 4 5 6 7 8 9 10 nodeGroups : - name : ng1-public ... labels : my-cool-label : pizza taints : feaster : \"true:NoSchedule\" tags : k8s.io/cluster-autoscaler/node-template/label/my-cool-label : pizza k8s.io/cluster-autoscaler/node-template/taint/feaster : \"true:NoSchedule\" You can read more about this here and here . Zone-aware Auto Scaling \u00b6 If your workloads are zone-specific you'll need to create separate nodegroups for each zone. This is because the cluster-autoscaler assumes that all nodes in a group are exactly equivalent. So, for example, if a scale-up event is triggered by a pod which needs a zone-specific PVC (e.g. an EBS volume), the new node might get scheduled in the wrong AZ and the pod will fail to start. You won't need a separate nodegroup for each AZ if your environment meets the following criteria: No zone-specific storage requirements. No required podAffinity with topology other than host. No required nodeAffinity on zone label. No nodeSelector on a zone label. (Read more here and here .) If you meet all of the above requirements (and possibly others) then you should be safe with a single nodegroup which spans multiple AZs. Otherwise you'll want to create separate, single-AZ nodegroups: BEFORE: 1 2 3 4 nodeGroups : - name : ng1-public instanceType : m5.xlarge # availabilityZones: [\"eu-west-2a\", \"eu-west-2b\"] AFTER: 1 2 3 4 5 6 7 nodeGroups : - name : ng1-public-2a instanceType : m5.xlarge availabilityZones : [ \"eu-west-2a\" ] - name : ng1-public-2b instanceType : m5.xlarge availabilityZones : [ \"eu-west-2b\" ]","title":"Auto Scaling"},{"location":"usage/autoscaling/#auto-scaling","text":"","title":"Auto Scaling"},{"location":"usage/autoscaling/#enable-auto-scaling","text":"You can create a cluster (or nodegroup in an existing cluster) with IAM role that will allow use of cluster autoscaler : 1 eksctl create cluster --asg-access Once cluster is running, you will need to install cluster autoscaler itself. This flag also sets k8s.io/cluster-autoscaler/enabled and k8s.io/cluster-autoscaler/<clusterName> tags, so nodegroup discovery should work.","title":"Enable Auto Scaling"},{"location":"usage/autoscaling/#scaling-up-from-0","text":"If you'd like to be able to scale your node group up from 0 and you have labels and/or taints defined on your nodegroups you'll need corresponding tags on your ASGs. You can do this with the tags key on your node group definitions. For example, given a node group with the following labels and taints: 1 2 3 4 5 6 7 nodeGroups : - name : ng1-public ... labels : my-cool-label : pizza taints : feaster : \"true:NoSchedule\" You would need to add the following ASG tags: 1 2 3 4 5 6 7 8 9 10 nodeGroups : - name : ng1-public ... labels : my-cool-label : pizza taints : feaster : \"true:NoSchedule\" tags : k8s.io/cluster-autoscaler/node-template/label/my-cool-label : pizza k8s.io/cluster-autoscaler/node-template/taint/feaster : \"true:NoSchedule\" You can read more about this here and here .","title":"Scaling up from 0"},{"location":"usage/autoscaling/#zone-aware-auto-scaling","text":"If your workloads are zone-specific you'll need to create separate nodegroups for each zone. This is because the cluster-autoscaler assumes that all nodes in a group are exactly equivalent. So, for example, if a scale-up event is triggered by a pod which needs a zone-specific PVC (e.g. an EBS volume), the new node might get scheduled in the wrong AZ and the pod will fail to start. You won't need a separate nodegroup for each AZ if your environment meets the following criteria: No zone-specific storage requirements. No required podAffinity with topology other than host. No required nodeAffinity on zone label. No nodeSelector on a zone label. (Read more here and here .) If you meet all of the above requirements (and possibly others) then you should be safe with a single nodegroup which spans multiple AZs. Otherwise you'll want to create separate, single-AZ nodegroups: BEFORE: 1 2 3 4 nodeGroups : - name : ng1-public instanceType : m5.xlarge # availabilityZones: [\"eu-west-2a\", \"eu-west-2b\"] AFTER: 1 2 3 4 5 6 7 nodeGroups : - name : ng1-public-2a instanceType : m5.xlarge availabilityZones : [ \"eu-west-2a\" ] - name : ng1-public-2b instanceType : m5.xlarge availabilityZones : [ \"eu-west-2b\" ]","title":"Zone-aware Auto Scaling"},{"location":"usage/cloudwatch-cluster-logging/","text":"CloudWatch logging \u00b6 CloudWatch logging for EKS control plane is not enabled by default due to data ingestion and storage costs. To enable control plane logging when cluster is created, you will need to define cloudWatch.clusterLogging.enableTypes setting in your ClusterConfig (see below for examples). So if you have a config file with correct cloudWatch.clusterLogging.enableTypes setting, you can create a cluster with eksctl create cluster --config-file=<path> . If you have created a cluster already, you can use eksctl utils update-cluster-logging . NOTE : this command runs in plan mode by default, you will need to specify --approve flag to apply the changes to your cluster. If you are using a config file, run: 1 eksctl utils update - cluster - logging --config-file=<path> --enable-types all Alternatively, you can use CLI flags. To enable all types of logs, run: 1 eksctl utils update - cluster - logging --enable-types all To enable audit logs, run: 1 eksctl utils update - cluster - logging --enable-types audit To enable all but controllerManager logs, run: 1 eksctl utils update - cluster - logging --enable-types=all --disable-types=controllerManager If the api and scheduler log types were already enabled, to disable scheduler and enable controllerManager at the same time, run: 1 eksctl utils update - cluster - logging --enable-types=controllerManager --disable-types=scheduler This will leave api and controllerManager as the only log types enabled. To disable all types of logs, run: 1 eksctl utils update - cluster - logging --disable-types all ClusterConfig Examples \u00b6 There 5 types of logs that you may wish to enable (see EKS documentation for more details): api audit authenticator controllerManager scheduler You can enable all types with \"*\" or \"all\" , i.e.: 1 2 3 cloudWatch : clusterLogging : enableTypes : [ \"*\" ] To disable all types, use [] or remove cloudWatch section completely. You can enable a subset of types by listing the types you want to enable: 1 2 3 4 5 cloudWatch : clusterLogging : enableTypes : - \"audit\" - \"authenticator\" Full example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-11 region : eu-west-2 nodeGroups : - name : ng-1 instanceType : m5.large desiredCapacity : 1 cloudWatch : clusterLogging : enableTypes : [ \"audit\" , \"authenticator\" ]","title":"CloudWatch logging"},{"location":"usage/cloudwatch-cluster-logging/#cloudwatch-logging","text":"CloudWatch logging for EKS control plane is not enabled by default due to data ingestion and storage costs. To enable control plane logging when cluster is created, you will need to define cloudWatch.clusterLogging.enableTypes setting in your ClusterConfig (see below for examples). So if you have a config file with correct cloudWatch.clusterLogging.enableTypes setting, you can create a cluster with eksctl create cluster --config-file=<path> . If you have created a cluster already, you can use eksctl utils update-cluster-logging . NOTE : this command runs in plan mode by default, you will need to specify --approve flag to apply the changes to your cluster. If you are using a config file, run: 1 eksctl utils update - cluster - logging --config-file=<path> --enable-types all Alternatively, you can use CLI flags. To enable all types of logs, run: 1 eksctl utils update - cluster - logging --enable-types all To enable audit logs, run: 1 eksctl utils update - cluster - logging --enable-types audit To enable all but controllerManager logs, run: 1 eksctl utils update - cluster - logging --enable-types=all --disable-types=controllerManager If the api and scheduler log types were already enabled, to disable scheduler and enable controllerManager at the same time, run: 1 eksctl utils update - cluster - logging --enable-types=controllerManager --disable-types=scheduler This will leave api and controllerManager as the only log types enabled. To disable all types of logs, run: 1 eksctl utils update - cluster - logging --disable-types all","title":"CloudWatch logging"},{"location":"usage/cloudwatch-cluster-logging/#clusterconfig-examples","text":"There 5 types of logs that you may wish to enable (see EKS documentation for more details): api audit authenticator controllerManager scheduler You can enable all types with \"*\" or \"all\" , i.e.: 1 2 3 cloudWatch : clusterLogging : enableTypes : [ \"*\" ] To disable all types, use [] or remove cloudWatch section completely. You can enable a subset of types by listing the types you want to enable: 1 2 3 4 5 cloudWatch : clusterLogging : enableTypes : - \"audit\" - \"authenticator\" Full example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-11 region : eu-west-2 nodeGroups : - name : ng-1 instanceType : m5.large desiredCapacity : 1 cloudWatch : clusterLogging : enableTypes : [ \"audit\" , \"authenticator\" ]","title":"ClusterConfig Examples"},{"location":"usage/cluster-upgrade/","text":"Cluster Upgrades \u00b6 An eksctl -managed cluster can be upgraded in 3 easy steps: update control plane version with eksctl update cluster update default add-ons: kube-proxy aws-node coredns replace each of the nodegroups by creating a new one and deleting the old one Please make sure to read this section in full before you proceed. Info Kubernetes supports version drift of up to two minor versions during upgrade process. So nodes can be up to two minor versions ahead or behind the control plane version. You can only upgrade the control plane one minor version at a time, but nodes can be upgraded more than one minor version at a time, provided the nodes stay within two minor versions of the control plane. Updating control plane version \u00b6 Control plane version updates must be done for one minor version at a time. To update control plane to the next available version run: 1 eksctl update cluster --name=<clusterName> This command will not apply any changes right away, you will need to re-run it with --approve to apply the changes. Updating nodegroups \u00b6 You should update nodegroups only after you ran eksctl update cluster . If you have a simple cluster with just an initial nodegroup (i.e. created with eksctl create cluster ), the process is very simple. Get the name of old nodegroup: 1 eksctl get nodegroups --cluster=<clusterName> NOTE: you should see only one nodegroup here, if you see more - read the next section Create new nodegroup: 1 eksctl create nodegroup --cluster=<clusterName> Delete old nodegroup: 1 eksctl delete nodegroup --cluster=<clusterName> --name=<oldNodeGroupName> Note This will drain all pods from that nodegroup before the instances are deleted. Updating multiple nodegroups \u00b6 If you have multiple nodegroups, it's your responsibility to track how each one was configured. You can do this by using config files, but if you haven't used it already, you will need to inspect your cluster to find out how each nodegroup was configured. In general terms, you are looking to: review nodegroups you have and which ones can be deleted or must be replaced for the new version note down configuration of each nodegroup, consider using config file to ease upgrades next time To create a new nodegroup: 1 eksctl create nodegroup --cluster=<clusterName> --name=<newNodeGroupName> To delete old nodegroup: 1 eksctl delete nodegroup --cluster=<clusterName> --name=<oldNodeGroupName> Updating multiple nodegroups with config file \u00b6 If you are using config file, you will need to do the following. Edit config file to add new nodegroups, and remove old nodegroups. If you just want to update nodegroups and keep the same configuration, you can just change nodegroup names, e.g. append -v2 to the name. To create all of new nodegroups defined in the config file, run: 1 eksctl create nodegroup --config-file=<path> Once you have new nodegroups in place, you can delete old ones: 1 eksctl delete nodegroup --config-file=<path> --only-missing Note First run is in plan mode, if you are happy with the proposed changes, re-run with --approve . Updating default add-ons \u00b6 There are 3 default add-ons that get included in each EKS cluster, the process for updating each of them is different, hence there are 3 distinct commands that you will need to run. Info All of the following commands accept --config-file . Note By default each of these commands runs in plan mode, if you are happy with the proposed changes, re-run with --approve . To update kube-proxy , run: 1 eksctl utils update - kube - proxy To update aws-node , run: 1 eksctl utils update - aws - node To update coredns , run: 1 eksctl utils update - coredns Once upgraded, be sure to run kubectl get pods -n kube-system and check if all addon pods are in ready state, you should see something like this: 1 2 3 4 5 6 7 NAME READY STATUS RESTARTS AGE aws - node - g5ghn 1 / 1 Running 0 2 m aws - node - zfc9s 1 / 1 Running 0 2 m coredns - 7 bcbfc4774 - g6gg8 1 / 1 Running 0 1 m coredns - 7 bcbfc4774 - hftng 1 / 1 Running 0 1 m kube - proxy - djkp7 1 / 1 Running 0 3 m kube - proxy - mpdsp 1 / 1 Running 0 3 m","title":"Cluster Upgrades"},{"location":"usage/cluster-upgrade/#cluster-upgrades","text":"An eksctl -managed cluster can be upgraded in 3 easy steps: update control plane version with eksctl update cluster update default add-ons: kube-proxy aws-node coredns replace each of the nodegroups by creating a new one and deleting the old one Please make sure to read this section in full before you proceed. Info Kubernetes supports version drift of up to two minor versions during upgrade process. So nodes can be up to two minor versions ahead or behind the control plane version. You can only upgrade the control plane one minor version at a time, but nodes can be upgraded more than one minor version at a time, provided the nodes stay within two minor versions of the control plane.","title":"Cluster Upgrades"},{"location":"usage/cluster-upgrade/#updating-control-plane-version","text":"Control plane version updates must be done for one minor version at a time. To update control plane to the next available version run: 1 eksctl update cluster --name=<clusterName> This command will not apply any changes right away, you will need to re-run it with --approve to apply the changes.","title":"Updating control plane version"},{"location":"usage/cluster-upgrade/#updating-nodegroups","text":"You should update nodegroups only after you ran eksctl update cluster . If you have a simple cluster with just an initial nodegroup (i.e. created with eksctl create cluster ), the process is very simple. Get the name of old nodegroup: 1 eksctl get nodegroups --cluster=<clusterName> NOTE: you should see only one nodegroup here, if you see more - read the next section Create new nodegroup: 1 eksctl create nodegroup --cluster=<clusterName> Delete old nodegroup: 1 eksctl delete nodegroup --cluster=<clusterName> --name=<oldNodeGroupName> Note This will drain all pods from that nodegroup before the instances are deleted.","title":"Updating nodegroups"},{"location":"usage/cluster-upgrade/#updating-multiple-nodegroups","text":"If you have multiple nodegroups, it's your responsibility to track how each one was configured. You can do this by using config files, but if you haven't used it already, you will need to inspect your cluster to find out how each nodegroup was configured. In general terms, you are looking to: review nodegroups you have and which ones can be deleted or must be replaced for the new version note down configuration of each nodegroup, consider using config file to ease upgrades next time To create a new nodegroup: 1 eksctl create nodegroup --cluster=<clusterName> --name=<newNodeGroupName> To delete old nodegroup: 1 eksctl delete nodegroup --cluster=<clusterName> --name=<oldNodeGroupName>","title":"Updating multiple nodegroups"},{"location":"usage/cluster-upgrade/#updating-multiple-nodegroups-with-config-file","text":"If you are using config file, you will need to do the following. Edit config file to add new nodegroups, and remove old nodegroups. If you just want to update nodegroups and keep the same configuration, you can just change nodegroup names, e.g. append -v2 to the name. To create all of new nodegroups defined in the config file, run: 1 eksctl create nodegroup --config-file=<path> Once you have new nodegroups in place, you can delete old ones: 1 eksctl delete nodegroup --config-file=<path> --only-missing Note First run is in plan mode, if you are happy with the proposed changes, re-run with --approve .","title":"Updating multiple nodegroups with config file"},{"location":"usage/cluster-upgrade/#updating-default-add-ons","text":"There are 3 default add-ons that get included in each EKS cluster, the process for updating each of them is different, hence there are 3 distinct commands that you will need to run. Info All of the following commands accept --config-file . Note By default each of these commands runs in plan mode, if you are happy with the proposed changes, re-run with --approve . To update kube-proxy , run: 1 eksctl utils update - kube - proxy To update aws-node , run: 1 eksctl utils update - aws - node To update coredns , run: 1 eksctl utils update - coredns Once upgraded, be sure to run kubectl get pods -n kube-system and check if all addon pods are in ready state, you should see something like this: 1 2 3 4 5 6 7 NAME READY STATUS RESTARTS AGE aws - node - g5ghn 1 / 1 Running 0 2 m aws - node - zfc9s 1 / 1 Running 0 2 m coredns - 7 bcbfc4774 - g6gg8 1 / 1 Running 0 1 m coredns - 7 bcbfc4774 - hftng 1 / 1 Running 0 1 m kube - proxy - djkp7 1 / 1 Running 0 3 m kube - proxy - mpdsp 1 / 1 Running 0 3 m","title":"Updating default add-ons"},{"location":"usage/creating-and-managing-clusters/","text":"Creating and managing clusters \u00b6 Creating a cluster \u00b6 Create a simple cluster with the following command: 1 eksctl create cluster That will create an EKS cluster in your default region (as specified by your AWS CLI configuration) with one nodegroup containing 2 m5.large nodes. After the cluster has been created, the appropriate kubernetes configuration will be added to your kubeconfig file. This is, the file that you have configured in the environment variable KUBECONFIG or ~/.kube/config by default. The path to the kubeconfig file can be overridden using the --kubeconfig flag. Other flags that can change how the kubeconfig file is written: flag type use default value --kubeconfig string path to write kubeconfig (incompatible with \u2013auto-kubeconfig) $KUBECONFIG or ~/.kube/config --set-kubeconfig-context bool if true then current-context will be set in kubeconfig; if a context is already set then it will be overwritten true --auto-kubeconfig bool save kubeconfig file by cluster name true --write-kubeconfig bool toggle writing of kubeconfig true Using Config Files \u00b6 You can create a cluster using a config file instead of flags. First, create cluster.yaml file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : basic-cluster region : eu-north-1 nodeGroups : - name : ng-1 instanceType : m5.large desiredCapacity : 10 ssh : allow : true # will use ~/.ssh/id_rsa.pub as the default ssh key - name : ng-2 instanceType : m5.xlarge desiredCapacity : 2 ssh : publicKeyPath : ~/.ssh/ec2_id_rsa.pub Next, run this command: 1 eksctl create cluster - f cluster . yaml This will create a cluster as described. If you needed to use an existing VPC, you can use a config file like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-in-existing-vpc region : eu-north-1 vpc : subnets : private : eu-north-1a : { id : subnet-0ff156e0c4a6d300c } eu-north-1b : { id : subnet-0549cdab573695c03 } eu-north-1c : { id : subnet-0426fb4a607393184 } nodeGroups : - name : ng-1-workers labels : { role : workers } instanceType : m5.xlarge desiredCapacity : 10 privateNetworking : true - name : ng-2-builders labels : { role : builders } instanceType : m5.2xlarge desiredCapacity : 2 privateNetworking : true iam : withAddonPolicies : imageBuilder : true To delete this cluster, run: 1 eksctl delete cluster - f cluster . yaml Note Without the --wait flag, this will only issue a delete operation to the cluster's CloudFormation stack and won't wait for its deletion. In some cases, AWS resources using the cluster or its VPC may cause cluster deletion to fail. To ensure any deletion errors are propagated in eksctl delete cluster , the --wait flag must be used. See examples/ directory for more sample config files.","title":"Creating and managing clusters"},{"location":"usage/creating-and-managing-clusters/#creating-and-managing-clusters","text":"","title":"Creating and managing clusters"},{"location":"usage/creating-and-managing-clusters/#creating-a-cluster","text":"Create a simple cluster with the following command: 1 eksctl create cluster That will create an EKS cluster in your default region (as specified by your AWS CLI configuration) with one nodegroup containing 2 m5.large nodes. After the cluster has been created, the appropriate kubernetes configuration will be added to your kubeconfig file. This is, the file that you have configured in the environment variable KUBECONFIG or ~/.kube/config by default. The path to the kubeconfig file can be overridden using the --kubeconfig flag. Other flags that can change how the kubeconfig file is written: flag type use default value --kubeconfig string path to write kubeconfig (incompatible with \u2013auto-kubeconfig) $KUBECONFIG or ~/.kube/config --set-kubeconfig-context bool if true then current-context will be set in kubeconfig; if a context is already set then it will be overwritten true --auto-kubeconfig bool save kubeconfig file by cluster name true --write-kubeconfig bool toggle writing of kubeconfig true","title":"Creating a cluster"},{"location":"usage/creating-and-managing-clusters/#using-config-files","text":"You can create a cluster using a config file instead of flags. First, create cluster.yaml file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : basic-cluster region : eu-north-1 nodeGroups : - name : ng-1 instanceType : m5.large desiredCapacity : 10 ssh : allow : true # will use ~/.ssh/id_rsa.pub as the default ssh key - name : ng-2 instanceType : m5.xlarge desiredCapacity : 2 ssh : publicKeyPath : ~/.ssh/ec2_id_rsa.pub Next, run this command: 1 eksctl create cluster - f cluster . yaml This will create a cluster as described. If you needed to use an existing VPC, you can use a config file like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-in-existing-vpc region : eu-north-1 vpc : subnets : private : eu-north-1a : { id : subnet-0ff156e0c4a6d300c } eu-north-1b : { id : subnet-0549cdab573695c03 } eu-north-1c : { id : subnet-0426fb4a607393184 } nodeGroups : - name : ng-1-workers labels : { role : workers } instanceType : m5.xlarge desiredCapacity : 10 privateNetworking : true - name : ng-2-builders labels : { role : builders } instanceType : m5.2xlarge desiredCapacity : 2 privateNetworking : true iam : withAddonPolicies : imageBuilder : true To delete this cluster, run: 1 eksctl delete cluster - f cluster . yaml Note Without the --wait flag, this will only issue a delete operation to the cluster's CloudFormation stack and won't wait for its deletion. In some cases, AWS resources using the cluster or its VPC may cause cluster deletion to fail. To ensure any deletion errors are propagated in eksctl delete cluster , the --wait flag must be used. See examples/ directory for more sample config files.","title":"Using Config Files"},{"location":"usage/custom-ami-support/","text":"Custom AMI Support \u00b6 With the 0.1.2 release we have introduced the --node-ami flag for use when creating a cluster. This enables a number of advanced use cases such as using a custom AMI or querying AWS in realtime to determine which AMI to use (non-GPU and GPU instances). The --node-ami can take the AMI image id for an image to explicitly use. It also can take the following 'special' keywords: Keyword Description static Indicates that the AMI images ids embedded into eksctl should be used. This relates to the static resolvers. auto Indicates that the AMI to use for the nodes should be found by querying AWS EC2. This relates to the auto resolver. auto-ssm Indicates that the AMI to use for the nodes should be found by querying AWS SSM Parameter Store. If, for example, AWS release a new version of the EKS node AMIs and a new version of eksctl hasn't been released you can use the latest AMI by doing the following: 1 eksctl create cluster --node-ami=auto With the 0.1.9 release we have introduced the --node-ami-family flag for use when creating the cluster. This makes it possible to choose between different officially supported EKS AMI families. The --node-ami-family can take following keywords: Keyword Description AmazonLinux2 Indicates that the EKS AMI image based on Amazon Linux 2 should be used (default). Ubuntu1804 Indicates that the EKS AMI image based on Ubuntu 18.04 should be used. WindowsServer2019FullContainer Indicates that the EKS AMI image based on Windows Server 2019 Full Container should be used. WindowsServer2019CoreContainer Indicates that the EKS AMI image based on Windows Server 2019 Core Container should be used.","title":"Custom AMI Support"},{"location":"usage/custom-ami-support/#custom-ami-support","text":"With the 0.1.2 release we have introduced the --node-ami flag for use when creating a cluster. This enables a number of advanced use cases such as using a custom AMI or querying AWS in realtime to determine which AMI to use (non-GPU and GPU instances). The --node-ami can take the AMI image id for an image to explicitly use. It also can take the following 'special' keywords: Keyword Description static Indicates that the AMI images ids embedded into eksctl should be used. This relates to the static resolvers. auto Indicates that the AMI to use for the nodes should be found by querying AWS EC2. This relates to the auto resolver. auto-ssm Indicates that the AMI to use for the nodes should be found by querying AWS SSM Parameter Store. If, for example, AWS release a new version of the EKS node AMIs and a new version of eksctl hasn't been released you can use the latest AMI by doing the following: 1 eksctl create cluster --node-ami=auto With the 0.1.9 release we have introduced the --node-ami-family flag for use when creating the cluster. This makes it possible to choose between different officially supported EKS AMI families. The --node-ami-family can take following keywords: Keyword Description AmazonLinux2 Indicates that the EKS AMI image based on Amazon Linux 2 should be used (default). Ubuntu1804 Indicates that the EKS AMI image based on Ubuntu 18.04 should be used. WindowsServer2019FullContainer Indicates that the EKS AMI image based on Windows Server 2019 Full Container should be used. WindowsServer2019CoreContainer Indicates that the EKS AMI image based on Windows Server 2019 Core Container should be used.","title":"Custom AMI Support"},{"location":"usage/customizing-the-kubelet/","text":"Customizing kubelet configuration \u00b6 System resources can be reserved through the configuration of the kubelet. This is recommended, because in the case of resource starvation the kubelet might not be able to evict pods and eventually make the node become NotReady . To do this, config files can include the kubeletExtraConfig field which accepts a free form yaml that will be embedded into the kubelet.yaml . Some fields in the kubelet.yaml are set by eksctl and therefore are not overwritable, such as the address , clusterDomain , authentication , authorization , or serverTLSBootstrap . The following example config file creates a nodegroup that reserves 300m vCPU, 300Mi of memory and 1Gi of ephemeral-storage for the kubelet; 300m vCPU, 300Mi of memory and 1Gi of ephemeral storage for OS system daemons; and kicks in eviction of pods when there is less than 200Mi of memory available or less than 10% of the root filesystem. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : dev-cluster-1 region : eu-north-1 nodeGroups : - name : ng-1 instanceType : m5a.xlarge desiredCapacity : 1 kubeletExtraConfig : kubeReserved : cpu : \"300m\" memory : \"300Mi\" ephemeral-storage : \"1Gi\" kubeReservedCgroup : \"/kube-reserved\" systemReserved : cpu : \"300m\" memory : \"300Mi\" ephemeral-storage : \"1Gi\" evictionHard : memory.available : \"200Mi\" nodefs.available : \"10%\" featureGates : DynamicKubeletConfig : true RotateKubeletServerCertificate : true # has to be enabled, otherwise it will be disabled In this example, given instances of type m5a.xlarge which have 4 vCPUs and 16GiB of memory, the Allocatable amount of CPUs would be 3.4 and 15.4 GiB of memory. In addition, the DynamicKubeletConfig feature gate is also enabled. It is important to know that the values specified in the config file for the the fields in kubeletExtraconfig will completely overwrite the default values specified by eksctl. Warning By default eksctl sets featureGates.RotateKubeletServerCertificate=true , but when custom featureGates are provided, it will be unset. You should always include featureGates.RotateKubeletServerCertificate=true , unless you have to disable it.","title":"Customizing kubelet configuration"},{"location":"usage/customizing-the-kubelet/#customizing-kubelet-configuration","text":"System resources can be reserved through the configuration of the kubelet. This is recommended, because in the case of resource starvation the kubelet might not be able to evict pods and eventually make the node become NotReady . To do this, config files can include the kubeletExtraConfig field which accepts a free form yaml that will be embedded into the kubelet.yaml . Some fields in the kubelet.yaml are set by eksctl and therefore are not overwritable, such as the address , clusterDomain , authentication , authorization , or serverTLSBootstrap . The following example config file creates a nodegroup that reserves 300m vCPU, 300Mi of memory and 1Gi of ephemeral-storage for the kubelet; 300m vCPU, 300Mi of memory and 1Gi of ephemeral storage for OS system daemons; and kicks in eviction of pods when there is less than 200Mi of memory available or less than 10% of the root filesystem. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : dev-cluster-1 region : eu-north-1 nodeGroups : - name : ng-1 instanceType : m5a.xlarge desiredCapacity : 1 kubeletExtraConfig : kubeReserved : cpu : \"300m\" memory : \"300Mi\" ephemeral-storage : \"1Gi\" kubeReservedCgroup : \"/kube-reserved\" systemReserved : cpu : \"300m\" memory : \"300Mi\" ephemeral-storage : \"1Gi\" evictionHard : memory.available : \"200Mi\" nodefs.available : \"10%\" featureGates : DynamicKubeletConfig : true RotateKubeletServerCertificate : true # has to be enabled, otherwise it will be disabled In this example, given instances of type m5a.xlarge which have 4 vCPUs and 16GiB of memory, the Allocatable amount of CPUs would be 3.4 and 15.4 GiB of memory. In addition, the DynamicKubeletConfig feature gate is also enabled. It is important to know that the values specified in the config file for the the fields in kubeletExtraconfig will completely overwrite the default values specified by eksctl. Warning By default eksctl sets featureGates.RotateKubeletServerCertificate=true , but when custom featureGates are provided, it will be unset. You should always include featureGates.RotateKubeletServerCertificate=true , unless you have to disable it.","title":"Customizing kubelet configuration"},{"location":"usage/eks-managed-nodes/","text":"EKS Managed Nodegroups \u00b6 Amazon EKS managed nodegroups is a feature that automates the provisioning and lifecycle management of nodes (EC2 instances) for Amazon EKS Kubernetes clusters. Customers can provision optimized groups of nodes for their clusters and EKS will keep their nodes up to date with the latest Kubernetes and host OS versions. An EKS managed node group is an autoscaling group and associated EC2 instances that are managed by AWS for an Amazon EKS cluster. Each node group uses the Amazon EKS-optimized Amazon Linux 2 AMI. Amazon EKS makes it easy to apply bug fixes and security patches to nodes, as well as update them to the latest Kubernetes versions. Each node group launches an autoscaling group for your cluster, which can span multiple AWS VPC availability zones and subnets for high-availability. Info The term \"unmanaged nodegroups\" has been used to refer to nodegroups that eksctl has supported since the beginning and uses by default. The ClusterConfig file continues to use the nodeGroups field for defining unmanaged nodegroups, and a new field managedNodeGroups has been added for defining managed nodegroups. Creating a cluster \u00b6 You can add a managed node group to new or existing clusters. To create a new cluster with a managed nodegroup, run 1 eksctl create cluster --managed To create multiple managed nodegroups and have more control over the configuration, a config file can be used. Note Managed nodegroups do not have complete feature parity with unmanaged nodegroups. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # cluster.yaml # A cluster with two managed nodegroups --- apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : managed-cluster region : us-west-2 managedNodeGroups : - name : managed-ng-1 minSize : 2 maxSize : 4 desiredCapacity : 3 volumeSize : 20 ssh : allow : true publicKeyPath : ~/.ssh/ec2_id_rsa.pub # new feature for restricting SSH access to certain AWS security group IDs sourceSecurityGroupIds : [ \"sg-00241fbb12c607007\" ] labels : { role : worker } tags : nodegroup-role : worker iam : withAddonPolicies : externalDNS : true certManager : true - name : managed-ng-2 instanceType : t2.large minSize : 2 maxSize : 3 It's possible to have a cluster with both managed and unmanaged nodegroups. Unmanaged nodegroups do not show up in the AWS EKS console but eksctl get nodegroup will list both types of nodegroups. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # cluster.yaml # A cluster with an unmanaged nodegroup and two managed nodegroups. --- apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : managed-cluster region : us-west-2 nodeGroups : - name : ng-1 minSize : 2 managedNodeGroups : - name : managed-ng-1 minSize : 2 maxSize : 4 desiredCapacity : 3 volumeSize : 20 ssh : allow : true publicKeyPath : ~/.ssh/ec2_id_rsa.pub # new feature for restricting SSH access to certain AWS security group IDs sourceSecurityGroupIds : [ \"sg-00241fbb12c607007\" ] labels : { role : worker } tags : nodegroup-role : worker iam : withAddonPolicies : externalDNS : true certManager : true - name : managed-ng-2 instanceType : t2.large minSize : 2 maxSize : 3 Upgrading managed nodegroups \u00b6 You can update a nodegroup to the latest EKS-optimized AMI release version for the AMI type you are using at any time. If your nodegroup is the same Kubernetes version as the cluster, you can update to the latest AMI release version for that Kubernetes version of the AMI type you are using. If your nodegroup is the previous Kubernetes version from the cluster\u2019s Kubernetes version, you can update the nodegroup to the latest AMI release version that matches the nodegroup\u2019s Kubernetes version, or update to the latest AMI release version that matches the clusters Kubernetes version. You cannot roll back a nodegroup to an earlier Kubernetes version. To upgrade a managed nodegroup to the latest AMI release version: 1 eksctl upgrade nodegroup --name=managed-ng-1 --cluster=managed-cluster If a nodegroup is on Kubernetes 1.13, and the cluster's Kubernetes version is 1.14, the nodegroup can be upgraded to the latest AMI release for Kubernetes 1.14 using: 1 eksctl upgrade nodegroup --name=managed-ng-1 --cluster=managed-cluster --kubernetes-version=1.14 Nodegroup health issues \u00b6 EKS Managed Nodegroups automatically checks the configuration of your nodegroup and nodes for health issues and reports them through the EKS API and console. To view health issues for a nodegroup: 1 eksctl utils nodegroup - health --name=managed-ng-1 --cluster=managed-cluster Managing labels \u00b6 EKS Managed Nodegroups supports attaching labels that are applied to the Kubernetes nodes in the nodegroup. This is specified via the labels field in eksctl during cluster or nodegroup creation. To set new labels or updating existing labels on a nodegroup: 1 eksctl set labels --cluster managed-cluster --nodegroup managed-ng-1 --labels kubernetes.io/managed-by=eks,kubernetes.io/role=worker To unset or remove labels from a nodegroup: 1 eksctl unset labels --cluster managed-cluster --nodegroup managed-ng-1 --labels kubernetes.io/managed-by,kubernetes.io/role To view all labels set on a nodegroup: 1 eksctl get labels --cluster managed-cluster --nodegroup managed-ng-1 Scaling managed modegroups \u00b6 eksctl scale nodegroup also supports managed nodegroups. The syntax for scaling a managed or unmanaged nodegroup is the same. 1 eksctl scale nodegroup --name=managed-ng-1 --cluster=managed-cluster --nodes=4 Feature parity with unmanaged nodegroups \u00b6 EKS Managed Nodegroups are managed by AWS EKS and do not offer the same level of configuration as unmanaged nodegroups. The unsupported options are noted below. No support for private networking ( nodeGroups[*].privateNetworking ). Tags ( managedNodeGroups[*].tags ) in managed nodegroups apply to the EKS Nodegroup resource and do not propagate to the provisioned Autoscaling Group like in unmanaged nodegroups. iam.instanceProfileARN and iam.instanceRoleARN are not supported for managed nodegroups. The amiFamily field supports only AmazonLinux2 instancesDistribution field is not supported volumeSize is the only field supported for configuring volumes Control over the node bootstrapping process and customization of the kubelet are not supported. This includes the following fields: maxPodsPerNode , taints , targetGroupARNs , preBootstrapCommands , overrideBootstrapCommand , clusterDNS and kubeletExtraConfig . eksctl versions below 0.12.0 \u00b6 For clusters upgraded from EKS 1.13 to EKS 1.14, managed nodegroups will not be able to communicate with unmanaged nodegroups. As a result, pods in a managed nodegroup will be unable to reach pods in an unmanaged nodegroup, and vice versa. To fix this, use eksctl 0.12.0 or above and run eksctl update cluster. To fix this manually, add ingress rules to the shared security group and the default cluster security group to allow traffic from each other. The shared security group and the default cluster security groups have the naming convention eksctl- -cluster-ClusterSharedNodeSecurityGroup- and eks-cluster-sg- - - respectively. Further information \u00b6 EKS Managed Nodegroups","title":"EKS Managed Nodegroups"},{"location":"usage/eks-managed-nodes/#eks-managed-nodegroups","text":"Amazon EKS managed nodegroups is a feature that automates the provisioning and lifecycle management of nodes (EC2 instances) for Amazon EKS Kubernetes clusters. Customers can provision optimized groups of nodes for their clusters and EKS will keep their nodes up to date with the latest Kubernetes and host OS versions. An EKS managed node group is an autoscaling group and associated EC2 instances that are managed by AWS for an Amazon EKS cluster. Each node group uses the Amazon EKS-optimized Amazon Linux 2 AMI. Amazon EKS makes it easy to apply bug fixes and security patches to nodes, as well as update them to the latest Kubernetes versions. Each node group launches an autoscaling group for your cluster, which can span multiple AWS VPC availability zones and subnets for high-availability. Info The term \"unmanaged nodegroups\" has been used to refer to nodegroups that eksctl has supported since the beginning and uses by default. The ClusterConfig file continues to use the nodeGroups field for defining unmanaged nodegroups, and a new field managedNodeGroups has been added for defining managed nodegroups.","title":"EKS Managed Nodegroups"},{"location":"usage/eks-managed-nodes/#creating-a-cluster","text":"You can add a managed node group to new or existing clusters. To create a new cluster with a managed nodegroup, run 1 eksctl create cluster --managed To create multiple managed nodegroups and have more control over the configuration, a config file can be used. Note Managed nodegroups do not have complete feature parity with unmanaged nodegroups. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # cluster.yaml # A cluster with two managed nodegroups --- apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : managed-cluster region : us-west-2 managedNodeGroups : - name : managed-ng-1 minSize : 2 maxSize : 4 desiredCapacity : 3 volumeSize : 20 ssh : allow : true publicKeyPath : ~/.ssh/ec2_id_rsa.pub # new feature for restricting SSH access to certain AWS security group IDs sourceSecurityGroupIds : [ \"sg-00241fbb12c607007\" ] labels : { role : worker } tags : nodegroup-role : worker iam : withAddonPolicies : externalDNS : true certManager : true - name : managed-ng-2 instanceType : t2.large minSize : 2 maxSize : 3 It's possible to have a cluster with both managed and unmanaged nodegroups. Unmanaged nodegroups do not show up in the AWS EKS console but eksctl get nodegroup will list both types of nodegroups. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # cluster.yaml # A cluster with an unmanaged nodegroup and two managed nodegroups. --- apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : managed-cluster region : us-west-2 nodeGroups : - name : ng-1 minSize : 2 managedNodeGroups : - name : managed-ng-1 minSize : 2 maxSize : 4 desiredCapacity : 3 volumeSize : 20 ssh : allow : true publicKeyPath : ~/.ssh/ec2_id_rsa.pub # new feature for restricting SSH access to certain AWS security group IDs sourceSecurityGroupIds : [ \"sg-00241fbb12c607007\" ] labels : { role : worker } tags : nodegroup-role : worker iam : withAddonPolicies : externalDNS : true certManager : true - name : managed-ng-2 instanceType : t2.large minSize : 2 maxSize : 3","title":"Creating a cluster"},{"location":"usage/eks-managed-nodes/#upgrading-managed-nodegroups","text":"You can update a nodegroup to the latest EKS-optimized AMI release version for the AMI type you are using at any time. If your nodegroup is the same Kubernetes version as the cluster, you can update to the latest AMI release version for that Kubernetes version of the AMI type you are using. If your nodegroup is the previous Kubernetes version from the cluster\u2019s Kubernetes version, you can update the nodegroup to the latest AMI release version that matches the nodegroup\u2019s Kubernetes version, or update to the latest AMI release version that matches the clusters Kubernetes version. You cannot roll back a nodegroup to an earlier Kubernetes version. To upgrade a managed nodegroup to the latest AMI release version: 1 eksctl upgrade nodegroup --name=managed-ng-1 --cluster=managed-cluster If a nodegroup is on Kubernetes 1.13, and the cluster's Kubernetes version is 1.14, the nodegroup can be upgraded to the latest AMI release for Kubernetes 1.14 using: 1 eksctl upgrade nodegroup --name=managed-ng-1 --cluster=managed-cluster --kubernetes-version=1.14","title":"Upgrading managed nodegroups"},{"location":"usage/eks-managed-nodes/#nodegroup-health-issues","text":"EKS Managed Nodegroups automatically checks the configuration of your nodegroup and nodes for health issues and reports them through the EKS API and console. To view health issues for a nodegroup: 1 eksctl utils nodegroup - health --name=managed-ng-1 --cluster=managed-cluster","title":"Nodegroup health issues"},{"location":"usage/eks-managed-nodes/#managing-labels","text":"EKS Managed Nodegroups supports attaching labels that are applied to the Kubernetes nodes in the nodegroup. This is specified via the labels field in eksctl during cluster or nodegroup creation. To set new labels or updating existing labels on a nodegroup: 1 eksctl set labels --cluster managed-cluster --nodegroup managed-ng-1 --labels kubernetes.io/managed-by=eks,kubernetes.io/role=worker To unset or remove labels from a nodegroup: 1 eksctl unset labels --cluster managed-cluster --nodegroup managed-ng-1 --labels kubernetes.io/managed-by,kubernetes.io/role To view all labels set on a nodegroup: 1 eksctl get labels --cluster managed-cluster --nodegroup managed-ng-1","title":"Managing labels"},{"location":"usage/eks-managed-nodes/#scaling-managed-modegroups","text":"eksctl scale nodegroup also supports managed nodegroups. The syntax for scaling a managed or unmanaged nodegroup is the same. 1 eksctl scale nodegroup --name=managed-ng-1 --cluster=managed-cluster --nodes=4","title":"Scaling managed modegroups"},{"location":"usage/eks-managed-nodes/#feature-parity-with-unmanaged-nodegroups","text":"EKS Managed Nodegroups are managed by AWS EKS and do not offer the same level of configuration as unmanaged nodegroups. The unsupported options are noted below. No support for private networking ( nodeGroups[*].privateNetworking ). Tags ( managedNodeGroups[*].tags ) in managed nodegroups apply to the EKS Nodegroup resource and do not propagate to the provisioned Autoscaling Group like in unmanaged nodegroups. iam.instanceProfileARN and iam.instanceRoleARN are not supported for managed nodegroups. The amiFamily field supports only AmazonLinux2 instancesDistribution field is not supported volumeSize is the only field supported for configuring volumes Control over the node bootstrapping process and customization of the kubelet are not supported. This includes the following fields: maxPodsPerNode , taints , targetGroupARNs , preBootstrapCommands , overrideBootstrapCommand , clusterDNS and kubeletExtraConfig .","title":"Feature parity with unmanaged nodegroups"},{"location":"usage/eks-managed-nodes/#eksctl-versions-below-0120","text":"For clusters upgraded from EKS 1.13 to EKS 1.14, managed nodegroups will not be able to communicate with unmanaged nodegroups. As a result, pods in a managed nodegroup will be unable to reach pods in an unmanaged nodegroup, and vice versa. To fix this, use eksctl 0.12.0 or above and run eksctl update cluster. To fix this manually, add ingress rules to the shared security group and the default cluster security group to allow traffic from each other. The shared security group and the default cluster security groups have the naming convention eksctl- -cluster-ClusterSharedNodeSecurityGroup- and eks-cluster-sg- - - respectively.","title":"eksctl versions below 0.12.0"},{"location":"usage/eks-managed-nodes/#further-information","text":"EKS Managed Nodegroups","title":"Further information"},{"location":"usage/faq/","text":"Nodegroups \u00b6 Question How can I change the instance type of my nodegroup? Answer From the point of view of eksctl , nodegroups are immutable. This means that once created the only thing eksctl can do is scale the nodegroup up or down. To change the instance type, create a new nodegroup with the desired instance type, then drain it so that the workloads move to the new one. After that step is complete you can delete the old nodegroup Ingress \u00b6 Question How do I set up ingress with eksctl ? Answer If the plan is to use AWS ALB Ingress controller, setting nodegroups[*].iam.withAddonPolicies.albIngress to true will add the required IAM policies to your nodes allowing the controller to provision load balancers. Then you can follow docs to set up the controller . For Nginx Ingress Controller, setup would be the same as any other Kubernetes cluster .","title":"FAQ"},{"location":"usage/faq/#nodegroups","text":"Question How can I change the instance type of my nodegroup? Answer From the point of view of eksctl , nodegroups are immutable. This means that once created the only thing eksctl can do is scale the nodegroup up or down. To change the instance type, create a new nodegroup with the desired instance type, then drain it so that the workloads move to the new one. After that step is complete you can delete the old nodegroup","title":"Nodegroups"},{"location":"usage/faq/#ingress","text":"Question How do I set up ingress with eksctl ? Answer If the plan is to use AWS ALB Ingress controller, setting nodegroups[*].iam.withAddonPolicies.albIngress to true will add the required IAM policies to your nodes allowing the controller to provision load balancers. Then you can follow docs to set up the controller . For Nginx Ingress Controller, setup would be the same as any other Kubernetes cluster .","title":"Ingress"},{"location":"usage/fargate-support/","text":"EKS Fargate Support \u00b6 AWS Fargate is a managed compute engine for Amazon ECS that can run containers. In Fargate you don't need to manage servers or clusters. Amazon EKS can now launch pods onto AWS Fargate . This removes the need to worry about how you provision or manage infrastructure for pods and makes it easier to build and run performant, highly-available Kubernetes applications on AWS. Creating a cluster with Fargate support \u00b6 You can add a cluster with Fargate support with: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 $ eksctl create cluster --fargate [\u2139] eksctl version 0.11.0 [\u2139] using region ap-northeast-1 [\u2139] setting availability zones to [ap-northeast-1a ap-northeast-1d ap-northeast-1c] [\u2139] subnets for ap-northeast-1a - public:192.168.0.0/19 private:192.168.96.0/19 [\u2139] subnets for ap-northeast-1d - public:192.168.32.0/19 private:192.168.128.0/19 [\u2139] subnets for ap-northeast-1c - public:192.168.64.0/19 private:192.168.160.0/19 [\u2139] nodegroup \"ng-dba9d731\" will use \"ami-02e124a380df41614\" [AmazonLinux2/1.14] [\u2139] using Kubernetes version 1.14 [\u2139] creating EKS cluster \"ridiculous-painting-1574859263\" in \"ap-northeast-1\" region [\u2139] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup [\u2139] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-1 --cluster=ridiculous-painting-1574859263' [\u2139] CloudWatch logging will not be enabled for cluster \"ridiculous-painting-1574859263\" in \"ap-northeast-1\" [\u2139] you can enable it with 'eksctl utils update-cluster-logging --region=ap-northeast-1 --cluster=ridiculous-painting-1574859263' [\u2139] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"ridiculous-painting-1574859263\" in \"ap-northeast-1\" [\u2139] 2 sequential tasks: { create cluster control plane \"ridiculous-painting-1574859263\", create nodegroup \"ng-dba9d731\" } [\u2139] building cluster stack \"eksctl-ridiculous-painting-1574859263-cluster\" [\u2139] deploying stack \"eksctl-ridiculous-painting-1574859263-cluster\" [\u2139] building nodegroup stack \"eksctl-ridiculous-painting-1574859263-nodegroup-ng-dba9d731\" [\u2139] --nodes-min=2 was set automatically for nodegroup ng-dba9d731 [\u2139] --nodes-max=2 was set automatically for nodegroup ng-dba9d731 [\u2139] deploying stack \"eksctl-ridiculous-painting-1574859263-nodegroup-ng-dba9d731\" [\u2714] all EKS cluster resources for \"ridiculous-painting-1574859263\" have been created [\u2714] saved kubeconfig as \"/Users/marc/.kube/config\" [\u2139] adding identity \"arn:aws:iam::123456789012:role/eksctl-ridiculous-painting-157485-NodeInstanceRole-104DXUJOFDPO5\" to auth ConfigMap [\u2139] nodegroup \"ng-dba9d731\" has 0 node(s) [\u2139] waiting for at least 2 node(s) to become ready in \"ng-dba9d731\" [\u2139] nodegroup \"ng-dba9d731\" has 2 node(s) [\u2139] node \"ip-192-168-27-156.ap-northeast-1.compute.internal\" is ready [\u2139] node \"ip-192-168-95-177.ap-northeast-1.compute.internal\" is ready [\u2139] creating Fargate profile \"default\" on EKS cluster \"ridiculous-painting-1574859263\" [\u2139] created Fargate profile \"default\" on EKS cluster \"ridiculous-painting-1574859263\" [\u2139] kubectl command should work with \"/Users/marc/.kube/config\", try 'kubectl get nodes' [\u2714] EKS cluster \"ridiculous-painting-1574859263\" in \"ap-northeast-1\" region is ready This command will have created a cluster and a Fargate profile. This profile contains certain information needed by AWS to instantiate pods in Fargate. These are: pod execution role to define the permissions required to run the pod and the networking location (subnet) to run the pod. This allows the same networking and security permissions to be applied to multiple Fargate pods and makes it easier to migrate existing pods on a cluster to Fargate. Selector to define which pods should run on Fargate. This is composed by a namespace and labels . When the profile is not specified but support for Fargate is enabled with --fargate a default Fargate profile is created. This profile targets the default and the kube-system namespaces so pods in those namespaces will run on Fargate. The Fargate profile that was created can be checked with the following command: 1 2 3 4 5 6 7 8 9 10 $ eksctl get fargateprofile --cluster ridiculous-painting-1574859263 -o yaml - name: fp-default podExecutionRoleARN: arn:aws:iam::123456789012:role/eksctl-ridiculous-painting-1574859263-ServiceRole-EIFQOH0S1GE7 selectors: - namespace: default - namespace: kube-system subnets: - subnet-0b3a5522f3b48a742 - subnet-0c35f1497067363f3 - subnet-0a29aa00b25082021 To learn more about selectors see Designing Fargate profiles . Creating a cluster using a config file \u00b6 The following config file declares an EKS cluster with both a nodegroup composed of one EC2 m5.large instance and two Fargate profiles. All pods defined in the default and kube-system namespaces will run on Fargate. All pods in the dev namespace that also have the label dev=passed will also run on Fargate. Any other pods will be scheduled on the node in ng-1 . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # An example of ClusterConfig with a normal nodegroup and a Fargate profile. --- apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : fargate-cluster region : ap-northeast-1 nodeGroups : - name : ng-1 instanceType : m5.large desiredCapacity : 1 fargateProfiles : - name : fp-default selectors : # All workloads in the \"default\" Kubernetes namespace will be # scheduled onto Fargate: - namespace : default # All workloads in the \"kube-system\" Kubernetes namespace will be # scheduled onto Fargate: - namespace : kube-system - name : fp-dev selectors : # All workloads in the \"dev\" Kubernetes namespace matching the following # label selectors will be scheduled onto Fargate: - namespace : dev labels : env : dev checks : passed 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 $ eksctl create cluster -f cluster-fargate.yaml [\u2139] eksctl version 0.11.0 [\u2139] using region ap-northeast-1 [\u2139] setting availability zones to [ap-northeast-1c ap-northeast-1a ap-northeast-1d] [\u2139] subnets for ap-northeast-1c - public:192.168.0.0/19 private:192.168.96.0/19 [\u2139] subnets for ap-northeast-1a - public:192.168.32.0/19 private:192.168.128.0/19 [\u2139] subnets for ap-northeast-1d - public:192.168.64.0/19 private:192.168.160.0/19 [\u2139] nodegroup \"ng-1\" will use \"ami-02e124a380df41614\" [AmazonLinux2/1.14] [\u2139] using Kubernetes version 1.14 [\u2139] creating EKS cluster \"fargate-cluster\" in \"ap-northeast-1\" region with Fargate profile and un-managed nodes [\u2139] 1 nodegroup (ng-1) was included (based on the include/exclude rules) [\u2139] will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s) [\u2139] will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s) [\u2139] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-1 --cluster=fargate-cluster' [\u2139] CloudWatch logging will not be enabled for cluster \"fargate-cluster\" in \"ap-northeast-1\" [\u2139] you can enable it with 'eksctl utils update-cluster-logging --region=ap-northeast-1 --cluster=fargate-cluster' [\u2139] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"fargate-cluster\" in \"ap-northeast-1\" [\u2139] 2 sequential tasks: { create cluster control plane \"fargate-cluster\", create nodegroup \"ng-1\" } [\u2139] building cluster stack \"eksctl-fargate-cluster-cluster\" [\u2139] deploying stack \"eksctl-fargate-cluster-cluster\" [\u2139] building nodegroup stack \"eksctl-fargate-cluster-nodegroup-ng-1\" [\u2139] --nodes-min=1 was set automatically for nodegroup ng-1 [\u2139] --nodes-max=1 was set automatically for nodegroup ng-1 [\u2139] deploying stack \"eksctl-fargate-cluster-nodegroup-ng-1\" [\u2714] all EKS cluster resources for \"fargate-cluster\" have been created [\u2714] saved kubeconfig as \"/home/user1/.kube/config\" [\u2139] adding identity \"arn:aws:iam::123456789012:role/eksctl-fargate-cluster-nod-NodeInstanceRole-42Q80B2Z147I\" to auth ConfigMap [\u2139] nodegroup \"ng-1\" has 0 node(s) [\u2139] waiting for at least 1 node(s) to become ready in \"ng-1\" [\u2139] nodegroup \"ng-1\" has 1 node(s) [\u2139] node \"ip-192-168-71-83.ap-northeast-1.compute.internal\" is ready [\u2139] creating Fargate profile \"fp-default\" on EKS cluster \"fargate-cluster\" [\u2139] created Fargate profile \"fp-default\" on EKS cluster \"fargate-cluster\" [\u2139] creating Fargate profile \"fp-dev\" on EKS cluster \"fargate-cluster\" [\u2139] created Fargate profile \"fp-dev\" on EKS cluster \"fargate-cluster\" [\u2139] \"coredns\" is now schedulable onto Fargate [\u2139] \"coredns\" is now scheduled onto Fargate [\u2139] \"coredns\" is now scheduled onto Fargate [\u2139] \"coredns\" pods are now scheduled onto Fargate [\u2139] kubectl command should work with \"/home/user1/.kube/config\", try 'kubectl get nodes' [\u2714] EKS cluster \"fargate-cluster\" in \"ap-northeast-1\" region is ready Designing Fargate profiles \u00b6 Each selector entry has up to two components, namespace and a list of key-value pairs. Only the namespace component is required to create a selector entry. All rules (namespaces, key value pairs) must apply to a pod to match a selector entry. A pod only needs to match one selector entry to run on the profile. Any pod that matches all the conditions in a selector field would be scheduled to be run on Fargate. Any pods not matching either the whitelisted Namespaces but where the user manually set the scheduler: fargate-scheduler filed would be stuck in a Pending state, as they were not authorized to run on Fargate. Profiles must meet the following requirements: One selector is mandatory per profile Each selector must include a namespace; labels are optional Example: scheduling workload in Fargate \u00b6 To schedule pods on Fargate for the example mentioned above, one could, for example, create a namespace called dev and deploy the workload there: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ kubectl create namespace dev namespace/dev created $ kubectl run nginx --image = nginx --restart = Never --namespace dev pod/nginx created $ kubectl get pods --all-namespaces --output wide NAMESPACE NAME READY STATUS AGE IP NODE dev nginx 1/1 Running 75s 192.168.183.140 fargate-ip-192-168-183-140.ap-northeast-1.compute.internal kube-system aws-node-44qst 1/1 Running 21m 192.168.70.246 ip-192-168-70-246.ap-northeast-1.compute.internal kube-system aws-node-4vr66 1/1 Running 21m 192.168.23.122 ip-192-168-23-122.ap-northeast-1.compute.internal kube-system coredns-699bb99bf8-84x74 1/1 Running 26m 192.168.2.95 ip-192-168-23-122.ap-northeast-1.compute.internal kube-system coredns-699bb99bf8-f6x6n 1/1 Running 26m 192.168.90.73 ip-192-168-70-246.ap-northeast-1.compute.internal kube-system kube-proxy-brxhg 1/1 Running 21m 192.168.23.122 ip-192-168-23-122.ap-northeast-1.compute.internal kube-system kube-proxy-zd7s8 1/1 Running 21m 192.168.70.246 ip-192-168-70-246.ap-northeast-1.compute.internal From the output of the last kubectl get pods command we can see that the nginx pod is deployed in a node called fargate-ip-192-168-183-140.ap-northeast-1.compute.internal . Managing Fargate profiles \u00b6 To deploy Kubernetes workloads on Fargate, EKS needs a Fargate profile. When creating a cluster like in the examples above, eksctl takes care of this by creating a default profile. Given an already existing cluster, it's also possible to create a Fargate profile with the eksctl create fargateprofile command: NOTE: This operation is only supported on clusters that run on the EKS platform version eks.5 or higher. NOTE: If the existing was created with a version of eksctl prior to 0.11.0, you will need to run eksctl update cluster before creating the Fargate profile. 1 2 3 $ eksctl create fargateprofile --namespace dev --cluster fargate-example-cluster [\u2139] creating Fargate profile \"fp-9bfc77ad\" on EKS cluster \"fargate-example-cluster\" [\u2139] created Fargate profile \"fp-9bfc77ad\" on EKS cluster \"fargate-example-cluster\" You can also specify the name of the Fargate profile to be created. This name must not start with the prefix eks- . 1 2 $ eksctl create fargateprofile --namespace dev --cluster fargate-example-cluster --name fp-development [\u2139] created Fargate profile \"fp-development\" on EKS cluster \"fargate-example-cluster\" Using this command with CLI flags eksctl can only create a single Fargate profile with a simple selector. For more complex selectors, for example with more namespaces, eksctl supports using a config file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : fargate-example-cluster region : ap-northeast-1 fargateProfiles : - name : fp-default selectors : # All workloads in the \"default\" Kubernetes namespace will be # scheduled onto Fargate: - namespace : default # All workloads in the \"kube-system\" Kubernetes namespace will be # scheduled onto Fargate: - namespace : kube-system - name : fp-dev selectors : # All workloads in the \"dev\" Kubernetes namespace matching the following # label selectors will be scheduled onto Fargate: - namespace : dev labels : env : dev checks : passed 1 2 3 4 5 6 7 $ eksctl create fargateprofile -f fargate-example-cluster.yaml [\u2139] creating Fargate profile \"fp-default\" on EKS cluster \"fargate-example-cluster\" [\u2139] created Fargate profile \"fp-default\" on EKS cluster \"fargate-example-cluster\" [\u2139] creating Fargate profile \"fp-dev\" on EKS cluster \"fargate-example-cluster\" [\u2139] created Fargate profile \"fp-dev\" on EKS cluster \"fargate-example-cluster\" [\u2139] \"coredns\" is now scheduled onto Fargate [\u2139] \"coredns\" pods are now scheduled onto Fargate To see existing Fargate profiles in a cluster: 1 2 3 $ eksctl get fargateprofile --cluster fargate-example-cluster NAME SELECTOR_NAMESPACE SELECTOR_LABELS POD_EXECUTION_ROLE_ARN SUBNETS fp-9bfc77ad dev <none> arn:aws:iam::123456789012:role/eksctl-fargate-example-cluster-ServiceRole-1T5F78E5FSH79 subnet-00adf1d8c99f83381,subnet-04affb163ffab17d4,subnet-035b34379d5ef5473 And to see them in yaml format: 1 2 3 4 5 6 7 8 9 $ eksctl get fargateprofile --cluster fargate-example-cluster -o yaml - name: fp-9bfc77ad podExecutionRoleARN: arn:aws:iam::123456789012:role/eksctl-fargate-example-cluster-ServiceRole-1T5F78E5FSH79 selectors: - namespace: dev subnets: - subnet-00adf1d8c99f83381 - subnet-04affb163ffab17d4 - subnet-035b34379d5ef5473 Or in json format: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ eksctl get fargateprofile --cluster fargate-example-cluster -o json [ { \"name\": \"fp-9bfc77ad\", \"podExecutionRoleARN\": \"arn:aws:iam::123456789012:role/eksctl-fargate-example-cluster-ServiceRole-1T5F78E5FSH79\", \"selectors\": [ { \"namespace\": \"dev\" } ], \"subnets\": [ \"subnet-00adf1d8c99f83381\", \"subnet-04affb163ffab17d4\", \"subnet-035b34379d5ef5473\" ] } ] Fargate profiles are immutable by design. To change something, create a new Fargate profile with the desired changes and delete the old one with the eksctl delete fargateprofile command like in the following example: 1 2 3 4 5 $ eksctl delete fargateprofile --cluster fargate-example-cluster --name fp-9bfc77ad --wait 2019-11-27T19:04:26+09:00 [\u2139] deleting Fargate profile \"fp-9bfc77ad\" ClusterName: \"fargate-example-cluster\", FargateProfileName: \"fp-9bfc77ad\" } Note that the profile deletion is a process that can take up to a few minutes. When the --wait flag is not specified, eksctl optimistically expects the profile to be deleted and returns as soon as the AWS API request has been sent. To make eksctl wait until the profile has been successfully deleted, use --wait like in the example above. Further reading \u00b6 Fargate Fargate from EKS","title":"EKS Fargate Support"},{"location":"usage/fargate-support/#eks-fargate-support","text":"AWS Fargate is a managed compute engine for Amazon ECS that can run containers. In Fargate you don't need to manage servers or clusters. Amazon EKS can now launch pods onto AWS Fargate . This removes the need to worry about how you provision or manage infrastructure for pods and makes it easier to build and run performant, highly-available Kubernetes applications on AWS.","title":"EKS Fargate Support"},{"location":"usage/fargate-support/#creating-a-cluster-with-fargate-support","text":"You can add a cluster with Fargate support with: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 $ eksctl create cluster --fargate [\u2139] eksctl version 0.11.0 [\u2139] using region ap-northeast-1 [\u2139] setting availability zones to [ap-northeast-1a ap-northeast-1d ap-northeast-1c] [\u2139] subnets for ap-northeast-1a - public:192.168.0.0/19 private:192.168.96.0/19 [\u2139] subnets for ap-northeast-1d - public:192.168.32.0/19 private:192.168.128.0/19 [\u2139] subnets for ap-northeast-1c - public:192.168.64.0/19 private:192.168.160.0/19 [\u2139] nodegroup \"ng-dba9d731\" will use \"ami-02e124a380df41614\" [AmazonLinux2/1.14] [\u2139] using Kubernetes version 1.14 [\u2139] creating EKS cluster \"ridiculous-painting-1574859263\" in \"ap-northeast-1\" region [\u2139] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup [\u2139] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-1 --cluster=ridiculous-painting-1574859263' [\u2139] CloudWatch logging will not be enabled for cluster \"ridiculous-painting-1574859263\" in \"ap-northeast-1\" [\u2139] you can enable it with 'eksctl utils update-cluster-logging --region=ap-northeast-1 --cluster=ridiculous-painting-1574859263' [\u2139] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"ridiculous-painting-1574859263\" in \"ap-northeast-1\" [\u2139] 2 sequential tasks: { create cluster control plane \"ridiculous-painting-1574859263\", create nodegroup \"ng-dba9d731\" } [\u2139] building cluster stack \"eksctl-ridiculous-painting-1574859263-cluster\" [\u2139] deploying stack \"eksctl-ridiculous-painting-1574859263-cluster\" [\u2139] building nodegroup stack \"eksctl-ridiculous-painting-1574859263-nodegroup-ng-dba9d731\" [\u2139] --nodes-min=2 was set automatically for nodegroup ng-dba9d731 [\u2139] --nodes-max=2 was set automatically for nodegroup ng-dba9d731 [\u2139] deploying stack \"eksctl-ridiculous-painting-1574859263-nodegroup-ng-dba9d731\" [\u2714] all EKS cluster resources for \"ridiculous-painting-1574859263\" have been created [\u2714] saved kubeconfig as \"/Users/marc/.kube/config\" [\u2139] adding identity \"arn:aws:iam::123456789012:role/eksctl-ridiculous-painting-157485-NodeInstanceRole-104DXUJOFDPO5\" to auth ConfigMap [\u2139] nodegroup \"ng-dba9d731\" has 0 node(s) [\u2139] waiting for at least 2 node(s) to become ready in \"ng-dba9d731\" [\u2139] nodegroup \"ng-dba9d731\" has 2 node(s) [\u2139] node \"ip-192-168-27-156.ap-northeast-1.compute.internal\" is ready [\u2139] node \"ip-192-168-95-177.ap-northeast-1.compute.internal\" is ready [\u2139] creating Fargate profile \"default\" on EKS cluster \"ridiculous-painting-1574859263\" [\u2139] created Fargate profile \"default\" on EKS cluster \"ridiculous-painting-1574859263\" [\u2139] kubectl command should work with \"/Users/marc/.kube/config\", try 'kubectl get nodes' [\u2714] EKS cluster \"ridiculous-painting-1574859263\" in \"ap-northeast-1\" region is ready This command will have created a cluster and a Fargate profile. This profile contains certain information needed by AWS to instantiate pods in Fargate. These are: pod execution role to define the permissions required to run the pod and the networking location (subnet) to run the pod. This allows the same networking and security permissions to be applied to multiple Fargate pods and makes it easier to migrate existing pods on a cluster to Fargate. Selector to define which pods should run on Fargate. This is composed by a namespace and labels . When the profile is not specified but support for Fargate is enabled with --fargate a default Fargate profile is created. This profile targets the default and the kube-system namespaces so pods in those namespaces will run on Fargate. The Fargate profile that was created can be checked with the following command: 1 2 3 4 5 6 7 8 9 10 $ eksctl get fargateprofile --cluster ridiculous-painting-1574859263 -o yaml - name: fp-default podExecutionRoleARN: arn:aws:iam::123456789012:role/eksctl-ridiculous-painting-1574859263-ServiceRole-EIFQOH0S1GE7 selectors: - namespace: default - namespace: kube-system subnets: - subnet-0b3a5522f3b48a742 - subnet-0c35f1497067363f3 - subnet-0a29aa00b25082021 To learn more about selectors see Designing Fargate profiles .","title":"Creating a cluster with Fargate support"},{"location":"usage/fargate-support/#creating-a-cluster-using-a-config-file","text":"The following config file declares an EKS cluster with both a nodegroup composed of one EC2 m5.large instance and two Fargate profiles. All pods defined in the default and kube-system namespaces will run on Fargate. All pods in the dev namespace that also have the label dev=passed will also run on Fargate. Any other pods will be scheduled on the node in ng-1 . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # An example of ClusterConfig with a normal nodegroup and a Fargate profile. --- apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : fargate-cluster region : ap-northeast-1 nodeGroups : - name : ng-1 instanceType : m5.large desiredCapacity : 1 fargateProfiles : - name : fp-default selectors : # All workloads in the \"default\" Kubernetes namespace will be # scheduled onto Fargate: - namespace : default # All workloads in the \"kube-system\" Kubernetes namespace will be # scheduled onto Fargate: - namespace : kube-system - name : fp-dev selectors : # All workloads in the \"dev\" Kubernetes namespace matching the following # label selectors will be scheduled onto Fargate: - namespace : dev labels : env : dev checks : passed 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 $ eksctl create cluster -f cluster-fargate.yaml [\u2139] eksctl version 0.11.0 [\u2139] using region ap-northeast-1 [\u2139] setting availability zones to [ap-northeast-1c ap-northeast-1a ap-northeast-1d] [\u2139] subnets for ap-northeast-1c - public:192.168.0.0/19 private:192.168.96.0/19 [\u2139] subnets for ap-northeast-1a - public:192.168.32.0/19 private:192.168.128.0/19 [\u2139] subnets for ap-northeast-1d - public:192.168.64.0/19 private:192.168.160.0/19 [\u2139] nodegroup \"ng-1\" will use \"ami-02e124a380df41614\" [AmazonLinux2/1.14] [\u2139] using Kubernetes version 1.14 [\u2139] creating EKS cluster \"fargate-cluster\" in \"ap-northeast-1\" region with Fargate profile and un-managed nodes [\u2139] 1 nodegroup (ng-1) was included (based on the include/exclude rules) [\u2139] will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s) [\u2139] will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s) [\u2139] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-1 --cluster=fargate-cluster' [\u2139] CloudWatch logging will not be enabled for cluster \"fargate-cluster\" in \"ap-northeast-1\" [\u2139] you can enable it with 'eksctl utils update-cluster-logging --region=ap-northeast-1 --cluster=fargate-cluster' [\u2139] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"fargate-cluster\" in \"ap-northeast-1\" [\u2139] 2 sequential tasks: { create cluster control plane \"fargate-cluster\", create nodegroup \"ng-1\" } [\u2139] building cluster stack \"eksctl-fargate-cluster-cluster\" [\u2139] deploying stack \"eksctl-fargate-cluster-cluster\" [\u2139] building nodegroup stack \"eksctl-fargate-cluster-nodegroup-ng-1\" [\u2139] --nodes-min=1 was set automatically for nodegroup ng-1 [\u2139] --nodes-max=1 was set automatically for nodegroup ng-1 [\u2139] deploying stack \"eksctl-fargate-cluster-nodegroup-ng-1\" [\u2714] all EKS cluster resources for \"fargate-cluster\" have been created [\u2714] saved kubeconfig as \"/home/user1/.kube/config\" [\u2139] adding identity \"arn:aws:iam::123456789012:role/eksctl-fargate-cluster-nod-NodeInstanceRole-42Q80B2Z147I\" to auth ConfigMap [\u2139] nodegroup \"ng-1\" has 0 node(s) [\u2139] waiting for at least 1 node(s) to become ready in \"ng-1\" [\u2139] nodegroup \"ng-1\" has 1 node(s) [\u2139] node \"ip-192-168-71-83.ap-northeast-1.compute.internal\" is ready [\u2139] creating Fargate profile \"fp-default\" on EKS cluster \"fargate-cluster\" [\u2139] created Fargate profile \"fp-default\" on EKS cluster \"fargate-cluster\" [\u2139] creating Fargate profile \"fp-dev\" on EKS cluster \"fargate-cluster\" [\u2139] created Fargate profile \"fp-dev\" on EKS cluster \"fargate-cluster\" [\u2139] \"coredns\" is now schedulable onto Fargate [\u2139] \"coredns\" is now scheduled onto Fargate [\u2139] \"coredns\" is now scheduled onto Fargate [\u2139] \"coredns\" pods are now scheduled onto Fargate [\u2139] kubectl command should work with \"/home/user1/.kube/config\", try 'kubectl get nodes' [\u2714] EKS cluster \"fargate-cluster\" in \"ap-northeast-1\" region is ready","title":"Creating a cluster using a config file"},{"location":"usage/fargate-support/#designing-fargate-profiles","text":"Each selector entry has up to two components, namespace and a list of key-value pairs. Only the namespace component is required to create a selector entry. All rules (namespaces, key value pairs) must apply to a pod to match a selector entry. A pod only needs to match one selector entry to run on the profile. Any pod that matches all the conditions in a selector field would be scheduled to be run on Fargate. Any pods not matching either the whitelisted Namespaces but where the user manually set the scheduler: fargate-scheduler filed would be stuck in a Pending state, as they were not authorized to run on Fargate. Profiles must meet the following requirements: One selector is mandatory per profile Each selector must include a namespace; labels are optional","title":"Designing Fargate profiles"},{"location":"usage/fargate-support/#example-scheduling-workload-in-fargate","text":"To schedule pods on Fargate for the example mentioned above, one could, for example, create a namespace called dev and deploy the workload there: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ kubectl create namespace dev namespace/dev created $ kubectl run nginx --image = nginx --restart = Never --namespace dev pod/nginx created $ kubectl get pods --all-namespaces --output wide NAMESPACE NAME READY STATUS AGE IP NODE dev nginx 1/1 Running 75s 192.168.183.140 fargate-ip-192-168-183-140.ap-northeast-1.compute.internal kube-system aws-node-44qst 1/1 Running 21m 192.168.70.246 ip-192-168-70-246.ap-northeast-1.compute.internal kube-system aws-node-4vr66 1/1 Running 21m 192.168.23.122 ip-192-168-23-122.ap-northeast-1.compute.internal kube-system coredns-699bb99bf8-84x74 1/1 Running 26m 192.168.2.95 ip-192-168-23-122.ap-northeast-1.compute.internal kube-system coredns-699bb99bf8-f6x6n 1/1 Running 26m 192.168.90.73 ip-192-168-70-246.ap-northeast-1.compute.internal kube-system kube-proxy-brxhg 1/1 Running 21m 192.168.23.122 ip-192-168-23-122.ap-northeast-1.compute.internal kube-system kube-proxy-zd7s8 1/1 Running 21m 192.168.70.246 ip-192-168-70-246.ap-northeast-1.compute.internal From the output of the last kubectl get pods command we can see that the nginx pod is deployed in a node called fargate-ip-192-168-183-140.ap-northeast-1.compute.internal .","title":"Example: scheduling workload in Fargate"},{"location":"usage/fargate-support/#managing-fargate-profiles","text":"To deploy Kubernetes workloads on Fargate, EKS needs a Fargate profile. When creating a cluster like in the examples above, eksctl takes care of this by creating a default profile. Given an already existing cluster, it's also possible to create a Fargate profile with the eksctl create fargateprofile command: NOTE: This operation is only supported on clusters that run on the EKS platform version eks.5 or higher. NOTE: If the existing was created with a version of eksctl prior to 0.11.0, you will need to run eksctl update cluster before creating the Fargate profile. 1 2 3 $ eksctl create fargateprofile --namespace dev --cluster fargate-example-cluster [\u2139] creating Fargate profile \"fp-9bfc77ad\" on EKS cluster \"fargate-example-cluster\" [\u2139] created Fargate profile \"fp-9bfc77ad\" on EKS cluster \"fargate-example-cluster\" You can also specify the name of the Fargate profile to be created. This name must not start with the prefix eks- . 1 2 $ eksctl create fargateprofile --namespace dev --cluster fargate-example-cluster --name fp-development [\u2139] created Fargate profile \"fp-development\" on EKS cluster \"fargate-example-cluster\" Using this command with CLI flags eksctl can only create a single Fargate profile with a simple selector. For more complex selectors, for example with more namespaces, eksctl supports using a config file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : fargate-example-cluster region : ap-northeast-1 fargateProfiles : - name : fp-default selectors : # All workloads in the \"default\" Kubernetes namespace will be # scheduled onto Fargate: - namespace : default # All workloads in the \"kube-system\" Kubernetes namespace will be # scheduled onto Fargate: - namespace : kube-system - name : fp-dev selectors : # All workloads in the \"dev\" Kubernetes namespace matching the following # label selectors will be scheduled onto Fargate: - namespace : dev labels : env : dev checks : passed 1 2 3 4 5 6 7 $ eksctl create fargateprofile -f fargate-example-cluster.yaml [\u2139] creating Fargate profile \"fp-default\" on EKS cluster \"fargate-example-cluster\" [\u2139] created Fargate profile \"fp-default\" on EKS cluster \"fargate-example-cluster\" [\u2139] creating Fargate profile \"fp-dev\" on EKS cluster \"fargate-example-cluster\" [\u2139] created Fargate profile \"fp-dev\" on EKS cluster \"fargate-example-cluster\" [\u2139] \"coredns\" is now scheduled onto Fargate [\u2139] \"coredns\" pods are now scheduled onto Fargate To see existing Fargate profiles in a cluster: 1 2 3 $ eksctl get fargateprofile --cluster fargate-example-cluster NAME SELECTOR_NAMESPACE SELECTOR_LABELS POD_EXECUTION_ROLE_ARN SUBNETS fp-9bfc77ad dev <none> arn:aws:iam::123456789012:role/eksctl-fargate-example-cluster-ServiceRole-1T5F78E5FSH79 subnet-00adf1d8c99f83381,subnet-04affb163ffab17d4,subnet-035b34379d5ef5473 And to see them in yaml format: 1 2 3 4 5 6 7 8 9 $ eksctl get fargateprofile --cluster fargate-example-cluster -o yaml - name: fp-9bfc77ad podExecutionRoleARN: arn:aws:iam::123456789012:role/eksctl-fargate-example-cluster-ServiceRole-1T5F78E5FSH79 selectors: - namespace: dev subnets: - subnet-00adf1d8c99f83381 - subnet-04affb163ffab17d4 - subnet-035b34379d5ef5473 Or in json format: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ eksctl get fargateprofile --cluster fargate-example-cluster -o json [ { \"name\": \"fp-9bfc77ad\", \"podExecutionRoleARN\": \"arn:aws:iam::123456789012:role/eksctl-fargate-example-cluster-ServiceRole-1T5F78E5FSH79\", \"selectors\": [ { \"namespace\": \"dev\" } ], \"subnets\": [ \"subnet-00adf1d8c99f83381\", \"subnet-04affb163ffab17d4\", \"subnet-035b34379d5ef5473\" ] } ] Fargate profiles are immutable by design. To change something, create a new Fargate profile with the desired changes and delete the old one with the eksctl delete fargateprofile command like in the following example: 1 2 3 4 5 $ eksctl delete fargateprofile --cluster fargate-example-cluster --name fp-9bfc77ad --wait 2019-11-27T19:04:26+09:00 [\u2139] deleting Fargate profile \"fp-9bfc77ad\" ClusterName: \"fargate-example-cluster\", FargateProfileName: \"fp-9bfc77ad\" } Note that the profile deletion is a process that can take up to a few minutes. When the --wait flag is not specified, eksctl optimistically expects the profile to be deleted and returns as soon as the AWS API request has been sent. To make eksctl wait until the profile has been successfully deleted, use --wait like in the example above.","title":"Managing Fargate profiles"},{"location":"usage/fargate-support/#further-reading","text":"Fargate Fargate from EKS","title":"Further reading"},{"location":"usage/gpu-support/","text":"GPU Support \u00b6 Warning If you'd like to use GPU instance types, that is, p2 or p3 types, then the first thing you need to do is subscribe to the EKS-optimized AMI with GPU Support . If you don't do this then node creation will fail. After subscribing to the AMI you can create a cluster specifying the GPU instance type you'd like to use for the nodes. For example: 1 eksctl create cluster --node-type=p2.xlarge The AMI resolvers ( static , auto and auto-ssm ) will see that you want to use a GPU instance type (p2 or p3 only) and they will select the correct AMI. Once the cluster is created you will need to install the NVIDIA Kubernetes device plugin . Check the repo for the most up to date instructions but you should be able to run this: 1 kubectl create - f https : // raw . githubusercontent . com / NVIDIA / k8s - device - plugin / v1 . 11 / nvidia - device - plugin . yml Note Once addon support has been added as part of 0.2.0 it is envisioned that there will be a addon to install the NVIDIA Kubernetes Device Plugin. This addon could potentially be installed automatically as we know an GPU instance type is being used.","title":"GPU Support"},{"location":"usage/gpu-support/#gpu-support","text":"Warning If you'd like to use GPU instance types, that is, p2 or p3 types, then the first thing you need to do is subscribe to the EKS-optimized AMI with GPU Support . If you don't do this then node creation will fail. After subscribing to the AMI you can create a cluster specifying the GPU instance type you'd like to use for the nodes. For example: 1 eksctl create cluster --node-type=p2.xlarge The AMI resolvers ( static , auto and auto-ssm ) will see that you want to use a GPU instance type (p2 or p3 only) and they will select the correct AMI. Once the cluster is created you will need to install the NVIDIA Kubernetes device plugin . Check the repo for the most up to date instructions but you should be able to run this: 1 kubectl create - f https : // raw . githubusercontent . com / NVIDIA / k8s - device - plugin / v1 . 11 / nvidia - device - plugin . yml Note Once addon support has been added as part of 0.2.0 it is envisioned that there will be a addon to install the NVIDIA Kubernetes Device Plugin. This addon could potentially be installed automatically as we know an GPU instance type is being used.","title":"GPU Support"},{"location":"usage/iam-identity-mappings/","text":"IAM users and roles \u00b6 EKS clusters use IAM users and roles to control access to the cluster. The rules are implemented in a config map called aws-auth . eksctl provides commands to read and edit this config map. Get all identity mappings: 1 eksctl get iamidentitymapping --cluster my-cluster-1 Get all identity mappings matching an arn: 1 eksctl get iamidentitymapping --cluster my-cluster-1 --arn arn:aws:iam::123456:role/testing-role Create an identity mapping: 1 eksctl create iamidentitymapping --cluster my-cluster-1 --arn arn:aws:iam::123456:role/testing --group system:masters --username admin Delete a mapping: 1 eksctl delete iamidentitymapping --cluster my-cluster-1 --arn arn:aws:iam::123456:role/testing Note Above command deletes a single mapping FIFO unless --all is given in which case it removes all matching. Will warn if more mappings matching this role are found.","title":"IAM users and roles"},{"location":"usage/iam-identity-mappings/#iam-users-and-roles","text":"EKS clusters use IAM users and roles to control access to the cluster. The rules are implemented in a config map called aws-auth . eksctl provides commands to read and edit this config map. Get all identity mappings: 1 eksctl get iamidentitymapping --cluster my-cluster-1 Get all identity mappings matching an arn: 1 eksctl get iamidentitymapping --cluster my-cluster-1 --arn arn:aws:iam::123456:role/testing-role Create an identity mapping: 1 eksctl create iamidentitymapping --cluster my-cluster-1 --arn arn:aws:iam::123456:role/testing --group system:masters --username admin Delete a mapping: 1 eksctl delete iamidentitymapping --cluster my-cluster-1 --arn arn:aws:iam::123456:role/testing Note Above command deletes a single mapping FIFO unless --all is given in which case it removes all matching. Will warn if more mappings matching this role are found.","title":"IAM users and roles"},{"location":"usage/iam-permissions-boundary/","text":"IAM permissions boundary \u00b6 A permissions boundary is an advanced AWS IAM feature in which the maximum permissions that an identity-based policy can grant to an IAM entity have been set; where those entities are either users or roles. When a permissions boundary is set for an entity, that entity can only perform the actions that are allowed by both its identity-based policies and its permissions boundaries. You can provide your permissions boundary so that all identity-based entities created by eksctl are created within that boundary. This example demonstrates how a permissions boundary can be provided to the various identity-based entities that are created by eksctl: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-17 region : us-west-2 iam : withOIDC : true serviceRolePermissionsBoundary : \"arn:aws:iam:11111:policy/entity/boundary\" fargatePodExecutionRolePermissionsBoundary : \"arn:aws:iam::11111:policy/entity/boundary\" serviceAccounts : - metadata : name : s3-reader attachPolicyARNs : - \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\" permissionsBoundary : \"arn:aws:iam::11111:policy/entity/boundary\" nodeGroups : - name : \"ng-1\" desiredCapacity : 1 iam : instanceRolePermissionsBoundary : \"arn:aws:iam::11111:policy/entity/boundary\" Warning It is not possible to provide both a role ARN and a permissions boundary!","title":"IAM permissions boundary"},{"location":"usage/iam-permissions-boundary/#iam-permissions-boundary","text":"A permissions boundary is an advanced AWS IAM feature in which the maximum permissions that an identity-based policy can grant to an IAM entity have been set; where those entities are either users or roles. When a permissions boundary is set for an entity, that entity can only perform the actions that are allowed by both its identity-based policies and its permissions boundaries. You can provide your permissions boundary so that all identity-based entities created by eksctl are created within that boundary. This example demonstrates how a permissions boundary can be provided to the various identity-based entities that are created by eksctl: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-17 region : us-west-2 iam : withOIDC : true serviceRolePermissionsBoundary : \"arn:aws:iam:11111:policy/entity/boundary\" fargatePodExecutionRolePermissionsBoundary : \"arn:aws:iam::11111:policy/entity/boundary\" serviceAccounts : - metadata : name : s3-reader attachPolicyARNs : - \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\" permissionsBoundary : \"arn:aws:iam::11111:policy/entity/boundary\" nodeGroups : - name : \"ng-1\" desiredCapacity : 1 iam : instanceRolePermissionsBoundary : \"arn:aws:iam::11111:policy/entity/boundary\" Warning It is not possible to provide both a role ARN and a permissions boundary!","title":"IAM permissions boundary"},{"location":"usage/iam-policies/","text":"IAM policies \u00b6 Supported IAM add-on policies \u00b6 Example of all supported add-on policies: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 nodeGroups : - name : ng-1 instanceType : m5.xlarge desiredCapacity : 1 iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true Image Builder Policy \u00b6 The imageBuilder policy allows for full ECR (Elastic Container Registry) access. This is useful for building, for example, a CI server that needs to push images to ECR. EBS Policy \u00b6 The ebs policy enables the new EBS CSI (Elastic Block Store Container Storage Interface) driver. Adding a custom instance role \u00b6 This example creates a nodegroup that reuses an existing IAM Instance Role from another cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : eksctl.io/v1alpha4 kind : ClusterConfig metadata : name : test-cluster-c-1 region : eu-north-1 nodeGroups : - name : ng2-private instanceType : m5.large desiredCapacity : 1 iam : instanceProfileARN : \"arn:aws:iam::123:instance-profile/eksctl-test-cluster-a-3-nodegroup-ng2-private-NodeInstanceProfile-Y4YKHLNINMXC\" instanceRoleARN : \"arn:aws:iam::123:role/eksctl-test-cluster-a-3-nodegroup-NodeInstanceRole-DNGMQTQHQHBJ\" Attaching policies by ARN \u00b6 1 2 3 4 5 6 7 8 9 10 11 nodeGroups : - name : my-special-nodegroup iam : attachPolicyARNs : - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy - arn:aws:iam::aws:policy/ElasticLoadBalancingFullAccess - arn:aws:iam::1111111111:policy/kube2iam withAddonPolicies : autoScaler : true imageBuilder : true Warning If a nodegroup includes the attachPolicyARNs it must also include the default node policies, like AmazonEKSWorkerNodePolicy and AmazonEKS_CNI_Policy in this example.","title":"IAM policies"},{"location":"usage/iam-policies/#iam-policies","text":"","title":"IAM policies"},{"location":"usage/iam-policies/#supported-iam-add-on-policies","text":"Example of all supported add-on policies: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 nodeGroups : - name : ng-1 instanceType : m5.xlarge desiredCapacity : 1 iam : withAddonPolicies : imageBuilder : true autoScaler : true externalDNS : true certManager : true appMesh : true ebs : true fsx : true efs : true albIngress : true xRay : true cloudWatch : true","title":"Supported IAM add-on policies"},{"location":"usage/iam-policies/#image-builder-policy","text":"The imageBuilder policy allows for full ECR (Elastic Container Registry) access. This is useful for building, for example, a CI server that needs to push images to ECR.","title":"Image Builder Policy"},{"location":"usage/iam-policies/#ebs-policy","text":"The ebs policy enables the new EBS CSI (Elastic Block Store Container Storage Interface) driver.","title":"EBS Policy"},{"location":"usage/iam-policies/#adding-a-custom-instance-role","text":"This example creates a nodegroup that reuses an existing IAM Instance Role from another cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : eksctl.io/v1alpha4 kind : ClusterConfig metadata : name : test-cluster-c-1 region : eu-north-1 nodeGroups : - name : ng2-private instanceType : m5.large desiredCapacity : 1 iam : instanceProfileARN : \"arn:aws:iam::123:instance-profile/eksctl-test-cluster-a-3-nodegroup-ng2-private-NodeInstanceProfile-Y4YKHLNINMXC\" instanceRoleARN : \"arn:aws:iam::123:role/eksctl-test-cluster-a-3-nodegroup-NodeInstanceRole-DNGMQTQHQHBJ\"","title":"Adding a custom instance role"},{"location":"usage/iam-policies/#attaching-policies-by-arn","text":"1 2 3 4 5 6 7 8 9 10 11 nodeGroups : - name : my-special-nodegroup iam : attachPolicyARNs : - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy - arn:aws:iam::aws:policy/ElasticLoadBalancingFullAccess - arn:aws:iam::1111111111:policy/kube2iam withAddonPolicies : autoScaler : true imageBuilder : true Warning If a nodegroup includes the attachPolicyARNs it must also include the default node policies, like AmazonEKSWorkerNodePolicy and AmazonEKS_CNI_Policy in this example.","title":"Attaching policies by ARN"},{"location":"usage/iamserviceaccounts/","text":"IAM roles for service sccounts \u00b6 Introduction \u00b6 Amazon EKS supports IAM Roles for Service Accounts (IRSA) that allows cluster operators to map AWS IAM Roles to Kubernetes Service Accounts. This provides fine-grained permission management for apps that run on EKS and use other AWS services. These could be apps that use S3, any other data services (RDS, MQ, STS, DynamoDB), or Kubernetes components like AWS ALB Ingress controller or ExternalDNS. You can easily create IAM Role and Service Account pairs with eksctl . NOTE: if you used instance roles , and are considering to use IRSA instead, you shouldn't mix the two. How it works \u00b6 It works via IAM OpenID Connect Provider (OIDC) that EKS exposes, and IAM Roles must be constructed with reference to the IAM OIDC Provider (specific to a given EKS cluster), and a reference to the Kubernetes Service Account it will be bound to. Once an IAM Role is created, a service account should include the ARN of that role as an annotation ( eks.amazonaws.com/role-arn ). Inside EKS, there is an admission controller that injects AWS session credentials into pods respectively of the roles based on the annotation on the Service Account used by the pod. The credentials will get exposed by AWS_ROLE_ARN & AWS_WEB_IDENTITY_TOKEN_FILE environment variables. Given a recent version of AWS SDK is used (see AWS documentation for details of exact version), the application will use these credentials. In eksctl the name of the resource is iamserviceaccount , which represents an IAM Role and Service Account pair. Usage without config files \u00b6 NOTE: IAM Roles for Service Accounts require Kubernetes version 1.13 or above. The IAM OIDC Provider is not enabled by default, you can use the following command to enable it, or use config file (see below): 1 eksctl utils associate-iam-oidc-provider --cluster=<clusterName> Once you have the IAM OIDC Provider associated with the cluster, to create a IAM role bound to a service account, run: 1 eksctl create iamserviceaccount --cluster=<clusterName> --name=<serviceAccountName> --namespace=<serviceAccountNamespace> --attach-policy-arn=<policyARN> NOTE: you can specify --attach-policy-arn multiple times to use more then one policy. More specifically, you can create a service account with read-only access to S3 by running: 1 eksctl create iamserviceaccount --cluster=<clusterName> --name=s3-read-only --attach-policy-arn=arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess By default, it will be created in default namespace, but you can specify any other namespace, e.g.: 1 eksctl create iamserviceaccount --cluster=<clusterName> --name=s3-read-only --namespace=s3-app --attach-policy-arn=arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess NOTE: If the namespace doesn't exist already, it will be created. If you have service account already created in the cluster (without an IAM Role), you will need to use --override-existing-serviceaccounts flag. Currently, to update a role you will need to re-create, run eksctl delete iamserviceaccount followed by eksctl create iamserviceaccount to achieve that. Usage with config files \u00b6 To manage iamserviceaccounts using config file, you will be looking to set iam.withOIDC: true and list account you want under iam.serviceAccount . All of the commands support --config-file , you can manage iamserviceaccounts the same way as nodegroups . The eksctl create iamserviceaccount command supports --include and --exclude flags. And the eksctl delete iamserviceaccount command supports --only-missing as well, so you can perform deletions the same way as nodegroups. You use the following config example with eksctl create cluster : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # An example of ClusterConfig with IAMServiceAccounts: --- apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-13 region : us-west-2 iam : withOIDC : true serviceAccounts : - metadata : name : s3-reader # if no namespace is set, \"default\" will be used; # the namespace will be created if it doesn't exist already namespace : backend-apps labels : { aws-usage : \"application\" } attachPolicyARNs : - \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\" - metadata : name : cache-access namespace : backend-apps labels : { aws-usage : \"application\" } attachPolicyARNs : - \"arn:aws:iam::aws:policy/AmazonDynamoDBReadOnlyAccess\" - \"arn:aws:iam::aws:policy/AmazonElastiCacheFullAccess\" - metadata : name : cluster-autoscaler namespace : kube-system labels : { aws-usage : \"cluster-ops\" } attachPolicy : # inline policy can be defined along with `attachPolicyARNs` Version : \"2012-10-17\" Statement : - Effect : Allow Action : - \"autoscaling:DescribeAutoScalingGroups\" - \"autoscaling:DescribeAutoScalingInstances\" - \"autoscaling:DescribeLaunchConfigurations\" - \"autoscaling:DescribeTags\" - \"autoscaling:SetDesiredCapacity\" - \"autoscaling:TerminateInstanceInAutoScalingGroup\" Resource : '*' nodeGroups : - name : \"ng-1\" tags : # EC2 tags required for cluster-autoscaler auto-discovery k8s.io/cluster-autoscaler/enabled : \"true\" k8s.io/cluster-autoscaler/cluster-13 : \"owned\" desiredCapacity : 1 If you create a cluster without these fields set, you can use the following commands to enable all you need: 1 2 eksctl utils associate-iam-oidc-provider --config-file=<path> eksctl create iamserviceaccount --config-file=<path> Further information \u00b6 Introducing Fine-grained IAM Roles For Service Accounts AWS EKS User Guide - IAM Roles For Service Accounts Mapping IAM users and role to Kubernetes RBAC roles","title":"IAM roles for service sccounts"},{"location":"usage/iamserviceaccounts/#iam-roles-for-service-sccounts","text":"","title":"IAM roles for service sccounts"},{"location":"usage/iamserviceaccounts/#introduction","text":"Amazon EKS supports IAM Roles for Service Accounts (IRSA) that allows cluster operators to map AWS IAM Roles to Kubernetes Service Accounts. This provides fine-grained permission management for apps that run on EKS and use other AWS services. These could be apps that use S3, any other data services (RDS, MQ, STS, DynamoDB), or Kubernetes components like AWS ALB Ingress controller or ExternalDNS. You can easily create IAM Role and Service Account pairs with eksctl . NOTE: if you used instance roles , and are considering to use IRSA instead, you shouldn't mix the two.","title":"Introduction"},{"location":"usage/iamserviceaccounts/#how-it-works","text":"It works via IAM OpenID Connect Provider (OIDC) that EKS exposes, and IAM Roles must be constructed with reference to the IAM OIDC Provider (specific to a given EKS cluster), and a reference to the Kubernetes Service Account it will be bound to. Once an IAM Role is created, a service account should include the ARN of that role as an annotation ( eks.amazonaws.com/role-arn ). Inside EKS, there is an admission controller that injects AWS session credentials into pods respectively of the roles based on the annotation on the Service Account used by the pod. The credentials will get exposed by AWS_ROLE_ARN & AWS_WEB_IDENTITY_TOKEN_FILE environment variables. Given a recent version of AWS SDK is used (see AWS documentation for details of exact version), the application will use these credentials. In eksctl the name of the resource is iamserviceaccount , which represents an IAM Role and Service Account pair.","title":"How it works"},{"location":"usage/iamserviceaccounts/#usage-without-config-files","text":"NOTE: IAM Roles for Service Accounts require Kubernetes version 1.13 or above. The IAM OIDC Provider is not enabled by default, you can use the following command to enable it, or use config file (see below): 1 eksctl utils associate-iam-oidc-provider --cluster=<clusterName> Once you have the IAM OIDC Provider associated with the cluster, to create a IAM role bound to a service account, run: 1 eksctl create iamserviceaccount --cluster=<clusterName> --name=<serviceAccountName> --namespace=<serviceAccountNamespace> --attach-policy-arn=<policyARN> NOTE: you can specify --attach-policy-arn multiple times to use more then one policy. More specifically, you can create a service account with read-only access to S3 by running: 1 eksctl create iamserviceaccount --cluster=<clusterName> --name=s3-read-only --attach-policy-arn=arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess By default, it will be created in default namespace, but you can specify any other namespace, e.g.: 1 eksctl create iamserviceaccount --cluster=<clusterName> --name=s3-read-only --namespace=s3-app --attach-policy-arn=arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess NOTE: If the namespace doesn't exist already, it will be created. If you have service account already created in the cluster (without an IAM Role), you will need to use --override-existing-serviceaccounts flag. Currently, to update a role you will need to re-create, run eksctl delete iamserviceaccount followed by eksctl create iamserviceaccount to achieve that.","title":"Usage without config files"},{"location":"usage/iamserviceaccounts/#usage-with-config-files","text":"To manage iamserviceaccounts using config file, you will be looking to set iam.withOIDC: true and list account you want under iam.serviceAccount . All of the commands support --config-file , you can manage iamserviceaccounts the same way as nodegroups . The eksctl create iamserviceaccount command supports --include and --exclude flags. And the eksctl delete iamserviceaccount command supports --only-missing as well, so you can perform deletions the same way as nodegroups. You use the following config example with eksctl create cluster : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # An example of ClusterConfig with IAMServiceAccounts: --- apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-13 region : us-west-2 iam : withOIDC : true serviceAccounts : - metadata : name : s3-reader # if no namespace is set, \"default\" will be used; # the namespace will be created if it doesn't exist already namespace : backend-apps labels : { aws-usage : \"application\" } attachPolicyARNs : - \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\" - metadata : name : cache-access namespace : backend-apps labels : { aws-usage : \"application\" } attachPolicyARNs : - \"arn:aws:iam::aws:policy/AmazonDynamoDBReadOnlyAccess\" - \"arn:aws:iam::aws:policy/AmazonElastiCacheFullAccess\" - metadata : name : cluster-autoscaler namespace : kube-system labels : { aws-usage : \"cluster-ops\" } attachPolicy : # inline policy can be defined along with `attachPolicyARNs` Version : \"2012-10-17\" Statement : - Effect : Allow Action : - \"autoscaling:DescribeAutoScalingGroups\" - \"autoscaling:DescribeAutoScalingInstances\" - \"autoscaling:DescribeLaunchConfigurations\" - \"autoscaling:DescribeTags\" - \"autoscaling:SetDesiredCapacity\" - \"autoscaling:TerminateInstanceInAutoScalingGroup\" Resource : '*' nodeGroups : - name : \"ng-1\" tags : # EC2 tags required for cluster-autoscaler auto-discovery k8s.io/cluster-autoscaler/enabled : \"true\" k8s.io/cluster-autoscaler/cluster-13 : \"owned\" desiredCapacity : 1 If you create a cluster without these fields set, you can use the following commands to enable all you need: 1 2 eksctl utils associate-iam-oidc-provider --config-file=<path> eksctl create iamserviceaccount --config-file=<path>","title":"Usage with config files"},{"location":"usage/iamserviceaccounts/#further-information","text":"Introducing Fine-grained IAM Roles For Service Accounts AWS EKS User Guide - IAM Roles For Service Accounts Mapping IAM users and role to Kubernetes RBAC roles","title":"Further information"},{"location":"usage/managing-nodegroups/","text":"Managing nodegroups \u00b6 You can add one or more nodegroups in addition to the initial nodegroup created along with the cluster. To create an additional nodegroup, use: 1 eksctl create nodegroup --cluster=<clusterName> [--name=<nodegroupName>] NOTE: By default, new nodegroups inherit the version from the control plane ( --version=auto ), but you can specify a different version e.g. --version=1.10 , you can also use --version=latest to force use of whichever is the latest version. Additionally, you can use the same config file used for eksctl create cluster : 1 eksctl create nodegroup --config-file=<path> If there are multiple nodegroups specified in the file, you can select a subset via --include=<glob,glob,...> and --exclude=<glob,glob,...> : 1 eksctl create nodegroup -- config - file =< path > -- include = ' ng-prod-*-?? ' -- exclude = ' ng-test-1-ml-a,ng-test-2-? ' Creating a nodegroup from a config file \u00b6 Nodegroups can also be created through a cluster definition or config file. Given the following example config file and an existing cluster called ``dev-cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : dev-cluster region : eu-north-1 nodeGroups : - name : ng-1-workers labels : { role : workers } instanceType : m5.xlarge desiredCapacity : 10 privateNetworking : true - name : ng-2-builders labels : { role : builders } instanceType : m5.2xlarge desiredCapacity : 2 privateNetworking : true The nodegroups ng-1-workers and ng-2-builders can be created with this command: 1 eksctl create nodegroup --config-file = dev-cluster.yaml Listing nodegroups \u00b6 To list the details about a nodegroup or all of the nodegroups, use: 1 eksctl get nodegroup --cluster=<clusterName> [--name=<nodegroupName>] Nodegroup immutability \u00b6 By design, nodegroups are immutable. This means that if you need to change something (other than scaling) like the AMI or the instance type of a nodegroup, you would need to create a new nodegroup with the desired changes, move the load and delete the old one. Check Deleting and draining . Scaling \u00b6 A nodegroup can be scaled by using the eksctl scale nodegroup command: 1 eksctl scale nodegroup --cluster=<clusterName> --nodes=<desiredCount> --name=<nodegroupName> For example, to scale nodegroup ng-a345f4e1 in cluster-1 to 5 nodes, run: 1 eksctl scale nodegroup --cluster=cluster-1 --nodes=5 ng-a345f4e1 If the desired number of nodes is greater than the current maximum set on the ASG then the maximum value will be increased to match the number of requested nodes. And likewise for the minimum. Scaling a nodegroup works by modifying the nodegroup CloudFormation stack via a ChangeSet. NOTE: Scaling a nodegroup down/in (i.e. reducing the number of nodes) may result in errors as we rely purely on changes to the ASG. This means that the node(s) being removed/terminated aren't explicitly drained. This may be an area for improvement in the future. You can also enable SSH, ASG access and other feature for each particular nodegroup, e.g.: 1 eksctl create nodegroup --cluster=cluster-1 --node-labels=\"autoscaling=enabled,purpose=ci-worker\" --asg-access --full-ecr-access --ssh-access Update labels \u00b6 There are no specific commands in eksctl to update the labels of a nodegroup but that can easily be achieved using kubectl : 1 kubectl label nodes -l alpha.eksctl.io/nodegroup-name = ng-1 new-label = foo Deleting and draining \u00b6 To delete a nodegroup, run: 1 eksctl delete nodegroup --cluster=<clusterName> --name=<nodegroupName> NOTE: this will drain all pods from that nodegroup before the instances are deleted. All nodes are cordoned and all pods are evicted from a nodegroup on deletion, but if you need to drain a nodegroup without deleting it, run: 1 eksctl drain nodegroup --cluster=<clusterName> --name=<nodegroupName> To uncordon a nodegroup, run: 1 eksctl drain nodegroup --cluster=<clusterName> --name=<nodegroupName> --undo Nodegroup selection in config files \u00b6 To perform a create or delete operation on only a subset of the nodegroups specified in a config file, there are two CLI flags: include and exclude . These accept a list of globs such as ng-dev-* , for example. Using the example config file above, one can create all the workers nodegroup except the workers one with the following command: 1 eksctl create nodegroup --config-file = dev-cluster.yaml --exclude = ng-1-workers Or one could delete the builders nodegroup with: 1 eksctl delete nodegroup --config-file = dev-cluster.yaml --include = ng-2-builders --approve In this case, we also need to supply the --approve command to actually delete the nodegroup.","title":"Managing nodegroups"},{"location":"usage/managing-nodegroups/#managing-nodegroups","text":"You can add one or more nodegroups in addition to the initial nodegroup created along with the cluster. To create an additional nodegroup, use: 1 eksctl create nodegroup --cluster=<clusterName> [--name=<nodegroupName>] NOTE: By default, new nodegroups inherit the version from the control plane ( --version=auto ), but you can specify a different version e.g. --version=1.10 , you can also use --version=latest to force use of whichever is the latest version. Additionally, you can use the same config file used for eksctl create cluster : 1 eksctl create nodegroup --config-file=<path> If there are multiple nodegroups specified in the file, you can select a subset via --include=<glob,glob,...> and --exclude=<glob,glob,...> : 1 eksctl create nodegroup -- config - file =< path > -- include = ' ng-prod-*-?? ' -- exclude = ' ng-test-1-ml-a,ng-test-2-? '","title":"Managing nodegroups"},{"location":"usage/managing-nodegroups/#creating-a-nodegroup-from-a-config-file","text":"Nodegroups can also be created through a cluster definition or config file. Given the following example config file and an existing cluster called ``dev-cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : dev-cluster region : eu-north-1 nodeGroups : - name : ng-1-workers labels : { role : workers } instanceType : m5.xlarge desiredCapacity : 10 privateNetworking : true - name : ng-2-builders labels : { role : builders } instanceType : m5.2xlarge desiredCapacity : 2 privateNetworking : true The nodegroups ng-1-workers and ng-2-builders can be created with this command: 1 eksctl create nodegroup --config-file = dev-cluster.yaml","title":"Creating a nodegroup from a config file"},{"location":"usage/managing-nodegroups/#listing-nodegroups","text":"To list the details about a nodegroup or all of the nodegroups, use: 1 eksctl get nodegroup --cluster=<clusterName> [--name=<nodegroupName>]","title":"Listing nodegroups"},{"location":"usage/managing-nodegroups/#nodegroup-immutability","text":"By design, nodegroups are immutable. This means that if you need to change something (other than scaling) like the AMI or the instance type of a nodegroup, you would need to create a new nodegroup with the desired changes, move the load and delete the old one. Check Deleting and draining .","title":"Nodegroup immutability"},{"location":"usage/managing-nodegroups/#scaling","text":"A nodegroup can be scaled by using the eksctl scale nodegroup command: 1 eksctl scale nodegroup --cluster=<clusterName> --nodes=<desiredCount> --name=<nodegroupName> For example, to scale nodegroup ng-a345f4e1 in cluster-1 to 5 nodes, run: 1 eksctl scale nodegroup --cluster=cluster-1 --nodes=5 ng-a345f4e1 If the desired number of nodes is greater than the current maximum set on the ASG then the maximum value will be increased to match the number of requested nodes. And likewise for the minimum. Scaling a nodegroup works by modifying the nodegroup CloudFormation stack via a ChangeSet. NOTE: Scaling a nodegroup down/in (i.e. reducing the number of nodes) may result in errors as we rely purely on changes to the ASG. This means that the node(s) being removed/terminated aren't explicitly drained. This may be an area for improvement in the future. You can also enable SSH, ASG access and other feature for each particular nodegroup, e.g.: 1 eksctl create nodegroup --cluster=cluster-1 --node-labels=\"autoscaling=enabled,purpose=ci-worker\" --asg-access --full-ecr-access --ssh-access","title":"Scaling"},{"location":"usage/managing-nodegroups/#update-labels","text":"There are no specific commands in eksctl to update the labels of a nodegroup but that can easily be achieved using kubectl : 1 kubectl label nodes -l alpha.eksctl.io/nodegroup-name = ng-1 new-label = foo","title":"Update labels"},{"location":"usage/managing-nodegroups/#deleting-and-draining","text":"To delete a nodegroup, run: 1 eksctl delete nodegroup --cluster=<clusterName> --name=<nodegroupName> NOTE: this will drain all pods from that nodegroup before the instances are deleted. All nodes are cordoned and all pods are evicted from a nodegroup on deletion, but if you need to drain a nodegroup without deleting it, run: 1 eksctl drain nodegroup --cluster=<clusterName> --name=<nodegroupName> To uncordon a nodegroup, run: 1 eksctl drain nodegroup --cluster=<clusterName> --name=<nodegroupName> --undo","title":"Deleting and draining"},{"location":"usage/managing-nodegroups/#nodegroup-selection-in-config-files","text":"To perform a create or delete operation on only a subset of the nodegroups specified in a config file, there are two CLI flags: include and exclude . These accept a list of globs such as ng-dev-* , for example. Using the example config file above, one can create all the workers nodegroup except the workers one with the following command: 1 eksctl create nodegroup --config-file = dev-cluster.yaml --exclude = ng-1-workers Or one could delete the builders nodegroup with: 1 eksctl delete nodegroup --config-file = dev-cluster.yaml --include = ng-2-builders --approve In this case, we also need to supply the --approve command to actually delete the nodegroup.","title":"Nodegroup selection in config files"},{"location":"usage/schema/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 ClusterCloudWatch : additionalProperties : false properties : clusterLogging : $ref : '#/definitions/ClusterCloudWatchLogging' $schema : http://json-schema.org/draft-04/schema# type : object ClusterCloudWatchLogging : additionalProperties : false properties : enableTypes : items : type : string type : array type : object ClusterConfig : additionalProperties : false properties : TypeMeta : $ref : '#/definitions/TypeMeta' $schema : http://json-schema.org/draft-04/schema# availabilityZones : items : type : string type : array cloudWatch : $ref : '#/definitions/ClusterCloudWatch' $schema : http://json-schema.org/draft-04/schema# fargateProfiles : items : $ref : '#/definitions/FargateProfile' $schema : http://json-schema.org/draft-04/schema# type : array iam : $ref : '#/definitions/ClusterIAM' $schema : http://json-schema.org/draft-04/schema# managedNodeGroups : items : $ref : '#/definitions/ManagedNodeGroup' $schema : http://json-schema.org/draft-04/schema# type : array metadata : $ref : '#/definitions/ClusterMeta' $schema : http://json-schema.org/draft-04/schema# nodeGroups : items : $ref : '#/definitions/NodeGroup' $schema : http://json-schema.org/draft-04/schema# type : array secretsEncryption : $ref : '#/definitions/SecretsEncryption' $schema : http://json-schema.org/draft-04/schema# status : $ref : '#/definitions/ClusterStatus' $schema : http://json-schema.org/draft-04/schema# vpc : $ref : '#/definitions/ClusterVPC' $schema : http://json-schema.org/draft-04/schema# required : - TypeMeta - metadata type : object ClusterEndpoints : additionalProperties : false properties : privateAccess : type : boolean publicAccess : type : boolean type : object ClusterIAM : additionalProperties : false properties : fargatePodExecutionRoleARN : type : string fargatePodExecutionRolePermissionsBoundary : type : string serviceAccounts : items : $ref : '#/definitions/ClusterIAMServiceAccount' $schema : http://json-schema.org/draft-04/schema# type : array serviceRoleARN : type : string serviceRolePermissionsBoundary : type : string withOIDC : type : boolean type : object ClusterIAMServiceAccount : additionalProperties : false properties : attachPolicy : patternProperties : .* : additionalProperties : true type : object type : object attachPolicyARNs : items : type : string type : array metadata : $ref : '#/definitions/ObjectMeta' $schema : http://json-schema.org/draft-04/schema# permissionsBoundary : type : string status : $ref : '#/definitions/ClusterIAMServiceAccountStatus' $schema : http://json-schema.org/draft-04/schema# type : object ClusterIAMServiceAccountStatus : additionalProperties : false properties : roleARN : type : string type : object ClusterMeta : additionalProperties : false properties : name : type : string region : type : string tags : patternProperties : .* : type : string type : object version : type : string required : - name - region type : object ClusterNAT : additionalProperties : false properties : gateway : type : string type : object ClusterStatus : additionalProperties : false properties : arn : type : string certificateAuthorityData : media : binaryEncoding : base64 type : string endpoint : type : string stackName : type : string type : object ClusterSubnets : additionalProperties : false properties : private : patternProperties : .* : $ref : '#/definitions/Network' type : object public : patternProperties : .* : $ref : '#/definitions/Network' type : object type : object ClusterVPC : additionalProperties : false properties : Network : $ref : '#/definitions/Network' $schema : http://json-schema.org/draft-04/schema# autoAllocateIPv6 : type : boolean clusterEndpoints : $ref : '#/definitions/ClusterEndpoints' $schema : http://json-schema.org/draft-04/schema# extraCIDRs : items : $ref : '#/definitions/IPNet' type : array nat : $ref : '#/definitions/ClusterNAT' $schema : http://json-schema.org/draft-04/schema# publicAccessCIDRs : items : type : string type : array securityGroup : type : string sharedNodeSecurityGroup : type : string subnets : $ref : '#/definitions/ClusterSubnets' $schema : http://json-schema.org/draft-04/schema# required : - Network type : object FargateProfile : additionalProperties : false properties : name : type : string podExecutionRoleARN : type : string selectors : items : $ref : '#/definitions/FargateProfileSelector' $schema : http://json-schema.org/draft-04/schema# type : array subnets : items : type : string type : array tags : patternProperties : .* : type : string type : object required : - name - selectors type : object FargateProfileSelector : additionalProperties : false properties : labels : patternProperties : .* : type : string type : object namespace : type : string required : - namespace type : object IPNet : additionalProperties : false properties : IP : format : ipv4 type : string Mask : items : type : integer type : array required : - IP - Mask type : object Initializer : additionalProperties : false properties : name : type : string required : - name type : object Initializers : additionalProperties : false properties : pending : items : $ref : '#/definitions/Initializer' $schema : http://json-schema.org/draft-04/schema# type : array result : $ref : '#/definitions/Status' $schema : http://json-schema.org/draft-04/schema# required : - pending type : object ListMeta : additionalProperties : false properties : continue : type : string resourceVersion : type : string selfLink : type : string type : object ManagedNodeGroup : additionalProperties : false properties : ScalingConfig : $ref : '#/definitions/ScalingConfig' $schema : http://json-schema.org/draft-04/schema# amiFamily : type : string availabilityZones : items : type : string type : array iam : $ref : '#/definitions/NodeGroupIAM' instanceType : type : string labels : patternProperties : .* : type : string type : object name : type : string ssh : $ref : '#/definitions/NodeGroupSSH' tags : patternProperties : .* : type : string type : object volumeSize : type : integer required : - name - ScalingConfig type : object Network : additionalProperties : false properties : cidr : $ref : '#/definitions/IPNet' $schema : http://json-schema.org/draft-04/schema# id : type : string type : object NodeGroup : additionalProperties : false properties : ami : type : string amiFamily : type : string availabilityZones : items : type : string type : array bottlerocket : $ref : '#/definitions/NodeGroupBottlerocket' $schema : http://json-schema.org/draft-04/schema# classicLoadBalancerNames : items : type : string type : array clusterDNS : type : string desiredCapacity : type : integer ebsOptimized : type : boolean iam : $ref : '#/definitions/NodeGroupIAM' $schema : http://json-schema.org/draft-04/schema# instanceType : type : string instancesDistribution : $ref : '#/definitions/NodeGroupInstancesDistribution' $schema : http://json-schema.org/draft-04/schema# kubeletExtraConfig : patternProperties : .* : additionalProperties : true type : object type : object labels : patternProperties : .* : type : string type : object maxPodsPerNode : type : integer maxSize : type : integer minSize : type : integer name : type : string overrideBootstrapCommand : type : string preBootstrapCommands : items : type : string type : array privateNetworking : type : boolean securityGroups : $ref : '#/definitions/NodeGroupSGs' $schema : http://json-schema.org/draft-04/schema# ssh : $ref : '#/definitions/NodeGroupSSH' $schema : http://json-schema.org/draft-04/schema# tags : patternProperties : .* : type : string type : object taints : patternProperties : .* : type : string type : object targetGroupARNs : items : type : string type : array volumeEncrypted : type : boolean volumeIOPS : type : integer volumeKmsKeyID : type : string volumeName : type : string volumeSize : type : integer volumeType : type : string required : - name - privateNetworking - volumeSize - volumeType - volumeIOPS - iam type : object NodeGroupBottlerocket : additionalProperties : false properties : enableAdminContainer : type : boolean settings : patternProperties : .* : additionalProperties : true type : object type : object type : object NodeGroupIAM : additionalProperties : false properties : attachPolicyARNs : items : type : string type : array instanceProfileARN : type : string instanceRoleARN : type : string instanceRoleName : type : string instanceRolePermissionsBoundary : type : string withAddonPolicies : $ref : '#/definitions/NodeGroupIAMAddonPolicies' $schema : http://json-schema.org/draft-04/schema# type : object NodeGroupIAMAddonPolicies : additionalProperties : false properties : albIngress : type : boolean appMesh : type : boolean autoScaler : type : boolean certManager : type : boolean cloudWatch : type : boolean ebs : type : boolean efs : type : boolean externalDNS : type : boolean fsx : type : boolean imageBuilder : type : boolean xRay : type : boolean required : - imageBuilder - autoScaler - externalDNS - certManager - appMesh - ebs - fsx - efs - albIngress - xRay - cloudWatch type : object NodeGroupInstancesDistribution : additionalProperties : false properties : instanceTypes : items : type : string type : array maxPrice : type : number onDemandBaseCapacity : type : integer onDemandPercentageAboveBaseCapacity : type : integer spotAllocationStrategy : type : string spotInstancePools : type : integer type : object NodeGroupSGs : additionalProperties : false properties : attachIDs : items : type : string type : array withLocal : type : boolean withShared : type : boolean required : - withShared - withLocal type : object NodeGroupSSH : additionalProperties : false properties : allow : type : boolean publicKey : type : string publicKeyName : type : string publicKeyPath : type : string sourceSecurityGroupIds : items : type : string type : array required : - allow type : object ObjectMeta : additionalProperties : false properties : annotations : patternProperties : .* : type : string type : object clusterName : type : string creationTimestamp : $ref : '#/definitions/Time' $schema : http://json-schema.org/draft-04/schema# deletionGracePeriodSeconds : type : integer deletionTimestamp : $ref : '#/definitions/Time' finalizers : items : type : string type : array generateName : type : string generation : type : integer initializers : $ref : '#/definitions/Initializers' $schema : http://json-schema.org/draft-04/schema# labels : patternProperties : .* : type : string type : object name : type : string namespace : type : string ownerReferences : items : $ref : '#/definitions/OwnerReference' $schema : http://json-schema.org/draft-04/schema# type : array resourceVersion : type : string selfLink : type : string uid : type : string type : object OwnerReference : additionalProperties : false properties : apiVersion : type : string blockOwnerDeletion : type : boolean controller : type : boolean kind : type : string name : type : string uid : type : string required : - apiVersion - kind - name - uid type : object ScalingConfig : additionalProperties : false properties : desiredCapacity : type : integer maxSize : type : integer minSize : type : integer type : object SecretsEncryption : additionalProperties : false properties : keyARN : type : string type : object Status : additionalProperties : false properties : TypeMeta : $ref : '#/definitions/TypeMeta' code : type : integer details : $ref : '#/definitions/StatusDetails' $schema : http://json-schema.org/draft-04/schema# message : type : string metadata : $ref : '#/definitions/ListMeta' $schema : http://json-schema.org/draft-04/schema# reason : type : string status : type : string required : - TypeMeta type : object StatusCause : additionalProperties : false properties : field : type : string message : type : string reason : type : string type : object StatusDetails : additionalProperties : false properties : causes : items : $ref : '#/definitions/StatusCause' $schema : http://json-schema.org/draft-04/schema# type : array group : type : string kind : type : string name : type : string retryAfterSeconds : type : integer uid : type : string type : object Time : additionalProperties : false type : object TypeMeta : additionalProperties : false properties : apiVersion : type : string kind : type : string type : object","title":"Schema"},{"location":"usage/spot-instances/","text":"Spot instances \u00b6 eksctl has support for spot instances through the MixedInstancesPolicy for Auto Scaling Groups. Here is an example of a nodegroup that uses 50% spot instances and 50% on demand instances: 1 2 3 4 5 6 7 8 9 10 nodeGroups : - name : ng-1 minSize : 2 maxSize : 5 instancesDistribution : maxPrice : 0.017 instanceTypes : [ \"t3.small\" , \"t3.medium\" ] # At least two instance types should be specified onDemandBaseCapacity : 0 onDemandPercentageAboveBaseCapacity : 50 spotInstancePools : 2 Note that the nodeGroups.X.instanceType field shouldn't be set when using the instancesDistribution field. This example uses GPU instances: 1 2 3 4 5 6 7 8 9 10 nodeGroups : - name : ng-gpu instanceType : mixed desiredCapacity : 1 instancesDistribution : instanceTypes : - p2.xlarge - p2.8xlarge - p2.16xlarge maxPrice : 0.50 Here is a minimal example: 1 2 3 4 nodeGroups : - name : ng-1 instancesDistribution : instanceTypes : [ \"t3.small\" , \"t3.medium\" ] # At least two instance types should be specified Parameters \u00b6 The parameters available in instancesDistribution are: type required default value instanceTypes []string required - maxPrice float optional on demand price onDemandBaseCapacity int optional 0 onDemandPercentageAboveBaseCapacity int [1-100] optional 100 spotInstancePools int [1-20] optional 2","title":"Spot instances"},{"location":"usage/spot-instances/#spot-instances","text":"eksctl has support for spot instances through the MixedInstancesPolicy for Auto Scaling Groups. Here is an example of a nodegroup that uses 50% spot instances and 50% on demand instances: 1 2 3 4 5 6 7 8 9 10 nodeGroups : - name : ng-1 minSize : 2 maxSize : 5 instancesDistribution : maxPrice : 0.017 instanceTypes : [ \"t3.small\" , \"t3.medium\" ] # At least two instance types should be specified onDemandBaseCapacity : 0 onDemandPercentageAboveBaseCapacity : 50 spotInstancePools : 2 Note that the nodeGroups.X.instanceType field shouldn't be set when using the instancesDistribution field. This example uses GPU instances: 1 2 3 4 5 6 7 8 9 10 nodeGroups : - name : ng-gpu instanceType : mixed desiredCapacity : 1 instancesDistribution : instanceTypes : - p2.xlarge - p2.8xlarge - p2.16xlarge maxPrice : 0.50 Here is a minimal example: 1 2 3 4 nodeGroups : - name : ng-1 instancesDistribution : instanceTypes : [ \"t3.small\" , \"t3.medium\" ] # At least two instance types should be specified","title":"Spot instances"},{"location":"usage/spot-instances/#parameters","text":"The parameters available in instancesDistribution are: type required default value instanceTypes []string required - maxPrice float optional on demand price onDemandBaseCapacity int optional 0 onDemandPercentageAboveBaseCapacity int [1-100] optional 100 spotInstancePools int [1-20] optional 2","title":"Parameters"},{"location":"usage/troubleshooting/","text":"Troubleshooting \u00b6 subnet ID \"subnet-11111111\" is not the same as \"subnet-22222222\" \u00b6 Given a config file specifying subnets for a VPC like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : test region : us-east-1 vpc : subnets : public : us-east-1a : { id : subnet-11111111 } us-east-1b : { id : subnet-22222222 } private : us-east-1a : { id : subnet-33333333 } us-east-1b : { id : subnet-44444444 } nodeGroups : [] An error subnet ID \"subnet-11111111\" is not the same as \"subnet-22222222\" means that the subnets specified are not placed in the right Availability zone. Check in the AWS console which is the right subnet ID for each Availability Zone. In this example, the correct configuration for the VPC would be: 1 2 3 4 5 6 7 8 vpc : subnets : public : us-east-1a : { id : subnet-22222222 } us-east-1b : { id : subnet-11111111 } private : us-east-1a : { id : subnet-33333333 } us-east-1b : { id : subnet-44444444 }","title":"Troubleshooting"},{"location":"usage/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"usage/troubleshooting/#subnet-id-subnet-11111111-is-not-the-same-as-subnet-22222222","text":"Given a config file specifying subnets for a VPC like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : test region : us-east-1 vpc : subnets : public : us-east-1a : { id : subnet-11111111 } us-east-1b : { id : subnet-22222222 } private : us-east-1a : { id : subnet-33333333 } us-east-1b : { id : subnet-44444444 } nodeGroups : [] An error subnet ID \"subnet-11111111\" is not the same as \"subnet-22222222\" means that the subnets specified are not placed in the right Availability zone. Check in the AWS console which is the right subnet ID for each Availability Zone. In this example, the correct configuration for the VPC would be: 1 2 3 4 5 6 7 8 vpc : subnets : public : us-east-1a : { id : subnet-22222222 } us-east-1b : { id : subnet-11111111 } private : us-east-1a : { id : subnet-33333333 } us-east-1b : { id : subnet-44444444 }","title":"subnet ID \"subnet-11111111\" is not the same as \"subnet-22222222\""},{"location":"usage/vpc-networking/","text":"VPC Networking \u00b6 By default, eksctl create cluster will build a dedicated VPC, in order to avoid interference with any existing resources for a variety of reasons, including security, but also because it's challenging to detect all the settings in an existing VPC. Default VPC CIDR used by eksctl is 192.168.0.0/16 , it is divided into 8 ( /19 ) subnets (3 private, 3 public & 2 reserved). Initial nodegroup is create in public subnets, with SSH access disabled unless --allow-ssh is specified. However, this implies that each of the EC2 instances in the initial nodegroup gets a public IP and can be accessed on ports 1025 - 65535, which is not insecure in principle, but some compromised workload could risk an access violation. If that functionality doesn't suit you, the following options are currently available. Change VPC CIDR \u00b6 If you need to setup peering with another VPC, or simply need larger or smaller range of IPs, you can use --vpc-cidr flag to change it. You cannot use just any sort of CIDR, there only certain ranges that can be used in AWS VPC . Use private subnets for initial nodegroup \u00b6 If you prefer to isolate initial nodegroup from the public internet, you can use --node-private-networking flag. When used in conjunction with --ssh-access flag, SSH port can only be accessed inside the VPC. Use existing VPC: shared with kops \u00b6 You can use a VPC of an existing Kubernetes cluster managed by kops. This feature is provided to facilitate migration and/or cluster peering. If you have previously created a cluster with kops, e.g. using commands similar to this: 1 2 export KOPS_STATE_STORE = s3 : // kops kops create cluster cluster - 1 . k8s . local --zones=us-west-2c,us-west-2b,us-west-2a --networking=weave --yes You can create an EKS cluster in the same AZs using the same VPC subnets (NOTE: at least 2 AZs/subnets are required): 1 eksctl create cluster --name=cluster-2 --region=us-west-2 --vpc-from-kops-cluster=cluster-1.k8s.local Use existing VPC: any custom configuration \u00b6 Use this feature if you must configure a VPC in a way that's different to how dedicated VPC is configured by eksctl , or have to use a VPC that already exists so your EKS cluster gets shared access to some resources inside that existing VPC, or you have any other use-case that requires you to manage VPCs separately. You can use an existing VPC by supplying private and/or public subnets using --vpc-private-subnets and --vpc-public-subnets flags. It is up to you to ensure which subnets you use, as there is no simple way to determine automatically whether a subnets is private or public, because configurations vary. Given these flags, eksctl create cluster will determine the VPC ID automatically, but it will not create any routing tables or other resources, such as internet/NAT gateways. It will, however, create dedicated security groups for the initial nodegroup and the control plane. You must ensure to provide at least 2 subnets in different AZs. There are other requirements that you will need to follow, but it's entirely up to you to address those. For example, tagging is not strictly necessary, tests have shown that its possible to create a functional cluster without any tags set on the subnets, however there is no guarantee that this will always hold and tagging is recommended. all subnets in the same VPC, within the same block of IPs sufficient IP addresses are available sufficient number of subnets (minimum 2) internet and/or NAT gateways are configured correctly routing tables have correct entries and the network is functional tagging of subnets kubernetes.io/cluster/<name> tag set to either shared or owned kubernetes.io/role/internal-elb tag set to 1 for private subnets There maybe other requirements imposed by EKS or Kubernetes, and it is entirely up to you to stay up-to-date on any requirements and/or recommendations, and implement those as needed/possible. Default security group settings applied by eksctl may or may not be sufficient for sharing access with resources in other security groups. If you wish to modify the ingress/egress rules of the either of security groups, you might need to use another tool to automate changes, or do it via EC2 console. If you are in doubt, don't use a custom VPC. Using eksctl create cluster without any --vpc-* flags will always configure the cluster with a fully-functional dedicated VPC. To create a cluster using 2x private and 2x public subnets, run: 1 2 3 eksctl create cluster \\ --vpc-private-subnets=subnet-0ff156e0c4a6d300c,subnet-0426fb4a607393184 \\ --vpc-public-subnets=subnet-0153e560b3129a696,subnet-009fa0199ec203c37 To create a cluster using 3x private subnets and make initial nodegroup use those subnets, run: 1 2 3 eksctl create cluster \\ --vpc-private-subnets=subnet-0ff156e0c4a6d300c,subnet-0549cdab573695c03,subnet-0426fb4a607393184 \\ --node-private-networking To create a cluster using 4x public subnets, run: 1 2 eksctl create cluster \\ --vpc-public-subnets=subnet-0153e560b3129a696,subnet-0cc9c5aebe75083fd,subnet-009fa0199ec203c37,subnet-018fa0176ba320e45 Custom Cluster DNS address \u00b6 There are two ways of overwriting the DNS server IP address used for all the internal and external DNs lookups (this is, the equivalent of the --cluster-dns flag for the kubelet ). The first, is through the clusterDNS field. Config files accept a string field called clusterDNS with the IP address of the DNS server to use. This will be passed to the kubelet that in turn will pass it to the pods through the /etc/resolv.conf file. 1 2 3 4 5 6 7 8 9 10 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-1 region : eu-north-1 nodeGroups : - name : ng-1 clusterDNS : 169.254.20.10 Note that this configuration only accepts one IP address. To specify more than one address, use the extraKubeletConfig parameter : 1 2 3 4 5 6 7 8 9 10 11 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-1 region : eu-north-1 nodeGroups : - name : ng-1 kubeletExtraConfig : clusterDNS : [ \"169.254.20.10\" , \"172.20.0.10\" ] NAT Gateway \u00b6 The NAT Gateway for a cluster can be configured to be Disabled , Single (default) or HighlyAvailable . It can be specified through the --vpc-nat-mode CLI flag or in the cluster config file like the example below: 1 2 3 vpc : nat : gateway : HighlyAvailable # other options : Disable, Single (default) See the complete example here . Note : Specifying the NAT Gateway is only supported during cluster creation and it is not touched during a cluster upgrade. There are plans to support changing between different modes on cluster update in the future. Managing Access to Kubernetes API Server Endpoints \u00b6 The default creation of an EKS cluster exposes the Kubernetes API server publicly but not directly from within the VPC subnets (public=true, private=false). Traffic destined for the API server from within the VPC must first exit the VPC networks (but not Amazon's network) and then re-enter to reach the API server. The Kubernetes API server endpoint access for a cluster can be configured for public and private access when creating the cluster using the cluster config file. Example below: 1 2 3 4 vpc : clusterEndpoints : publicAccess : <true|false> privateAccess : <true|false> There are some additional caveats when configuring Kubernetes API endpoint access: EKS doesn't allow one to create or update a cluster without at least one of private or public access being enabled. EKS does allow creating a configuration which allows only private access to be enabled, but eksctl doesn't support it during cluster creation as it prevents eksctl from being able to join the worker nodes to the cluster. To create private-only Kubernetes API endpoint access, one must first create the cluster with public Kubernetes API endpoint access, and then use eksctl utils update-cluster-endpoints to change it after the cluster is finished creating. Updating a cluster to have private only Kubernetes API endpoint access means that Kubernetes commands (e.g. kubectl ) as well as eksctl delete cluster , eksctl utils write-kubeconfig , and possibly the command eksctl utils update-kube-proxy must be run within the cluster VPC. This requires some changes to various AWS resources. See: EKS user guide The following is an example of how one could configure the Kubernetes API endpoint access using the utils sub-command: 1 eksctl utils update - cluster - endpoints --name=<clustername> --private-access=true --public-access=false Note that if you don't pass a flag in it will keep the current value. Once you are satisfied with the proposed changes, add the approve flag to make the change to the running cluster. Restricting Access to EKS Kubernetes Public API endpoint \u00b6 The default creation of an EKS cluster exposes the Kubernetes API server publicly. To restrict access to the public API endpoint to a set of CIDRs when creating a cluster, set the publicAccessCIDRs field: 1 2 vpc : publicAccessCIDRs : [ \"1.1.1.1/32\" , \"2.2.2.0/24\" ] To update the restrictions on an existing cluster, use: 1 eksctl utils set-public-access-cidrs --cluster=<cluster> 1.1.1.1/32,2.2.2.0/24 To update the restrictions using a ClusterConfig file, set the new CIDRs in vpc.publicAccessCIDRs and run: 1 eksctl utils set-public-access-cidrs -f config.yaml Note : This feature only applies to the public endpoint. The API server endpoint access configuration options won't change, and you will still have the option to disable the public endpoint so your cluster is not accessible from the Internet (source: AWS containers roadmap comment and implementation notes in comment ).","title":"VPC Networking"},{"location":"usage/vpc-networking/#vpc-networking","text":"By default, eksctl create cluster will build a dedicated VPC, in order to avoid interference with any existing resources for a variety of reasons, including security, but also because it's challenging to detect all the settings in an existing VPC. Default VPC CIDR used by eksctl is 192.168.0.0/16 , it is divided into 8 ( /19 ) subnets (3 private, 3 public & 2 reserved). Initial nodegroup is create in public subnets, with SSH access disabled unless --allow-ssh is specified. However, this implies that each of the EC2 instances in the initial nodegroup gets a public IP and can be accessed on ports 1025 - 65535, which is not insecure in principle, but some compromised workload could risk an access violation. If that functionality doesn't suit you, the following options are currently available.","title":"VPC Networking"},{"location":"usage/vpc-networking/#change-vpc-cidr","text":"If you need to setup peering with another VPC, or simply need larger or smaller range of IPs, you can use --vpc-cidr flag to change it. You cannot use just any sort of CIDR, there only certain ranges that can be used in AWS VPC .","title":"Change VPC CIDR"},{"location":"usage/vpc-networking/#use-private-subnets-for-initial-nodegroup","text":"If you prefer to isolate initial nodegroup from the public internet, you can use --node-private-networking flag. When used in conjunction with --ssh-access flag, SSH port can only be accessed inside the VPC.","title":"Use private subnets for initial nodegroup"},{"location":"usage/vpc-networking/#use-existing-vpc-shared-with-kops","text":"You can use a VPC of an existing Kubernetes cluster managed by kops. This feature is provided to facilitate migration and/or cluster peering. If you have previously created a cluster with kops, e.g. using commands similar to this: 1 2 export KOPS_STATE_STORE = s3 : // kops kops create cluster cluster - 1 . k8s . local --zones=us-west-2c,us-west-2b,us-west-2a --networking=weave --yes You can create an EKS cluster in the same AZs using the same VPC subnets (NOTE: at least 2 AZs/subnets are required): 1 eksctl create cluster --name=cluster-2 --region=us-west-2 --vpc-from-kops-cluster=cluster-1.k8s.local","title":"Use existing VPC: shared with kops"},{"location":"usage/vpc-networking/#use-existing-vpc-any-custom-configuration","text":"Use this feature if you must configure a VPC in a way that's different to how dedicated VPC is configured by eksctl , or have to use a VPC that already exists so your EKS cluster gets shared access to some resources inside that existing VPC, or you have any other use-case that requires you to manage VPCs separately. You can use an existing VPC by supplying private and/or public subnets using --vpc-private-subnets and --vpc-public-subnets flags. It is up to you to ensure which subnets you use, as there is no simple way to determine automatically whether a subnets is private or public, because configurations vary. Given these flags, eksctl create cluster will determine the VPC ID automatically, but it will not create any routing tables or other resources, such as internet/NAT gateways. It will, however, create dedicated security groups for the initial nodegroup and the control plane. You must ensure to provide at least 2 subnets in different AZs. There are other requirements that you will need to follow, but it's entirely up to you to address those. For example, tagging is not strictly necessary, tests have shown that its possible to create a functional cluster without any tags set on the subnets, however there is no guarantee that this will always hold and tagging is recommended. all subnets in the same VPC, within the same block of IPs sufficient IP addresses are available sufficient number of subnets (minimum 2) internet and/or NAT gateways are configured correctly routing tables have correct entries and the network is functional tagging of subnets kubernetes.io/cluster/<name> tag set to either shared or owned kubernetes.io/role/internal-elb tag set to 1 for private subnets There maybe other requirements imposed by EKS or Kubernetes, and it is entirely up to you to stay up-to-date on any requirements and/or recommendations, and implement those as needed/possible. Default security group settings applied by eksctl may or may not be sufficient for sharing access with resources in other security groups. If you wish to modify the ingress/egress rules of the either of security groups, you might need to use another tool to automate changes, or do it via EC2 console. If you are in doubt, don't use a custom VPC. Using eksctl create cluster without any --vpc-* flags will always configure the cluster with a fully-functional dedicated VPC. To create a cluster using 2x private and 2x public subnets, run: 1 2 3 eksctl create cluster \\ --vpc-private-subnets=subnet-0ff156e0c4a6d300c,subnet-0426fb4a607393184 \\ --vpc-public-subnets=subnet-0153e560b3129a696,subnet-009fa0199ec203c37 To create a cluster using 3x private subnets and make initial nodegroup use those subnets, run: 1 2 3 eksctl create cluster \\ --vpc-private-subnets=subnet-0ff156e0c4a6d300c,subnet-0549cdab573695c03,subnet-0426fb4a607393184 \\ --node-private-networking To create a cluster using 4x public subnets, run: 1 2 eksctl create cluster \\ --vpc-public-subnets=subnet-0153e560b3129a696,subnet-0cc9c5aebe75083fd,subnet-009fa0199ec203c37,subnet-018fa0176ba320e45","title":"Use existing VPC: any custom configuration"},{"location":"usage/vpc-networking/#custom-cluster-dns-address","text":"There are two ways of overwriting the DNS server IP address used for all the internal and external DNs lookups (this is, the equivalent of the --cluster-dns flag for the kubelet ). The first, is through the clusterDNS field. Config files accept a string field called clusterDNS with the IP address of the DNS server to use. This will be passed to the kubelet that in turn will pass it to the pods through the /etc/resolv.conf file. 1 2 3 4 5 6 7 8 9 10 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-1 region : eu-north-1 nodeGroups : - name : ng-1 clusterDNS : 169.254.20.10 Note that this configuration only accepts one IP address. To specify more than one address, use the extraKubeletConfig parameter : 1 2 3 4 5 6 7 8 9 10 11 apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-1 region : eu-north-1 nodeGroups : - name : ng-1 kubeletExtraConfig : clusterDNS : [ \"169.254.20.10\" , \"172.20.0.10\" ]","title":"Custom Cluster DNS address"},{"location":"usage/vpc-networking/#nat-gateway","text":"The NAT Gateway for a cluster can be configured to be Disabled , Single (default) or HighlyAvailable . It can be specified through the --vpc-nat-mode CLI flag or in the cluster config file like the example below: 1 2 3 vpc : nat : gateway : HighlyAvailable # other options : Disable, Single (default) See the complete example here . Note : Specifying the NAT Gateway is only supported during cluster creation and it is not touched during a cluster upgrade. There are plans to support changing between different modes on cluster update in the future.","title":"NAT Gateway"},{"location":"usage/vpc-networking/#managing-access-to-kubernetes-api-server-endpoints","text":"The default creation of an EKS cluster exposes the Kubernetes API server publicly but not directly from within the VPC subnets (public=true, private=false). Traffic destined for the API server from within the VPC must first exit the VPC networks (but not Amazon's network) and then re-enter to reach the API server. The Kubernetes API server endpoint access for a cluster can be configured for public and private access when creating the cluster using the cluster config file. Example below: 1 2 3 4 vpc : clusterEndpoints : publicAccess : <true|false> privateAccess : <true|false> There are some additional caveats when configuring Kubernetes API endpoint access: EKS doesn't allow one to create or update a cluster without at least one of private or public access being enabled. EKS does allow creating a configuration which allows only private access to be enabled, but eksctl doesn't support it during cluster creation as it prevents eksctl from being able to join the worker nodes to the cluster. To create private-only Kubernetes API endpoint access, one must first create the cluster with public Kubernetes API endpoint access, and then use eksctl utils update-cluster-endpoints to change it after the cluster is finished creating. Updating a cluster to have private only Kubernetes API endpoint access means that Kubernetes commands (e.g. kubectl ) as well as eksctl delete cluster , eksctl utils write-kubeconfig , and possibly the command eksctl utils update-kube-proxy must be run within the cluster VPC. This requires some changes to various AWS resources. See: EKS user guide The following is an example of how one could configure the Kubernetes API endpoint access using the utils sub-command: 1 eksctl utils update - cluster - endpoints --name=<clustername> --private-access=true --public-access=false Note that if you don't pass a flag in it will keep the current value. Once you are satisfied with the proposed changes, add the approve flag to make the change to the running cluster.","title":"Managing Access to Kubernetes API Server Endpoints"},{"location":"usage/vpc-networking/#restricting-access-to-eks-kubernetes-public-api-endpoint","text":"The default creation of an EKS cluster exposes the Kubernetes API server publicly. To restrict access to the public API endpoint to a set of CIDRs when creating a cluster, set the publicAccessCIDRs field: 1 2 vpc : publicAccessCIDRs : [ \"1.1.1.1/32\" , \"2.2.2.0/24\" ] To update the restrictions on an existing cluster, use: 1 eksctl utils set-public-access-cidrs --cluster=<cluster> 1.1.1.1/32,2.2.2.0/24 To update the restrictions using a ClusterConfig file, set the new CIDRs in vpc.publicAccessCIDRs and run: 1 eksctl utils set-public-access-cidrs -f config.yaml Note : This feature only applies to the public endpoint. The API server endpoint access configuration options won't change, and you will still have the option to disable the public endpoint so your cluster is not accessible from the Internet (source: AWS containers roadmap comment and implementation notes in comment ).","title":"Restricting Access to EKS Kubernetes Public API endpoint"},{"location":"usage/windows-worker-nodes/","text":"Windows Worker Nodes \u00b6 Amazon EKS 1.14 supports Windows Nodes that allow running Windows containers. In addition to having Windows nodes, a Linux node in the cluster is required to run the VPC resource controller and CoreDNS, as Microsoft doesn't support host-networking mode yet. Thus, a Windows EKS cluster will be a mixed-mode cluster containing Windows nodes and at least one Linux node. The Linux nodes are critical to the functioning of the cluster, and thus, for a production-grade cluster, it's recommended to have at least two t2.large Linux nodes for HA. eksctl provides a flag to install the VPC resource controller as part of cluster creation, and a command to install it after a cluster has been created. Creating a new Windows cluster \u00b6 The config file syntax allows creating a fully-functioning Windows cluster in a single command: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # cluster.yaml # An example of ClusterConfig containing Windows and Linux node groups to support Windows workloads --- apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : windows-cluster region : us-west-2 nodeGroups : - name : windows-ng amiFamily : WindowsServer2019FullContainer minSize : 2 maxSize : 3 - name : linux-ng instanceType : t2.large minSize : 2 maxSize : 3 1 eksctl create cluster -f cluster.yaml --install-vpc-controllers To create a new cluster without using a config file, issue the following commands: 1 2 3 eksctl create cluster --name=windows-cluster --node-ami-family=WindowsServer2019CoreContainer eksctl create nodegroup --cluster=windows-cluster --node-ami-family=AmazonLinux2 --nodes-min=2 --node-type=t2.large eksctl utils install-vpc-controllers --name=windows-cluster --approve Adding Windows support to an existing Linux cluster \u00b6 To enable running Windows workloads on an existing cluster with Linux nodes ( AmazonLinux2 AMI family), you need to add a Windows node group and install the Windows VPC controller: 1 2 eksctl create nodegroup --cluster=existing-cluster --node-ami-family=WindowsServer2019CoreContainer eksctl utils install-vpc-controllers --name=windows-cluster --approve To ensure workloads are scheduled on the right OS, they must have a nodeSelector targeting the OS it must run on: 1 2 3 4 5 6 7 8 9 # Targeting Windows nodeSelector : beta.kubernetes.io/os : windows beta.kubernetes.io/arch : amd64 # Targeting Linux nodeSelector : beta.kubernetes.io/os : linux beta.kubernetes.io/arch : amd64 Further information \u00b6 EKS Windows Support","title":"Windows worker nodes"},{"location":"usage/windows-worker-nodes/#windows-worker-nodes","text":"Amazon EKS 1.14 supports Windows Nodes that allow running Windows containers. In addition to having Windows nodes, a Linux node in the cluster is required to run the VPC resource controller and CoreDNS, as Microsoft doesn't support host-networking mode yet. Thus, a Windows EKS cluster will be a mixed-mode cluster containing Windows nodes and at least one Linux node. The Linux nodes are critical to the functioning of the cluster, and thus, for a production-grade cluster, it's recommended to have at least two t2.large Linux nodes for HA. eksctl provides a flag to install the VPC resource controller as part of cluster creation, and a command to install it after a cluster has been created.","title":"Windows Worker Nodes"},{"location":"usage/windows-worker-nodes/#creating-a-new-windows-cluster","text":"The config file syntax allows creating a fully-functioning Windows cluster in a single command: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # cluster.yaml # An example of ClusterConfig containing Windows and Linux node groups to support Windows workloads --- apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : windows-cluster region : us-west-2 nodeGroups : - name : windows-ng amiFamily : WindowsServer2019FullContainer minSize : 2 maxSize : 3 - name : linux-ng instanceType : t2.large minSize : 2 maxSize : 3 1 eksctl create cluster -f cluster.yaml --install-vpc-controllers To create a new cluster without using a config file, issue the following commands: 1 2 3 eksctl create cluster --name=windows-cluster --node-ami-family=WindowsServer2019CoreContainer eksctl create nodegroup --cluster=windows-cluster --node-ami-family=AmazonLinux2 --nodes-min=2 --node-type=t2.large eksctl utils install-vpc-controllers --name=windows-cluster --approve","title":"Creating a new Windows cluster"},{"location":"usage/windows-worker-nodes/#adding-windows-support-to-an-existing-linux-cluster","text":"To enable running Windows workloads on an existing cluster with Linux nodes ( AmazonLinux2 AMI family), you need to add a Windows node group and install the Windows VPC controller: 1 2 eksctl create nodegroup --cluster=existing-cluster --node-ami-family=WindowsServer2019CoreContainer eksctl utils install-vpc-controllers --name=windows-cluster --approve To ensure workloads are scheduled on the right OS, they must have a nodeSelector targeting the OS it must run on: 1 2 3 4 5 6 7 8 9 # Targeting Windows nodeSelector : beta.kubernetes.io/os : windows beta.kubernetes.io/arch : amd64 # Targeting Linux nodeSelector : beta.kubernetes.io/os : linux beta.kubernetes.io/arch : amd64","title":"Adding Windows support to an existing Linux cluster"},{"location":"usage/windows-worker-nodes/#further-information","text":"EKS Windows Support","title":"Further information"},{"location":"usage/experimental/gitops/","text":"gitops \u00b6 gitops is a way to do Kubernetes application delivery. It works by using Git as a single source of truth for Kubernetes resources. With Git at the center of your delivery pipelines, developers can make pull requests to accelerate and simplify application deployments and operations tasks to Kubernetes. eksctl provides an easy way to set up gitops in an existing cluster with the eksctl enable repo command. Installing Flux \u00b6 Warning This is an experimental feature. To enable it, set the environment variable EKSCTL_EXPERIMENTAL=true . Experimental features are not stable and their command name and flags may change. Installing Flux on the cluster is the first step towards a gitops workflow. To install it, you need a Git repository and an existing EKS cluster. Then run the following command: 1 EKSCTL_EXPERIMENTAL = true eksctl enable repo --cluster=<cluster_name> --region=<region> --git-url=<git_repo> --git-email=<git_user_email> Or use a config file: 1 EKSCTL_EXPERIMENTAL = true eksctl enable repo - f examples / 01 - simple - cluster . yaml --git-url=git@github.com:weaveworks/cluster-1-gitops.git --git-email=johndoe+flux@weave.works Note that, by default, eksctl enable repo installs Helm server components to the cluster (it installs Tiller and the Flux Helm Operator ). To disable the installation of the Helm server components, pass the flag --with-helm=false . Full example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 $ EKSCTL_EXPERIMENTAL = true . / eksctl enable repo -- cluster = cluster - 1 -- region = eu - west - 2 -- git - url = git @ github . com : weaveworks / cluster - 1 - gitops . git -- git - email = johndoe + flux @ weave . works -- namespace = flux [ \u2139 ] Generating public key infrastructure for the Helm Operator and Tiller [ \u2139 ] this may take up to a minute , please be patient [ ! ] Public key infrastructure files were written into directory \"/var/folders/zt/sh1tk7ts24sc6dybr5z9qtfh0000gn/T/eksctl-helm-pki330304977\" [ ! ] please move the files into a safe place or delete them [ \u2139 ] Generating manifests [ \u2139 ] Cloning git @ github . com : weaveworks / cluster - 1 - gitops . git Cloning into ' / var / folders / zt / sh1tk7ts24sc6dybr5z9qtfh0000gn / T / eksctl - install - flux - clone - 142184188 ' ... remote : Enumerating objects : 74 , done . remote : Counting objects : 100 % ( 74 / 74 ), done . remote : Compressing objects : 100 % ( 55 / 55 ), done . remote : Total 74 ( delta 19 ), reused 69 ( delta 17 ), pack - reused 0 Receiving objects : 100 % ( 74 / 74 ), 30.57 KiB | 381.00 KiB / s , done . Resolving deltas : 100 % ( 19 / 19 ), done . [ \u2139 ] Writing Flux manifests [ \u2139 ] Applying manifests [ \u2139 ] created \"Namespace/flux\" [ \u2139 ] created \"flux:Secret/flux-git-deploy\" [ \u2139 ] created \"flux:Deployment.apps/memcached\" [ \u2139 ] created \"flux:ServiceAccount/flux\" [ \u2139 ] created \"ClusterRole.rbac.authorization.k8s.io/flux\" [ \u2139 ] created \"ClusterRoleBinding.rbac.authorization.k8s.io/flux\" [ \u2139 ] created \"flux:ConfigMap/flux-helm-tls-ca-config\" [ \u2139 ] created \"flux:Deployment.extensions/tiller-deploy\" [ \u2139 ] created \"flux:Service/tiller-deploy\" [ \u2139 ] created \"CustomResourceDefinition.apiextensions.k8s.io/helmreleases.helm.fluxcd.io\" [ \u2139 ] created \"flux:ServiceAccount/tiller\" [ \u2139 ] created \"ClusterRoleBinding.rbac.authorization.k8s.io/tiller\" [ \u2139 ] created \"flux:ServiceAccount/helm\" [ \u2139 ] created \"flux:Role.rbac.authorization.k8s.io/tiller-user\" [ \u2139 ] created \"kube-system:RoleBinding.rbac.authorization.k8s.io/tiller-user-binding\" [ \u2139 ] created \"flux:Deployment.apps/flux\" [ \u2139 ] created \"flux:Service/memcached\" [ \u2139 ] created \"flux:Deployment.apps/flux-helm-operator\" [ \u2139 ] created \"flux:ServiceAccount/flux-helm-operator\" [ \u2139 ] created \"ClusterRole.rbac.authorization.k8s.io/flux-helm-operator\" [ \u2139 ] created \"ClusterRoleBinding.rbac.authorization.k8s.io/flux-helm-operator\" [ \u2139 ] Applying Helm TLS Secret ( s ) [ \u2139 ] created \"flux:Secret/flux-helm-tls-cert\" [ \u2139 ] created \"flux:Secret/tiller-secret\" [ ! ] Note : certificate secrets aren ' t added to the Git repository for security reasons [ \u2139 ] Waiting for Helm Operator to start ERROR : logging before flag . Parse : E0820 16 : 05 : 12.218007 98823 portforward . go : 331 ] an error occurred forwarding 60356 -> 3030 : error forwarding port 3030 to pod b1a872e7e6a7f86567488d66c1a880fcfa26179143115b102041e0ee77fe6f9e , uid : exit status 1 : 2019 / 08 / 20 14 : 05 : 12 socat [ 2873 ] E connect ( 5 , AF = 2 127.0.0.1 : 3030 , 16 ) : Connection refused [ ! ] Helm Operator is not ready yet ( Get http : //127.0.0.1:60356/healthz: EOF), retrying ... [ \u2139 ] Helm Operator started successfully [ \u2139 ] Waiting for Flux to start [ \u2139 ] Flux started successfully [ \u2139 ] Committing and pushing manifests to git @ github . com : weaveworks / cluster - 1 - gitops . git [ master ec43024 ] Add Initial Flux configuration Author : Flux < johndoe + flux @ weave . works > 14 files changed , 694 insertions ( + ) Enumerating objects : 11 , done . Counting objects : 100 % ( 11 / 11 ), done . Delta compression using up to 4 threads Compressing objects : 100 % ( 6 / 6 ), done . Writing objects : 100 % ( 6 / 6 ), 2.09 KiB | 2.09 MiB / s , done . Total 6 ( delta 3 ), reused 0 ( delta 0 ) remote : Resolving deltas : 100 % ( 3 / 3 ), completed with 3 local objects . To github . com : weaveworks / cluster - 1 - gitops . git 5f e1eb8 .. ec43024 master -> master [ \u2139 ] Flux will only operate properly once it has write - access to the Git repository [ \u2139 ] please configure git @ github . com : weaveworks / cluster - 1 - gitops . git so that the following Flux SSH public key has write access to it ssh - rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDYYsPuHzo1L29u3zhr4uAOF29HNyMcS8zJmOTDNZC4EiIwa5BXgg / IBDKudxQ + NBJ7mknPlNv17cqo4ncEq1xiQidfaUawwx3xxtDkZWam5nCBMXEJwkr4VXx / 6 QQ9Z1QGXpaFwdoVRcY / kM4NaxM54pEh5m43yeqkcpRMKraE0EgbdqFNNARN8rIEHY / giDorCrXp7e6AbzBgZSvc / in7Ul9FQhJ6K4 + 7 QuMFpJt3O / N8KDumoTG0e5ssJGp5L1ugIqhzqvbHdmHVfnXsEvq6cR1SJtYKi2GLCscypoF3XahfjK + xGV / 92 a1E7X + 6f HXSq + bdOKfBc4Z3f9NBwz0v At this point Flux and the Helm server components should be installed in the specified cluster. The only thing left to do is to give Flux write access to the repository. Configure your repository to allow write access to that ssh key, for example, through the Deploy keys if it lives in GitHub. 1 2 3 4 $ kubectl get pods --namespace flux NAME READY STATUS RESTARTS AGE flux-699cc7f4cb-9qc45 1 /1 Running 0 29m memcached-958f745c-qdfgz 1 /1 Running 0 29m Adding a workload \u00b6 To deploy a new workload on the cluster using gitops just add a kubernetes manifest to the repository. After a few minutes you should see the resources appearing in the cluster. Further reading \u00b6 To learn more about gitops and Flux, check the Flux documentation . Installing components from a Quick Start profile \u00b6 eksctl provides an application development Quick Star profile which can install the following components in your cluster: - Metrics Server - Prometheus - Grafana - Kubernetes Dashboard - FluentD with connection to CloudWatch logs - CNI, present by default in EKS clusters - Cluster Autoscaler - ALB ingress controller - Podinfo as a demo application To install those components the command generate profile can be used: 1 EKSCTL_EXPERIMENTAL = true eksctl generate profile --config-file=<cluster_config_file> --git-url git@github.com:weaveworks/eks-quickstart-app-dev.git --profile-path <output_directory> For example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 $ EKSCTL_EXPERIMENTAL = true eksctl generate profile --config-file 01 -simple-cluster.yaml --git-url git@github.com:weaveworks/eks-quickstart-app-dev.git --profile-path my-gitops-repo/base/ [ \u2139 ] cloning repository \"git@github.com:weaveworks/eks-quickstart-app-dev.git\" :master Cloning into '/tmp/quickstart-224631067' ... warning: templates not found /home/.../.git_template remote: Enumerating objects: 75 , done . remote: Counting objects: 100 % ( 75 /75 ) , done . remote: Compressing objects: 100 % ( 63 /63 ) , done . remote: Total 75 ( delta 25 ) , reused 49 ( delta 11 ) , pack-reused 0 Receiving objects: 100 % ( 75 /75 ) , 19 .02 KiB | 1 .19 MiB/s, done . Resolving deltas: 100 % ( 25 /25 ) , done . [ \u2139 ] processing template files in repository [ \u2139 ] writing new manifests to \"base/\" $ tree my-gitops-repo/base $ tree base/ base/ \u251c\u2500\u2500 amazon-cloudwatch \u2502 \u251c\u2500\u2500 cloudwatch-agent-configmap.yaml \u2502 \u251c\u2500\u2500 cloudwatch-agent-daemonset.yaml \u2502 \u251c\u2500\u2500 cloudwatch-agent-rbac.yaml \u2502 \u251c\u2500\u2500 fluentd-configmap-cluster-info.yaml \u2502 \u251c\u2500\u2500 fluentd-configmap-fluentd-config.yaml \u2502 \u251c\u2500\u2500 fluentd-daemonset.yaml \u2502 \u2514\u2500\u2500 fluentd-rbac.yaml \u251c\u2500\u2500 demo \u2502 \u2514\u2500\u2500 helm-release.yaml \u251c\u2500\u2500 kubernetes-dashboard \u2502 \u251c\u2500\u2500 dashboard-metrics-scraper-deployment.yaml \u2502 \u251c\u2500\u2500 dashboard-metrics-scraper-service.yaml \u2502 \u251c\u2500\u2500 kubernetes-dashboard-configmap.yaml \u2502 \u251c\u2500\u2500 kubernetes-dashboard-deployment.yaml \u2502 \u251c\u2500\u2500 kubernetes-dashboard-rbac.yaml \u2502 \u251c\u2500\u2500 kubernetes-dashboard-secrets.yaml \u2502 \u2514\u2500\u2500 kubernetes-dashboard-service.yaml \u251c\u2500\u2500 kube-system \u2502 \u251c\u2500\u2500 alb-ingress-controller-deployment.yaml \u2502 \u251c\u2500\u2500 alb-ingress-controller-rbac.yaml \u2502 \u251c\u2500\u2500 cluster-autoscaler-deployment.yaml \u2502 \u2514\u2500\u2500 cluster-autoscaler-rbac.yaml \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 monitoring \u2502 \u251c\u2500\u2500 metrics-server.yaml \u2502 \u2514\u2500\u2500 prometheus-operator.yaml \u251c\u2500\u2500 namespaces \u2502 \u251c\u2500\u2500 amazon-cloudwatch.yaml \u2502 \u251c\u2500\u2500 demo.yaml \u2502 \u251c\u2500\u2500 kubernetes-dashboard.yaml \u2502 \u2514\u2500\u2500 monitoring.yaml \u2514\u2500\u2500 README.md After running the command, add, commit and push the files: 1 2 3 4 cd my-gitops-repo/ git add . git commit -m \"Add application development quick start components\" git push origin master After a few minutes, Flux and Helm should have installed all the components in your cluster. Setting up gitops in a repo from a Quick Start \u00b6 Configuring gitops can be done easily with eksctl. The command eksctl enable profile takes an existing EKS cluster and an empty repository and sets them up with gitops and a specified Quick Start profile. This means that with one command the cluster will have all the components provided by the Quick Start profile installed in the cluster and you can enjoy the advantages of gitops moving forward. The basic command usage looks like this: 1 EKSCTL_EXPERIMENTAL = true eksctl enable profile --cluster <cluster-name> --region <region> --git-url=<url_to_your_repo> app-dev This command will clone the specified repository in your current working directory and then it will follow these steps: install Flux, Helm and Tiller in the cluster and add the manifests of those components into the flux/ folder in your repo add the component manifests of the Quick Start profile to your repository inside the base/ folder commit the Quick Start files and push the changes to the origin remote once you have given read and write access to your repository to the the SSH key printed by the command, Flux will install the components from the base/ folder into your cluster Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 $ EKSCTL_EXPERIMENTAL = true eksctl enable profile -- cluster production - cluster -- region eu - north - 1 -- git - url = git @ github . com : myorg / production - kubernetes app - dev [ \u2139 ] Generating public key infrastructure for the Helm Operator and Tiller [ \u2139 ] this may take up to a minute , please be patient [ ! ] Public key infrastructure files were written into directory \"/tmp/eksctl-helm-pki786744152\" [ ! ] please move the files into a safe place or delete them [ \u2139 ] Generating manifests [ \u2139 ] Cloning git @ github . com : myorg / production - kubernetes Cloning into ' / tmp / eksctl - install - flux - clone - 615092439 ' ... remote : Enumerating objects : 114 , done . remote : Counting objects : 100 % ( 114 / 114 ), done . remote : Compressing objects : 100 % ( 94 / 94 ), done . remote : Total 114 ( delta 36 ), reused 93 ( delta 17 ), pack - reused 0 Receiving objects : 100 % ( 114 / 114 ), 31.43 KiB | 4.49 MiB / s , done . Resolving deltas : 100 % ( 36 / 36 ), done . [ \u2139 ] Writing Flux manifests [ \u2139 ] Applying manifests [ \u2139 ] created \"Namespace/flux\" [ \u2139 ] created \"flux:Deployment.apps/flux-helm-operator\" [ \u2139 ] created \"flux:Deployment.apps/flux\" [ \u2139 ] created \"flux:Deployment.apps/memcached\" [ \u2139 ] created \"flux:ConfigMap/flux-helm-tls-ca-config\" [ \u2139 ] created \"flux:Deployment.extensions/tiller-deploy\" [ \u2139 ] created \"flux:Service/tiller-deploy\" [ \u2139 ] created \"CustomResourceDefinition.apiextensions.k8s.io/helmreleases.helm.fluxcd.io\" [ \u2139 ] created \"flux:Service/memcached\" [ \u2139 ] created \"flux:ServiceAccount/flux\" [ \u2139 ] created \"ClusterRole.rbac.authorization.k8s.io/flux\" [ \u2139 ] created \"ClusterRoleBinding.rbac.authorization.k8s.io/flux\" [ \u2139 ] created \"flux:Secret/flux-git-deploy\" [ \u2139 ] created \"flux:ServiceAccount/flux-helm-operator\" [ \u2139 ] created \"ClusterRole.rbac.authorization.k8s.io/flux-helm-operator\" [ \u2139 ] created \"ClusterRoleBinding.rbac.authorization.k8s.io/flux-helm-operator\" [ \u2139 ] created \"flux:ServiceAccount/tiller\" [ \u2139 ] created \"ClusterRoleBinding.rbac.authorization.k8s.io/tiller\" [ \u2139 ] created \"flux:ServiceAccount/helm\" [ \u2139 ] created \"flux:Role.rbac.authorization.k8s.io/tiller-user\" [ \u2139 ] created \"kube-system:RoleBinding.rbac.authorization.k8s.io/tiller-user-binding\" [ \u2139 ] Applying Helm TLS Secret ( s ) [ \u2139 ] created \"flux:Secret/flux-helm-tls-cert\" [ \u2139 ] created \"flux:Secret/tiller-secret\" [ ! ] Note : certificate secrets aren ' t added to the Git repository for security reasons [ \u2139 ] Waiting for Helm Operator to start ERROR : logging before flag . Parse : E0822 14 : 45 : 28.440236 17028 portforward . go : 331 ] an error occurred forwarding 44915 -> 3030 : error forwarding port 3030 to pod 2f 6282 bf597b345b3ffad8a0447bdd8515d91060335456591d759ad87a976ed2 , uid : exit status 1 : 2019 / 08 / 22 12 : 45 : 28 socat [ 8131 ] E connect ( 5 , AF = 2 127.0.0.1 : 3030 , 16 ) : Connection refused [ ! ] Helm Operator is not ready yet ( Get http : //127.0.0.1:44915/healthz: EOF), retrying ... [ ! ] Helm Operator is not ready yet ( Get http : //127.0.0.1:44915/healthz: EOF), retrying ... [ \u2139 ] Helm Operator started successfully [ \u2139 ] see https : //docs.fluxcd.io/projects/helm-operator for details on how to use the Helm Operator [ \u2139 ] Waiting for Flux to start [ \u2139 ] Flux started successfully [ \u2139 ] see https : //docs.fluxcd.io/projects/flux for details on how to use Flux [ \u2139 ] Committing and pushing manifests to git @ github . com : myorg / production - kubernetes [ master 0985830 ] Add Initial Flux configuration Author : Flux <> 13 files changed , 727 insertions ( + ) create mode 100644 flux / flux - account . yaml create mode 100644 flux / flux - deployment . yaml create mode 100644 flux / flux - helm - operator - account . yaml create mode 100644 flux / flux - helm - release - crd . yaml create mode 100644 flux / flux - namespace . yaml create mode 100644 flux / flux - secret . yaml create mode 100644 flux / helm - operator - deployment . yaml create mode 100644 flux / memcache - dep . yaml create mode 100644 flux / memcache - svc . yaml create mode 100644 flux / tiller - ca - cert - configmap . yaml create mode 100644 flux / tiller - dep . yaml create mode 100644 flux / tiller - rbac . yaml create mode 100644 flux / tiller - svc . yaml Counting objects : 16 , done . Delta compression using up to 4 threads . Compressing objects : 100 % ( 15 / 15 ), done . Writing objects : 100 % ( 16 / 16 ), 8.23 KiB | 8.23 MiB / s , done . Total 16 ( delta 1 ), reused 12 ( delta 1 ) remote : Resolving deltas : 100 % ( 1 / 1 ), done . To github . com : myorg / production - kubernetes 3 ea1fdc . .0985830 master -> master [ \u2139 ] Flux will only operate properly once it has write - access to the Git repository [ \u2139 ] please configure git @ github . com : myorg / production - kubernetes so that the following Flux SSH public key has write access to it ssh - rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDAdDG4LAEiEOTbT3XVL5sYf0Hy7T30PG2sFReIwrylR7syA + IU9GPf7azgjjbzbQc / 5 BXTx2E0GotrzDkvCNScuYfw7wXKK87yr5jhPudpNubK9bFsKKwOj7wxO2XsUOceKVRhTKP7VJgpAliCCPK288HvQzIZfWEgbDQjhE0EnFgZVYXKkgye2Cc3MkwiYuZJtuynxipb5rPrY / 3 Kjywk / vWxLeZ / hvv58mZSdRQwX6zbGGW1h70QA47B + W2076MBQQ1t0H0KKctuS8A1 / n + aKjpD4Ne6lXqHDhqi25SBhJxK3zEXhskS9DMW8DYi1xHT2MCjE8HhiVBMRIITyTox Cloning into ' / tmp / gitops - repos / flux - test - 3 ' ... remote : Enumerating objects : 118 , done . remote : Counting objects : 100 % ( 118 / 118 ), done . remote : Compressing objects : 100 % ( 98 / 98 ), done . remote : Total 118 ( delta 37 ), reused 96 ( delta 17 ), pack - reused 0 Receiving objects : 100 % ( 118 / 118 ), 33.15 KiB | 1.44 MiB / s , done . Resolving deltas : 100 % ( 37 / 37 ), done . [ \u2139 ] cloning repository \"git@github.com:weaveworks/eks-quickstart-app-dev.git\" : master Cloning into ' / tmp / quickstart - 365477450 ' ... remote : Enumerating objects : 127 , done . remote : Counting objects : 100 % ( 127 / 127 ), done . remote : Compressing objects : 100 % ( 95 / 95 ), done . remote : Total 127 ( delta 53 ), reused 92 ( delta 30 ), pack - reused 0 Receiving objects : 100 % ( 127 / 127 ), 30.20 KiB | 351.00 KiB / s , done . Resolving deltas : 100 % ( 53 / 53 ), done . [ \u2139 ] processing template files in repository [ \u2139 ] writing new manifests to \"/tmp/gitops-repos/flux-test-3/base\" [ master d0810f7 ] Add app - dev quickstart components Author : Flux <> 27 files changed , 1207 insertions ( + ) create mode 100644 base / LICENSE create mode 100644 base / README . md create mode 100644 base / amazon - cloudwatch / cloudwatch - agent - configmap . yaml create mode 100644 base / amazon - cloudwatch / cloudwatch - agent - daemonset . yaml create mode 100644 base / amazon - cloudwatch / cloudwatch - agent - rbac . yaml create mode 100644 base / amazon - cloudwatch / fluentd - configmap - cluster - info . yaml create mode 100644 base / amazon - cloudwatch / fluentd - configmap - fluentd - config . yaml create mode 100644 base / amazon - cloudwatch / fluentd - daemonset . yaml create mode 100644 base / amazon - cloudwatch / fluentd - rbac . yaml create mode 100644 base / demo / helm - release . yaml create mode 100644 base / kube - system / alb - ingress - controller - deployment . yaml create mode 100644 base / kube - system / alb - ingress - controller - rbac . yaml create mode 100644 base / kube - system / cluster - autoscaler - deployment . yaml create mode 100644 base / kube - system / cluster - autoscaler - rbac . yaml create mode 100644 base / kubernetes - dashboard / dashboard - metrics - scraper - deployment . yaml create mode 100644 base / kubernetes - dashboard / dashboard - metrics - scraper - service . yaml create mode 100644 base / kubernetes - dashboard / kubernetes - dashboard - configmap . yaml create mode 100644 base / kubernetes - dashboard / kubernetes - dashboard - deployment . yaml create mode 100644 base / kubernetes - dashboard / kubernetes - dashboard - rbac . yaml create mode 100644 base / kubernetes - dashboard / kubernetes - dashboard - secrets . yaml create mode 100644 base / kubernetes - dashboard / kubernetes - dashboard - service . yaml create mode 100644 base / monitoring / metrics - server . yaml create mode 100644 base / monitoring / prometheus - operator . yaml create mode 100644 base / namespaces / amazon - cloudwatch . yaml create mode 100644 base / namespaces / demo . yaml create mode 100644 base / namespaces / kubernetes - dashboard . yaml create mode 100644 base / namespaces / monitoring . yaml Counting objects : 36 , done . Delta compression using up to 4 threads . Compressing objects : 100 % ( 27 / 27 ), done . Writing objects : 100 % ( 36 / 36 ), 11.17 KiB | 3.72 MiB / s , done . Total 36 ( delta 8 ), reused 27 ( delta 8 ) remote : Resolving deltas : 100 % ( 8 / 8 ), done . To github . com : myorg / production - kubernetes 0985830. . d0810f7 master -> master Now the ssh key printed above: 1 ssh - rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDAdDG4LAEiEOTbT3XVL5sYf0Hy7T30PG2sFReIwrylR7syA + IU9GPf7azgjjbzbQc / 5 BXTx2E0GotrzDkvCNScuYfw7wXKK87yr5jhPudpNubK9bFsKKwOj7wxO2XsUOceKVRhTKP7VJgpAliCCPK288HvQzIZfWEgbDQjhE0EnFgZVYXKkgye2Cc3MkwiYuZJtuynxipb5rPrY / 3 Kjywk / vWxLeZ / hvv58mZSdRQwX6zbGGW1h70QA47B + W2076MBQQ1t0H0KKctuS8A1 / n + aKjpD4Ne6lXqHDhqi25SBhJxK3zEXhskS9DMW8DYi1xHT2MCjE8HhiVBMRIITyTox needs to be added as a deploy key to the chosen Github repository, in this case github.com:myorg/production-kubernetes . Once that is done, Flux will pick up the changes in the repository with the Quick Start components and deploy them to the cluster. After a couple of minutes the pods should appear in the cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE amazon-cloudwatch cloudwatch-agent-qtdmc 1 /1 Running 0 4m28s amazon-cloudwatch fluentd-cloudwatch-4rwwr 1 /1 Running 0 4m28s demo podinfo-75b8547f78-56dll 1 /1 Running 0 103s flux flux-56b5664cdd-nfzx2 1 /1 Running 0 11m flux flux-helm-operator-6bc7c85bb5-l2nzn 1 /1 Running 0 11m flux memcached-958f745c-dqllc 1 /1 Running 0 11m flux tiller-deploy-7ccc4b4d45-w2mrt 1 /1 Running 0 11m kube-system alb-ingress-controller-6b64bcbbd8-6l7kf 1 /1 Running 0 4m28s kube-system aws-node-l49ct 1 /1 Running 0 14m kube-system cluster-autoscaler-5b8c96cd98-26z5f 1 /1 Running 0 4m28s kube-system coredns-7d7755744b-4jkp6 1 /1 Running 0 21m kube-system coredns-7d7755744b-ls5d9 1 /1 Running 0 21m kube-system kube-proxy-wllff 1 /1 Running 0 14m kubernetes-dashboard dashboard-metrics-scraper-f7b5dbf7d-rm5z7 1 /1 Running 0 4m28s kubernetes-dashboard kubernetes-dashboard-7447f48f55-94rhg 1 /1 Running 0 4m28s monitoring alertmanager-prometheus-operator-alertmanager-0 2 /2 Running 0 78s monitoring metrics-server-7dfc675884-q9qps 1 /1 Running 0 4m24s monitoring prometheus-operator-grafana-9bb769cf-pjk4r 2 /2 Running 0 89s monitoring prometheus-operator-kube-state-metrics-79f476bff6-r9m2s 1 /1 Running 0 89s monitoring prometheus-operator-operator-58fcb66576-6dwpg 1 /1 Running 0 89s monitoring prometheus-operator-prometheus-node-exporter-tllwl 1 /1 Running 0 89s monitoring prometheus-prometheus-operator-prometheus-0 3 /3 Running 1 72s All CLI arguments: Flag Default value Type Required Use --cluster string required name of the EKS cluster to add the nodegroup to --name string required name or URL of the Quick Start profile. For example, app-dev string required same as --name --git-url string required URL --git-branch master string optional Git branch --output-path ./ string optional Path --git-user Flux string optional Username --git-email string optional Email --git-private-ssh-key-path string optional Optional path to the private SSH key to use with Git Creating your own Quick Start profile \u00b6 A Quick Start profile is a Git repository that contains Kubernetes manifests that can be installed in a cluster using gitops (through Flux ). These manifests, will probably need some information about the cluster they will be installed in, such as the cluster name or the AWS region. That's why they are templated using Go templates . Please bear in mind that this is an experimental feature and therefore the chosen templating technology can be changed before the feature becomes stable. The variables that can be templated are shown below: Name Template cluster name {{ .ClusterName }} cluster region {{ .Region }} For example, we could create a config map using these variables: 1 2 3 4 5 6 7 8 apiVersion : v1 data : cluster.name : {{ .ClusterName }} logs.region : {{ .Region }} kind : ConfigMap metadata : name : cluster-info namespace : my-namespace Write this into a file with the extension *.yaml.tmpl and commit it to your Quick Start repository. Files with this extension get processed by eksctl before committing them to the user's gitops repository, while the rest get copied unmodified. Regarding the folder structure inside the Quick Start repository, we recommend using a folder for each namespace and one file per Kubernetes resource. 1 2 3 4 5 6 7 8 9 10 11 repository - name / \u251c\u2500\u2500 kube - system \u2502 \u251c\u2500\u2500 ingress - controller - deployment . yaml . tmpl \u2502 \u2514\u2500\u2500 ingress - controller - rbac . yaml . tmpl \u2514\u2500\u2500 alerting \u251c\u2500\u2500 alerting - app - deployment . yaml \u251c\u2500\u2500 alerting - app - service . yaml . tmpl \u251c\u2500\u2500 monitoring - sidecar - deployment . yaml \u251c\u2500\u2500 monitoring - sidecar - service . yaml . tmpl \u251c\u2500\u2500 cluster - info - configmap . yaml . tmpl \u2514\u2500\u2500 alerting - namespace . yaml Note that some files have the extension *.yaml while others have *.yaml.tmpl . The last ones are the ones that can contain template actions while the former are plain yaml files. These files can now be committed and pushed to your Quick Start repository, for example git@github.com:my-org/production-infra . 1 2 3 4 cd repository - name / git add . git commit - m \"Add component templates\" git push origin master Now that the templates are in the remote repository, the Quick Start is ready to be used with eksctl enable profile : 1 EKSCTL_EXPERIMENTAL = true eksctl enable profile --cluster team1 --region eu-west-1 --git-url git@github.com:my-org/team1-cluster --git-email alice@my-org.com git@github.com:my-org/production-infra In this example we provide github.com:my-org/production-infra as the Quick Start profile and github.com:my-org/team1-cluster as the gitops repository that is connected to the Flux instance in the cluster named cluster1 . For a full example of a Quick Start profile, check out App Dev .","title":"Experimental"},{"location":"usage/experimental/gitops/#gitops","text":"gitops is a way to do Kubernetes application delivery. It works by using Git as a single source of truth for Kubernetes resources. With Git at the center of your delivery pipelines, developers can make pull requests to accelerate and simplify application deployments and operations tasks to Kubernetes. eksctl provides an easy way to set up gitops in an existing cluster with the eksctl enable repo command.","title":"gitops"},{"location":"usage/experimental/gitops/#installing-flux","text":"Warning This is an experimental feature. To enable it, set the environment variable EKSCTL_EXPERIMENTAL=true . Experimental features are not stable and their command name and flags may change. Installing Flux on the cluster is the first step towards a gitops workflow. To install it, you need a Git repository and an existing EKS cluster. Then run the following command: 1 EKSCTL_EXPERIMENTAL = true eksctl enable repo --cluster=<cluster_name> --region=<region> --git-url=<git_repo> --git-email=<git_user_email> Or use a config file: 1 EKSCTL_EXPERIMENTAL = true eksctl enable repo - f examples / 01 - simple - cluster . yaml --git-url=git@github.com:weaveworks/cluster-1-gitops.git --git-email=johndoe+flux@weave.works Note that, by default, eksctl enable repo installs Helm server components to the cluster (it installs Tiller and the Flux Helm Operator ). To disable the installation of the Helm server components, pass the flag --with-helm=false . Full example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 $ EKSCTL_EXPERIMENTAL = true . / eksctl enable repo -- cluster = cluster - 1 -- region = eu - west - 2 -- git - url = git @ github . com : weaveworks / cluster - 1 - gitops . git -- git - email = johndoe + flux @ weave . works -- namespace = flux [ \u2139 ] Generating public key infrastructure for the Helm Operator and Tiller [ \u2139 ] this may take up to a minute , please be patient [ ! ] Public key infrastructure files were written into directory \"/var/folders/zt/sh1tk7ts24sc6dybr5z9qtfh0000gn/T/eksctl-helm-pki330304977\" [ ! ] please move the files into a safe place or delete them [ \u2139 ] Generating manifests [ \u2139 ] Cloning git @ github . com : weaveworks / cluster - 1 - gitops . git Cloning into ' / var / folders / zt / sh1tk7ts24sc6dybr5z9qtfh0000gn / T / eksctl - install - flux - clone - 142184188 ' ... remote : Enumerating objects : 74 , done . remote : Counting objects : 100 % ( 74 / 74 ), done . remote : Compressing objects : 100 % ( 55 / 55 ), done . remote : Total 74 ( delta 19 ), reused 69 ( delta 17 ), pack - reused 0 Receiving objects : 100 % ( 74 / 74 ), 30.57 KiB | 381.00 KiB / s , done . Resolving deltas : 100 % ( 19 / 19 ), done . [ \u2139 ] Writing Flux manifests [ \u2139 ] Applying manifests [ \u2139 ] created \"Namespace/flux\" [ \u2139 ] created \"flux:Secret/flux-git-deploy\" [ \u2139 ] created \"flux:Deployment.apps/memcached\" [ \u2139 ] created \"flux:ServiceAccount/flux\" [ \u2139 ] created \"ClusterRole.rbac.authorization.k8s.io/flux\" [ \u2139 ] created \"ClusterRoleBinding.rbac.authorization.k8s.io/flux\" [ \u2139 ] created \"flux:ConfigMap/flux-helm-tls-ca-config\" [ \u2139 ] created \"flux:Deployment.extensions/tiller-deploy\" [ \u2139 ] created \"flux:Service/tiller-deploy\" [ \u2139 ] created \"CustomResourceDefinition.apiextensions.k8s.io/helmreleases.helm.fluxcd.io\" [ \u2139 ] created \"flux:ServiceAccount/tiller\" [ \u2139 ] created \"ClusterRoleBinding.rbac.authorization.k8s.io/tiller\" [ \u2139 ] created \"flux:ServiceAccount/helm\" [ \u2139 ] created \"flux:Role.rbac.authorization.k8s.io/tiller-user\" [ \u2139 ] created \"kube-system:RoleBinding.rbac.authorization.k8s.io/tiller-user-binding\" [ \u2139 ] created \"flux:Deployment.apps/flux\" [ \u2139 ] created \"flux:Service/memcached\" [ \u2139 ] created \"flux:Deployment.apps/flux-helm-operator\" [ \u2139 ] created \"flux:ServiceAccount/flux-helm-operator\" [ \u2139 ] created \"ClusterRole.rbac.authorization.k8s.io/flux-helm-operator\" [ \u2139 ] created \"ClusterRoleBinding.rbac.authorization.k8s.io/flux-helm-operator\" [ \u2139 ] Applying Helm TLS Secret ( s ) [ \u2139 ] created \"flux:Secret/flux-helm-tls-cert\" [ \u2139 ] created \"flux:Secret/tiller-secret\" [ ! ] Note : certificate secrets aren ' t added to the Git repository for security reasons [ \u2139 ] Waiting for Helm Operator to start ERROR : logging before flag . Parse : E0820 16 : 05 : 12.218007 98823 portforward . go : 331 ] an error occurred forwarding 60356 -> 3030 : error forwarding port 3030 to pod b1a872e7e6a7f86567488d66c1a880fcfa26179143115b102041e0ee77fe6f9e , uid : exit status 1 : 2019 / 08 / 20 14 : 05 : 12 socat [ 2873 ] E connect ( 5 , AF = 2 127.0.0.1 : 3030 , 16 ) : Connection refused [ ! ] Helm Operator is not ready yet ( Get http : //127.0.0.1:60356/healthz: EOF), retrying ... [ \u2139 ] Helm Operator started successfully [ \u2139 ] Waiting for Flux to start [ \u2139 ] Flux started successfully [ \u2139 ] Committing and pushing manifests to git @ github . com : weaveworks / cluster - 1 - gitops . git [ master ec43024 ] Add Initial Flux configuration Author : Flux < johndoe + flux @ weave . works > 14 files changed , 694 insertions ( + ) Enumerating objects : 11 , done . Counting objects : 100 % ( 11 / 11 ), done . Delta compression using up to 4 threads Compressing objects : 100 % ( 6 / 6 ), done . Writing objects : 100 % ( 6 / 6 ), 2.09 KiB | 2.09 MiB / s , done . Total 6 ( delta 3 ), reused 0 ( delta 0 ) remote : Resolving deltas : 100 % ( 3 / 3 ), completed with 3 local objects . To github . com : weaveworks / cluster - 1 - gitops . git 5f e1eb8 .. ec43024 master -> master [ \u2139 ] Flux will only operate properly once it has write - access to the Git repository [ \u2139 ] please configure git @ github . com : weaveworks / cluster - 1 - gitops . git so that the following Flux SSH public key has write access to it ssh - rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDYYsPuHzo1L29u3zhr4uAOF29HNyMcS8zJmOTDNZC4EiIwa5BXgg / IBDKudxQ + NBJ7mknPlNv17cqo4ncEq1xiQidfaUawwx3xxtDkZWam5nCBMXEJwkr4VXx / 6 QQ9Z1QGXpaFwdoVRcY / kM4NaxM54pEh5m43yeqkcpRMKraE0EgbdqFNNARN8rIEHY / giDorCrXp7e6AbzBgZSvc / in7Ul9FQhJ6K4 + 7 QuMFpJt3O / N8KDumoTG0e5ssJGp5L1ugIqhzqvbHdmHVfnXsEvq6cR1SJtYKi2GLCscypoF3XahfjK + xGV / 92 a1E7X + 6f HXSq + bdOKfBc4Z3f9NBwz0v At this point Flux and the Helm server components should be installed in the specified cluster. The only thing left to do is to give Flux write access to the repository. Configure your repository to allow write access to that ssh key, for example, through the Deploy keys if it lives in GitHub. 1 2 3 4 $ kubectl get pods --namespace flux NAME READY STATUS RESTARTS AGE flux-699cc7f4cb-9qc45 1 /1 Running 0 29m memcached-958f745c-qdfgz 1 /1 Running 0 29m","title":"Installing Flux"},{"location":"usage/experimental/gitops/#adding-a-workload","text":"To deploy a new workload on the cluster using gitops just add a kubernetes manifest to the repository. After a few minutes you should see the resources appearing in the cluster.","title":"Adding a workload"},{"location":"usage/experimental/gitops/#further-reading","text":"To learn more about gitops and Flux, check the Flux documentation .","title":"Further reading"},{"location":"usage/experimental/gitops/#installing-components-from-a-quick-start-profile","text":"eksctl provides an application development Quick Star profile which can install the following components in your cluster: - Metrics Server - Prometheus - Grafana - Kubernetes Dashboard - FluentD with connection to CloudWatch logs - CNI, present by default in EKS clusters - Cluster Autoscaler - ALB ingress controller - Podinfo as a demo application To install those components the command generate profile can be used: 1 EKSCTL_EXPERIMENTAL = true eksctl generate profile --config-file=<cluster_config_file> --git-url git@github.com:weaveworks/eks-quickstart-app-dev.git --profile-path <output_directory> For example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 $ EKSCTL_EXPERIMENTAL = true eksctl generate profile --config-file 01 -simple-cluster.yaml --git-url git@github.com:weaveworks/eks-quickstart-app-dev.git --profile-path my-gitops-repo/base/ [ \u2139 ] cloning repository \"git@github.com:weaveworks/eks-quickstart-app-dev.git\" :master Cloning into '/tmp/quickstart-224631067' ... warning: templates not found /home/.../.git_template remote: Enumerating objects: 75 , done . remote: Counting objects: 100 % ( 75 /75 ) , done . remote: Compressing objects: 100 % ( 63 /63 ) , done . remote: Total 75 ( delta 25 ) , reused 49 ( delta 11 ) , pack-reused 0 Receiving objects: 100 % ( 75 /75 ) , 19 .02 KiB | 1 .19 MiB/s, done . Resolving deltas: 100 % ( 25 /25 ) , done . [ \u2139 ] processing template files in repository [ \u2139 ] writing new manifests to \"base/\" $ tree my-gitops-repo/base $ tree base/ base/ \u251c\u2500\u2500 amazon-cloudwatch \u2502 \u251c\u2500\u2500 cloudwatch-agent-configmap.yaml \u2502 \u251c\u2500\u2500 cloudwatch-agent-daemonset.yaml \u2502 \u251c\u2500\u2500 cloudwatch-agent-rbac.yaml \u2502 \u251c\u2500\u2500 fluentd-configmap-cluster-info.yaml \u2502 \u251c\u2500\u2500 fluentd-configmap-fluentd-config.yaml \u2502 \u251c\u2500\u2500 fluentd-daemonset.yaml \u2502 \u2514\u2500\u2500 fluentd-rbac.yaml \u251c\u2500\u2500 demo \u2502 \u2514\u2500\u2500 helm-release.yaml \u251c\u2500\u2500 kubernetes-dashboard \u2502 \u251c\u2500\u2500 dashboard-metrics-scraper-deployment.yaml \u2502 \u251c\u2500\u2500 dashboard-metrics-scraper-service.yaml \u2502 \u251c\u2500\u2500 kubernetes-dashboard-configmap.yaml \u2502 \u251c\u2500\u2500 kubernetes-dashboard-deployment.yaml \u2502 \u251c\u2500\u2500 kubernetes-dashboard-rbac.yaml \u2502 \u251c\u2500\u2500 kubernetes-dashboard-secrets.yaml \u2502 \u2514\u2500\u2500 kubernetes-dashboard-service.yaml \u251c\u2500\u2500 kube-system \u2502 \u251c\u2500\u2500 alb-ingress-controller-deployment.yaml \u2502 \u251c\u2500\u2500 alb-ingress-controller-rbac.yaml \u2502 \u251c\u2500\u2500 cluster-autoscaler-deployment.yaml \u2502 \u2514\u2500\u2500 cluster-autoscaler-rbac.yaml \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 monitoring \u2502 \u251c\u2500\u2500 metrics-server.yaml \u2502 \u2514\u2500\u2500 prometheus-operator.yaml \u251c\u2500\u2500 namespaces \u2502 \u251c\u2500\u2500 amazon-cloudwatch.yaml \u2502 \u251c\u2500\u2500 demo.yaml \u2502 \u251c\u2500\u2500 kubernetes-dashboard.yaml \u2502 \u2514\u2500\u2500 monitoring.yaml \u2514\u2500\u2500 README.md After running the command, add, commit and push the files: 1 2 3 4 cd my-gitops-repo/ git add . git commit -m \"Add application development quick start components\" git push origin master After a few minutes, Flux and Helm should have installed all the components in your cluster.","title":"Installing components from a Quick Start profile"},{"location":"usage/experimental/gitops/#setting-up-gitops-in-a-repo-from-a-quick-start","text":"Configuring gitops can be done easily with eksctl. The command eksctl enable profile takes an existing EKS cluster and an empty repository and sets them up with gitops and a specified Quick Start profile. This means that with one command the cluster will have all the components provided by the Quick Start profile installed in the cluster and you can enjoy the advantages of gitops moving forward. The basic command usage looks like this: 1 EKSCTL_EXPERIMENTAL = true eksctl enable profile --cluster <cluster-name> --region <region> --git-url=<url_to_your_repo> app-dev This command will clone the specified repository in your current working directory and then it will follow these steps: install Flux, Helm and Tiller in the cluster and add the manifests of those components into the flux/ folder in your repo add the component manifests of the Quick Start profile to your repository inside the base/ folder commit the Quick Start files and push the changes to the origin remote once you have given read and write access to your repository to the the SSH key printed by the command, Flux will install the components from the base/ folder into your cluster Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 $ EKSCTL_EXPERIMENTAL = true eksctl enable profile -- cluster production - cluster -- region eu - north - 1 -- git - url = git @ github . com : myorg / production - kubernetes app - dev [ \u2139 ] Generating public key infrastructure for the Helm Operator and Tiller [ \u2139 ] this may take up to a minute , please be patient [ ! ] Public key infrastructure files were written into directory \"/tmp/eksctl-helm-pki786744152\" [ ! ] please move the files into a safe place or delete them [ \u2139 ] Generating manifests [ \u2139 ] Cloning git @ github . com : myorg / production - kubernetes Cloning into ' / tmp / eksctl - install - flux - clone - 615092439 ' ... remote : Enumerating objects : 114 , done . remote : Counting objects : 100 % ( 114 / 114 ), done . remote : Compressing objects : 100 % ( 94 / 94 ), done . remote : Total 114 ( delta 36 ), reused 93 ( delta 17 ), pack - reused 0 Receiving objects : 100 % ( 114 / 114 ), 31.43 KiB | 4.49 MiB / s , done . Resolving deltas : 100 % ( 36 / 36 ), done . [ \u2139 ] Writing Flux manifests [ \u2139 ] Applying manifests [ \u2139 ] created \"Namespace/flux\" [ \u2139 ] created \"flux:Deployment.apps/flux-helm-operator\" [ \u2139 ] created \"flux:Deployment.apps/flux\" [ \u2139 ] created \"flux:Deployment.apps/memcached\" [ \u2139 ] created \"flux:ConfigMap/flux-helm-tls-ca-config\" [ \u2139 ] created \"flux:Deployment.extensions/tiller-deploy\" [ \u2139 ] created \"flux:Service/tiller-deploy\" [ \u2139 ] created \"CustomResourceDefinition.apiextensions.k8s.io/helmreleases.helm.fluxcd.io\" [ \u2139 ] created \"flux:Service/memcached\" [ \u2139 ] created \"flux:ServiceAccount/flux\" [ \u2139 ] created \"ClusterRole.rbac.authorization.k8s.io/flux\" [ \u2139 ] created \"ClusterRoleBinding.rbac.authorization.k8s.io/flux\" [ \u2139 ] created \"flux:Secret/flux-git-deploy\" [ \u2139 ] created \"flux:ServiceAccount/flux-helm-operator\" [ \u2139 ] created \"ClusterRole.rbac.authorization.k8s.io/flux-helm-operator\" [ \u2139 ] created \"ClusterRoleBinding.rbac.authorization.k8s.io/flux-helm-operator\" [ \u2139 ] created \"flux:ServiceAccount/tiller\" [ \u2139 ] created \"ClusterRoleBinding.rbac.authorization.k8s.io/tiller\" [ \u2139 ] created \"flux:ServiceAccount/helm\" [ \u2139 ] created \"flux:Role.rbac.authorization.k8s.io/tiller-user\" [ \u2139 ] created \"kube-system:RoleBinding.rbac.authorization.k8s.io/tiller-user-binding\" [ \u2139 ] Applying Helm TLS Secret ( s ) [ \u2139 ] created \"flux:Secret/flux-helm-tls-cert\" [ \u2139 ] created \"flux:Secret/tiller-secret\" [ ! ] Note : certificate secrets aren ' t added to the Git repository for security reasons [ \u2139 ] Waiting for Helm Operator to start ERROR : logging before flag . Parse : E0822 14 : 45 : 28.440236 17028 portforward . go : 331 ] an error occurred forwarding 44915 -> 3030 : error forwarding port 3030 to pod 2f 6282 bf597b345b3ffad8a0447bdd8515d91060335456591d759ad87a976ed2 , uid : exit status 1 : 2019 / 08 / 22 12 : 45 : 28 socat [ 8131 ] E connect ( 5 , AF = 2 127.0.0.1 : 3030 , 16 ) : Connection refused [ ! ] Helm Operator is not ready yet ( Get http : //127.0.0.1:44915/healthz: EOF), retrying ... [ ! ] Helm Operator is not ready yet ( Get http : //127.0.0.1:44915/healthz: EOF), retrying ... [ \u2139 ] Helm Operator started successfully [ \u2139 ] see https : //docs.fluxcd.io/projects/helm-operator for details on how to use the Helm Operator [ \u2139 ] Waiting for Flux to start [ \u2139 ] Flux started successfully [ \u2139 ] see https : //docs.fluxcd.io/projects/flux for details on how to use Flux [ \u2139 ] Committing and pushing manifests to git @ github . com : myorg / production - kubernetes [ master 0985830 ] Add Initial Flux configuration Author : Flux <> 13 files changed , 727 insertions ( + ) create mode 100644 flux / flux - account . yaml create mode 100644 flux / flux - deployment . yaml create mode 100644 flux / flux - helm - operator - account . yaml create mode 100644 flux / flux - helm - release - crd . yaml create mode 100644 flux / flux - namespace . yaml create mode 100644 flux / flux - secret . yaml create mode 100644 flux / helm - operator - deployment . yaml create mode 100644 flux / memcache - dep . yaml create mode 100644 flux / memcache - svc . yaml create mode 100644 flux / tiller - ca - cert - configmap . yaml create mode 100644 flux / tiller - dep . yaml create mode 100644 flux / tiller - rbac . yaml create mode 100644 flux / tiller - svc . yaml Counting objects : 16 , done . Delta compression using up to 4 threads . Compressing objects : 100 % ( 15 / 15 ), done . Writing objects : 100 % ( 16 / 16 ), 8.23 KiB | 8.23 MiB / s , done . Total 16 ( delta 1 ), reused 12 ( delta 1 ) remote : Resolving deltas : 100 % ( 1 / 1 ), done . To github . com : myorg / production - kubernetes 3 ea1fdc . .0985830 master -> master [ \u2139 ] Flux will only operate properly once it has write - access to the Git repository [ \u2139 ] please configure git @ github . com : myorg / production - kubernetes so that the following Flux SSH public key has write access to it ssh - rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDAdDG4LAEiEOTbT3XVL5sYf0Hy7T30PG2sFReIwrylR7syA + IU9GPf7azgjjbzbQc / 5 BXTx2E0GotrzDkvCNScuYfw7wXKK87yr5jhPudpNubK9bFsKKwOj7wxO2XsUOceKVRhTKP7VJgpAliCCPK288HvQzIZfWEgbDQjhE0EnFgZVYXKkgye2Cc3MkwiYuZJtuynxipb5rPrY / 3 Kjywk / vWxLeZ / hvv58mZSdRQwX6zbGGW1h70QA47B + W2076MBQQ1t0H0KKctuS8A1 / n + aKjpD4Ne6lXqHDhqi25SBhJxK3zEXhskS9DMW8DYi1xHT2MCjE8HhiVBMRIITyTox Cloning into ' / tmp / gitops - repos / flux - test - 3 ' ... remote : Enumerating objects : 118 , done . remote : Counting objects : 100 % ( 118 / 118 ), done . remote : Compressing objects : 100 % ( 98 / 98 ), done . remote : Total 118 ( delta 37 ), reused 96 ( delta 17 ), pack - reused 0 Receiving objects : 100 % ( 118 / 118 ), 33.15 KiB | 1.44 MiB / s , done . Resolving deltas : 100 % ( 37 / 37 ), done . [ \u2139 ] cloning repository \"git@github.com:weaveworks/eks-quickstart-app-dev.git\" : master Cloning into ' / tmp / quickstart - 365477450 ' ... remote : Enumerating objects : 127 , done . remote : Counting objects : 100 % ( 127 / 127 ), done . remote : Compressing objects : 100 % ( 95 / 95 ), done . remote : Total 127 ( delta 53 ), reused 92 ( delta 30 ), pack - reused 0 Receiving objects : 100 % ( 127 / 127 ), 30.20 KiB | 351.00 KiB / s , done . Resolving deltas : 100 % ( 53 / 53 ), done . [ \u2139 ] processing template files in repository [ \u2139 ] writing new manifests to \"/tmp/gitops-repos/flux-test-3/base\" [ master d0810f7 ] Add app - dev quickstart components Author : Flux <> 27 files changed , 1207 insertions ( + ) create mode 100644 base / LICENSE create mode 100644 base / README . md create mode 100644 base / amazon - cloudwatch / cloudwatch - agent - configmap . yaml create mode 100644 base / amazon - cloudwatch / cloudwatch - agent - daemonset . yaml create mode 100644 base / amazon - cloudwatch / cloudwatch - agent - rbac . yaml create mode 100644 base / amazon - cloudwatch / fluentd - configmap - cluster - info . yaml create mode 100644 base / amazon - cloudwatch / fluentd - configmap - fluentd - config . yaml create mode 100644 base / amazon - cloudwatch / fluentd - daemonset . yaml create mode 100644 base / amazon - cloudwatch / fluentd - rbac . yaml create mode 100644 base / demo / helm - release . yaml create mode 100644 base / kube - system / alb - ingress - controller - deployment . yaml create mode 100644 base / kube - system / alb - ingress - controller - rbac . yaml create mode 100644 base / kube - system / cluster - autoscaler - deployment . yaml create mode 100644 base / kube - system / cluster - autoscaler - rbac . yaml create mode 100644 base / kubernetes - dashboard / dashboard - metrics - scraper - deployment . yaml create mode 100644 base / kubernetes - dashboard / dashboard - metrics - scraper - service . yaml create mode 100644 base / kubernetes - dashboard / kubernetes - dashboard - configmap . yaml create mode 100644 base / kubernetes - dashboard / kubernetes - dashboard - deployment . yaml create mode 100644 base / kubernetes - dashboard / kubernetes - dashboard - rbac . yaml create mode 100644 base / kubernetes - dashboard / kubernetes - dashboard - secrets . yaml create mode 100644 base / kubernetes - dashboard / kubernetes - dashboard - service . yaml create mode 100644 base / monitoring / metrics - server . yaml create mode 100644 base / monitoring / prometheus - operator . yaml create mode 100644 base / namespaces / amazon - cloudwatch . yaml create mode 100644 base / namespaces / demo . yaml create mode 100644 base / namespaces / kubernetes - dashboard . yaml create mode 100644 base / namespaces / monitoring . yaml Counting objects : 36 , done . Delta compression using up to 4 threads . Compressing objects : 100 % ( 27 / 27 ), done . Writing objects : 100 % ( 36 / 36 ), 11.17 KiB | 3.72 MiB / s , done . Total 36 ( delta 8 ), reused 27 ( delta 8 ) remote : Resolving deltas : 100 % ( 8 / 8 ), done . To github . com : myorg / production - kubernetes 0985830. . d0810f7 master -> master Now the ssh key printed above: 1 ssh - rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDAdDG4LAEiEOTbT3XVL5sYf0Hy7T30PG2sFReIwrylR7syA + IU9GPf7azgjjbzbQc / 5 BXTx2E0GotrzDkvCNScuYfw7wXKK87yr5jhPudpNubK9bFsKKwOj7wxO2XsUOceKVRhTKP7VJgpAliCCPK288HvQzIZfWEgbDQjhE0EnFgZVYXKkgye2Cc3MkwiYuZJtuynxipb5rPrY / 3 Kjywk / vWxLeZ / hvv58mZSdRQwX6zbGGW1h70QA47B + W2076MBQQ1t0H0KKctuS8A1 / n + aKjpD4Ne6lXqHDhqi25SBhJxK3zEXhskS9DMW8DYi1xHT2MCjE8HhiVBMRIITyTox needs to be added as a deploy key to the chosen Github repository, in this case github.com:myorg/production-kubernetes . Once that is done, Flux will pick up the changes in the repository with the Quick Start components and deploy them to the cluster. After a couple of minutes the pods should appear in the cluster: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE amazon-cloudwatch cloudwatch-agent-qtdmc 1 /1 Running 0 4m28s amazon-cloudwatch fluentd-cloudwatch-4rwwr 1 /1 Running 0 4m28s demo podinfo-75b8547f78-56dll 1 /1 Running 0 103s flux flux-56b5664cdd-nfzx2 1 /1 Running 0 11m flux flux-helm-operator-6bc7c85bb5-l2nzn 1 /1 Running 0 11m flux memcached-958f745c-dqllc 1 /1 Running 0 11m flux tiller-deploy-7ccc4b4d45-w2mrt 1 /1 Running 0 11m kube-system alb-ingress-controller-6b64bcbbd8-6l7kf 1 /1 Running 0 4m28s kube-system aws-node-l49ct 1 /1 Running 0 14m kube-system cluster-autoscaler-5b8c96cd98-26z5f 1 /1 Running 0 4m28s kube-system coredns-7d7755744b-4jkp6 1 /1 Running 0 21m kube-system coredns-7d7755744b-ls5d9 1 /1 Running 0 21m kube-system kube-proxy-wllff 1 /1 Running 0 14m kubernetes-dashboard dashboard-metrics-scraper-f7b5dbf7d-rm5z7 1 /1 Running 0 4m28s kubernetes-dashboard kubernetes-dashboard-7447f48f55-94rhg 1 /1 Running 0 4m28s monitoring alertmanager-prometheus-operator-alertmanager-0 2 /2 Running 0 78s monitoring metrics-server-7dfc675884-q9qps 1 /1 Running 0 4m24s monitoring prometheus-operator-grafana-9bb769cf-pjk4r 2 /2 Running 0 89s monitoring prometheus-operator-kube-state-metrics-79f476bff6-r9m2s 1 /1 Running 0 89s monitoring prometheus-operator-operator-58fcb66576-6dwpg 1 /1 Running 0 89s monitoring prometheus-operator-prometheus-node-exporter-tllwl 1 /1 Running 0 89s monitoring prometheus-prometheus-operator-prometheus-0 3 /3 Running 1 72s All CLI arguments: Flag Default value Type Required Use --cluster string required name of the EKS cluster to add the nodegroup to --name string required name or URL of the Quick Start profile. For example, app-dev string required same as --name --git-url string required URL --git-branch master string optional Git branch --output-path ./ string optional Path --git-user Flux string optional Username --git-email string optional Email --git-private-ssh-key-path string optional Optional path to the private SSH key to use with Git","title":"Setting up gitops in a repo from a Quick Start"},{"location":"usage/experimental/gitops/#creating-your-own-quick-start-profile","text":"A Quick Start profile is a Git repository that contains Kubernetes manifests that can be installed in a cluster using gitops (through Flux ). These manifests, will probably need some information about the cluster they will be installed in, such as the cluster name or the AWS region. That's why they are templated using Go templates . Please bear in mind that this is an experimental feature and therefore the chosen templating technology can be changed before the feature becomes stable. The variables that can be templated are shown below: Name Template cluster name {{ .ClusterName }} cluster region {{ .Region }} For example, we could create a config map using these variables: 1 2 3 4 5 6 7 8 apiVersion : v1 data : cluster.name : {{ .ClusterName }} logs.region : {{ .Region }} kind : ConfigMap metadata : name : cluster-info namespace : my-namespace Write this into a file with the extension *.yaml.tmpl and commit it to your Quick Start repository. Files with this extension get processed by eksctl before committing them to the user's gitops repository, while the rest get copied unmodified. Regarding the folder structure inside the Quick Start repository, we recommend using a folder for each namespace and one file per Kubernetes resource. 1 2 3 4 5 6 7 8 9 10 11 repository - name / \u251c\u2500\u2500 kube - system \u2502 \u251c\u2500\u2500 ingress - controller - deployment . yaml . tmpl \u2502 \u2514\u2500\u2500 ingress - controller - rbac . yaml . tmpl \u2514\u2500\u2500 alerting \u251c\u2500\u2500 alerting - app - deployment . yaml \u251c\u2500\u2500 alerting - app - service . yaml . tmpl \u251c\u2500\u2500 monitoring - sidecar - deployment . yaml \u251c\u2500\u2500 monitoring - sidecar - service . yaml . tmpl \u251c\u2500\u2500 cluster - info - configmap . yaml . tmpl \u2514\u2500\u2500 alerting - namespace . yaml Note that some files have the extension *.yaml while others have *.yaml.tmpl . The last ones are the ones that can contain template actions while the former are plain yaml files. These files can now be committed and pushed to your Quick Start repository, for example git@github.com:my-org/production-infra . 1 2 3 4 cd repository - name / git add . git commit - m \"Add component templates\" git push origin master Now that the templates are in the remote repository, the Quick Start is ready to be used with eksctl enable profile : 1 EKSCTL_EXPERIMENTAL = true eksctl enable profile --cluster team1 --region eu-west-1 --git-url git@github.com:my-org/team1-cluster --git-email alice@my-org.com git@github.com:my-org/production-infra In this example we provide github.com:my-org/production-infra as the Quick Start profile and github.com:my-org/team1-cluster as the gitops repository that is connected to the Flux instance in the cluster named cluster1 . For a full example of a Quick Start profile, check out App Dev .","title":"Creating your own Quick Start profile"}]}